{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lakehouse Lab: Data Emulator &amp; Pipeline Architecture Documentation","text":"<p>Lakehouse Lab is an open, extensible platform designed to simulate realistic data ingestion workflows. It enables users to generate synthetic data across multiple domains, stream data into storage systems, and process it through data transformation pipelines. This documentation details the system's architecture, components, data flows, and relational models.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>System Architecture Overview</li> <li>Data Factories<ul> <li>DeviceLogFactory</li> <li>TransactionFakeFactory</li> <li>UserProfileFactory</li> </ul> </li> <li>API Workflow &amp; Data Ingestion</li> <li>Orchestration and Spark Processing</li> <li>Diagrams<ul> <li>Sequence Diagram</li> <li>ER Diagram (Raw Data Model)</li> </ul> </li> </ol>"},{"location":"#introduction","title":"Introduction","text":"<p>Lakehouse Lab emulates a robust data ingestion and processing environment for modern data architectures. The platform simulates the data production lifecycle\u2014from synthetic record generation to advanced processing\u2014facilitating testing, development, and analytics at scale. The solution is built using modern technologies such as Kafka, Minio, Apache Airflow, and Apache Spark.</p>"},{"location":"#system-architecture-overview","title":"System Architecture Overview","text":"<p>Lakehouse Lab\u2019s architecture is segmented into three primary areas:</p> <ul> <li>Data Generation: Synthetic data is produced using dedicated data factories, each targeting a specific domain (e.g., user profiles, transactions, device logs).</li> <li>Data Ingestion: The synthetic data is ingested into raw storage via either message brokers (Kafka) or object storage (Minio).</li> <li>Data Processing: Downstream processes orchestrated by Apache Airflow trigger Apache Spark jobs. These jobs transform raw data into analytics-ready layers (Bronze, Silver, and Gold).</li> </ul> <p>This modular design ensures extensibility and ease of maintenance while replicating realistic data ingestion workflows.</p>"},{"location":"#data-factories","title":"Data Factories","text":"<p>Each factory is responsible for generating realistic synthetic data for a specific domain. All factories expose similar metadata so that downstream processes can treat data uniformly.</p>"},{"location":"#devicelogfactory","title":"DeviceLogFactory","text":"<ul> <li>Purpose:   Generates synthetic device log records capturing information such as user device details, login/logout timestamps, and network specifics.</li> <li>Key Fields:</li> <li><code>log_id</code></li> <li><code>user_id</code></li> <li><code>device_id</code></li> <li><code>device_type</code></li> <li><code>os</code></li> <li><code>ip_address</code></li> <li><code>location</code></li> <li><code>user_agent</code></li> <li><code>login_timestamp</code></li> <li><code>logout_timestamp</code> (optional)</li> </ul>"},{"location":"#transactionfakefactory","title":"TransactionFakeFactory","text":"<ul> <li>Purpose:   Produces synthetic financial transaction data, including optional fraud indicators.</li> <li>Key Fields:</li> <li><code>transaction_id</code></li> <li><code>user_id</code></li> <li><code>amount</code></li> <li><code>currency</code></li> <li><code>merchant</code></li> <li><code>timestamp</code></li> <li><code>location</code></li> <li><code>is_fraud</code> (optional)</li> <li>Fraud Rules:   Incorporates logic based on user compromise, card testing, merchant collusion, geographical anomalies, and an inherent fraud probability.</li> </ul>"},{"location":"#userprofilefactory","title":"UserProfileFactory","text":"<ul> <li>Purpose:   Generates synthetic user profiles, incorporating demographic and risk assessment data.</li> <li>Key Fields:</li> <li><code>user_id</code></li> <li><code>name</code></li> <li><code>email</code></li> <li><code>phone</code></li> <li><code>date_of_birth</code></li> <li><code>address</code></li> <li><code>country</code></li> <li><code>signup_date</code></li> <li><code>credit_score</code></li> <li><code>risk_level</code></li> </ul>"},{"location":"#api-workflow-data-ingestion","title":"API Workflow &amp; Data Ingestion","text":"<p>The Data Emulator Service provides a RESTful API to initiate data emulation. The following outlines the typical workflow:</p>"},{"location":"#request-initiation","title":"Request Initiation","text":"<p>An external client sends a POST request to the FastAPI endpoint. For example:</p> <pre><code>curl -X 'POST' 'http://localhost:8000/emulator/' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"emulator_sync\": \"minio\", \"emulation_domain\": \"user-profile\", \"timeout\": 120}'\n</code></pre>"},{"location":"#processing-steps","title":"Processing Steps","text":"<ol> <li> <p>Endpoint Invocation:    The FastAPI receives a <code>StartEmulatorDTO</code> payload with parameters such as the destination type (Kafka or Minio), the data domain, and an optional timeout.</p> </li> <li> <p>Factory Selection &amp; Task Scheduling:    Based on the <code>emulation_domain</code>, the correct data factory (e.g., <code>UserProfileFactory</code>) is selected. A background task is scheduled with the specified timeout.</p> </li> <li> <p>Data Generation &amp; Ingestion:    The factory continuously generates synthetic data:</p> </li> <li>For <code>emulator_sync</code> set to <code>\"kafka\"</code>: Data is published as JSON messages to Kafka.</li> <li> <p>For <code>emulator_sync</code> set to <code>\"minio\"</code>: Raw records are stored within a Minio bucket.</p> </li> <li> <p>Response Generation:    The service responds with an <code>EmulationScheduledDTO</code> that includes a unique emulation ID and execution details.</p> </li> <li> <p>Triggering Downstream Processing:    An Airflow operator may later trigger the API to indicate completion. Subsequently, Apache Spark jobs are dispatched to read, transform, and store the processed data.</p> </li> </ol>"},{"location":"#orchestration-and-spark-processing","title":"Orchestration and Spark Processing","text":""},{"location":"#airflow-orchestration","title":"Airflow Orchestration","text":"<ul> <li>Role:   Airflow is responsible for scheduling and managing the lifecycle of data emulation tasks. Once a task is completed, Airflow initiates subsequent Spark processing jobs.</li> </ul>"},{"location":"#apache-spark-processing","title":"Apache Spark Processing","text":"<ul> <li>Role:   Apache Spark is used for both batch and stream processing:</li> <li>Batch Jobs: Regularly process raw data stored in Minio or received via Kafka.</li> <li>Streaming Jobs: Continuously process incoming data (if required).</li> <li>Pipeline Layers:   The processed data is organized into multiple layers:</li> <li>Bronze: Raw ingested data.</li> <li>Silver: Cleaned and pre-aggregated data.</li> <li>Gold: Curated, analytics-ready datasets.</li> </ul>"},{"location":"#diagrams","title":"Diagrams","text":""},{"location":"#sequence-diagram","title":"Sequence Diagram","text":"<p>This diagram shows the interactions between system components during a typical data emulation workflow.</p> <p></p>"},{"location":"#er-diagram-raw-data-model","title":"ER Diagram (Raw Data Model)","text":"<p>The following ER diagram represents the relational structure of the raw data generated by the data factories.</p> <p></p>"},{"location":"getting-start/","title":"Getting Started with Lakehouse Lab","text":"<p>This guide will walk you through the prerequisites, installation, configuration, and usage of Lakehouse Lab.</p>"},{"location":"getting-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Node 23.5.0 or higher</li> <li>Golang 1.24 or higher</li> <li>Python 3.12 or higher</li> <li>Poetry 1.8.5 \u2013 a dependency management tool for Python</li> <li>Docker 27.4.0 or higher</li> </ul>"},{"location":"getting-start/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Clone the repository from GitHub and change to the project directory:</p> <pre><code>git clone https://github.com/FabioCaffarello/lakehouse-lab.git\ncd lakehouse-lab\n</code></pre>"},{"location":"getting-start/#2-set-up-the-virtual-environment","title":"2. Set Up the Virtual Environment","text":"<p>Set up the project with a single command:</p> <pre><code>make setup\n</code></pre> <p>This command will:</p> <ul> <li>Create a virtual environment in the <code>.venv</code> directory.</li> <li>Install all dependencies (including development and documentation extras).</li> <li>Set up pre-commit hooks for both commit and push stages.</li> </ul>"},{"location":"getting-start/#3-running-the-service","title":"3. Running the Service","text":"<p>To start the development server with auto-reload enabled, run:</p> <pre><code>make run\n</code></pre> <p>This command launches the Lakehouse Lab service, making the REST API available for testing and integration.</p>"},{"location":"getting-start/#4-running-tests","title":"4. Running Tests","text":"<p>To run the entire test suite, use the following command:</p> <pre><code>make check-all\n</code></pre> <p>This command executes all tests with <code>pytest</code> in the controlled environment and outputs the results along with a coverage report.</p>"},{"location":"getting-start/#5-linting-and-code-quality","title":"5. Linting and Code Quality","text":"<p>To keep your code clean and consistent, use these commands:</p> <ul> <li>Lint Code (using Ruff):</li> </ul> <pre><code>make lint\n</code></pre> <ul> <li>Run Pre-commit Hooks:</li> </ul> <pre><code>make precommit\n</code></pre>"},{"location":"getting-start/#6-documentation","title":"6. Documentation","text":""},{"location":"getting-start/#serve-documentation-locally","title":"Serve Documentation Locally","text":"<p>The project documentation is managed with MkDocs. To serve it locally with live reload, run:</p> <pre><code>make server-docs\n</code></pre> <p>Then visit http://127.0.0.1:8000 in your browser.</p>"},{"location":"getting-start/#deploy-documentation","title":"Deploy Documentation","text":"<p>To build and deploy the documentation to GitHub Pages:</p> <pre><code>make deploy-docs\n</code></pre> <p>This command builds the docs and pushes them to the appropriate branch for GitHub Pages hosting.</p>"},{"location":"getting-start/#7-additional-commands","title":"7. Additional Commands","text":"<p>For more tasks and a complete list of available commands, run:</p> <pre><code>make help\n</code></pre> <p>This will display a summary of all available Makefile targets and their descriptions.</p>"},{"location":"getting-start/#8-troubleshooting","title":"8. Troubleshooting","text":"<ul> <li>Virtual Environment Issues:   If you experience problems with the virtual environment, try clearing cached files by running:</li> </ul> <pre><code>make clean\nmake setup\n</code></pre> <ul> <li>Pre-commit Hooks Not Running:   If pre-commit hooks aren\u2019t working as expected, install them manually:</li> </ul> <pre><code>pre-commit install\n</code></pre> <ul> <li>Dependency Updates:   When updating dependencies in <code>pyproject.toml</code>, run the following commands to refresh the lock file:</li> </ul> <pre><code>npx nx reset\npoetry update\n</code></pre>"},{"location":"summary/","title":"Summary","text":"<ul> <li>Home</li> <li>Getting Start</li> <li>Services</li> <li>Libs</li> <li>Dependency Graph</li> </ul>"},{"location":"reference/libs/ddd/adapters/api/","title":"API","text":"<p>The API provides an HTTP interface for interacting with the Emulator Service. Built on FastAPI, it leverages modular controllers to coordinate use cases, dependency injection to streamline configuration and resource instantiation, and background tasks to offload long-running emulation processes.</p>"},{"location":"reference/libs/ddd/adapters/api/#features","title":"Features","text":"<ul> <li> <p>RESTful Interface:   Exposes endpoints to trigger and manage emulation processes. A dedicated <code>/emulator</code> endpoint (provided by the controllers library) allows external clients to initiate emulator workflows.</p> </li> <li> <p>Modular Architecture:   Organized into controllers and use cases, the API layer wires these components together within a FastAPI application while maintaining a clear separation of concerns.</p> </li> <li> <p>Dependency Injection:   Utilizes FastAPI\u2019s dependency injection to automatically supply configuration settings, storage clients (Minio), producer implementations (Kafka), and other dependencies.</p> </li> <li> <p>Background Processing:   Emulation tasks run as background jobs via FastAPI\u2019s BackgroundTasks, ensuring that the API remains responsive while handling long-running processes asynchronously.</p> </li> <li> <p>Graceful Shutdown:   A shutdown event hook is available for cleanup tasks, ensuring that resources are properly released when the service stops.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/api/#installation","title":"Installation","text":"<p>Install the API library along with its dependencies using your package manager (e.g., Poetry). Make sure you have configured environment variables or settings files for your external resources (e.g., Kafka, Minio).</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-adapters-api --local\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/adapters/api/#endpoints","title":"Endpoints","text":""},{"location":"reference/libs/ddd/adapters/api/#get","title":"GET <code>/</code>","text":"<p>Returns a welcome message to confirm that the API is running.</p> <p>Example Response:</p> <pre><code>{\n  \"message\": \"Welcome to the Emulator Service REST API!\"\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#post-emulator","title":"POST <code>/emulator</code>","text":"<p>Triggers the emulator process using the configuration provided in the request payload (conforming to the <code>StartEmulatorDTO</code> schema). This endpoint invokes the Start Emulator Use Case to set up producers, generate fake data, and schedule a background task.</p> <p>Request Payload Example:</p> <pre><code>{\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n  \"id\": \"generated-uuid-string\",\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#configuration","title":"Configuration","text":"<p>The API relies on a configuration object (using a custom <code>Settings</code> class) stored on the FastAPI app state. This configuration provides connection details for Kafka, Minio, and other necessary resources.</p> <p>Example Configuration on Startup:</p> <pre><code>from emulator_settings.settings import Settings\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Emulator Service REST API\",\n    description=\"API for the Emulator Service.\",\n    version=\"1.0.0\"\n)\n\napp.state.config = Settings(\n    kafka_bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",\n    kafka_password=\"your_password\",\n    minio_endpoint=\"minio.example.com\",\n    minio_access_key=\"your_access_key\",\n    minio_secret_key=\"your_secret_key\",\n    minio_secure=False\n)\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#background-tasks-and-graceful-shutdown","title":"Background Tasks and Graceful Shutdown","text":"<ul> <li> <p>Background Tasks:   Emulation processing is executed in the background via FastAPI\u2019s <code>BackgroundTasks</code>, ensuring that requests return promptly while the heavy processing continues asynchronously.</p> </li> <li> <p>Shutdown Hooks:   The <code>@app.on_event(\"shutdown\")</code> decorator registers cleanup functions to handle any necessary resource release or finalization when the service shuts down.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/api/#running-the-api","title":"Running the API","text":"<p>To run the API locally, you can use Uvicorn:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>Ensure that your application (e.g., the <code>main.py</code> module) includes the API router and configuration, as shown in the usage examples above.</p>"},{"location":"reference/libs/ddd/adapters/api/#testing","title":"Testing","text":"<p>Unit tests for the API controllers and use cases are provided. To run the API tests, execute:</p> <pre><code>npx nx test ddd-adapters-api\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/","title":"Emulator rest api","text":"<p>Emulator Service REST API This module sets up a FastAPI application for the Emulator Service. It includes the main application instance, routes, and event handlers.</p>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/#libs.ddd.adapters.api.api.emulator_rest_api.root","title":"<code>root()</code>","text":"<p>Root endpoint for the Emulator Service REST API. Returns a welcome message.</p> Source code in <code>libs/ddd/adapters/api/api/emulator_rest_api.py</code> <pre><code>@app.get(\"/\", tags=[\"Root\"])\ndef root():\n    \"\"\"\n    Root endpoint for the Emulator Service REST API.\n    Returns a welcome message.\n    \"\"\"\n    return {\"message\": \"Welcome to the Emulator Service REST API!\"}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/#libs.ddd.adapters.api.api.emulator_rest_api.shutdown_event","title":"<code>shutdown_event()</code>","text":"<p>Event handler for application shutdown. Can be used to perform cleanup tasks.</p> Source code in <code>libs/ddd/adapters/api/api/emulator_rest_api.py</code> <pre><code>@app.on_event(\"shutdown\")\ndef shutdown_event():\n    \"\"\"\n    Event handler for application shutdown.\n    Can be used to perform cleanup tasks.\n    \"\"\"\n    # Perform any necessary cleanup tasks here\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/","title":"Controllers","text":"<p>The Controllers Library encapsulates your HTTP endpoints, providing a RESTful interface for triggering and managing emulator processes. Built on top of FastAPI, the library leverages dependency injection to seamlessly integrate configuration settings, storage clients, and producer strategies into your application. This ensures that your emulator use case is easily accessible via HTTP while maintaining a clean separation of concerns.</p>"},{"location":"reference/libs/ddd/adapters/controllers/#features","title":"Features","text":"<ul> <li> <p>RESTful Endpoints:   Exposes endpoints (e.g., POST <code>/emulator</code>) that accept input via DTOs and return structured responses, enabling external clients to trigger emulator processes.</p> </li> <li> <p>Dependency Injection:   Uses FastAPI\u2019s dependency injection system to automatically obtain configuration parameters and instantiate required dependencies such as Kafka producers, Minio storage clients, and use cases.</p> </li> <li> <p>Centralized Error Handling:   Employs HTTP exceptions with appropriate status codes to manage errors and communicate issues clearly to API consumers.</p> </li> <li> <p>Integration with Background Tasks:   Schedules emulation tasks in the background using FastAPI's BackgroundTasks, ensuring non-blocking request handling during long-running emulation processes.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#installation","title":"Installation","text":"<p>Add the Controllers library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-controllers --local\n</code></pre> <p>Ensure that all required dependencies (e.g., FastAPI, confluent-kafka, Faker, and your logger library) are installed via your dependency manager (such as Poetry).</p>"},{"location":"reference/libs/ddd/adapters/controllers/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/adapters/controllers/#configuration-and-dependency-injection","title":"Configuration and Dependency Injection","text":"<p>The controllers rely on FastAPI dependency injection to supply configuration settings (via a custom <code>Settings</code> model) and to instantiate needed clients:</p> <ul> <li>get_config: Retrieves the application\u2019s configuration from the FastAPI app state.</li> <li>get_minio_client: Instantiates a <code>MinioStorageClient</code> using configuration values.</li> <li>get_kafka_producer: Instantiates a <code>KafkaProducerStrategy</code> using configuration values.</li> <li>get_start_emulator_usecase: Combines the above dependencies to construct the <code>StartEmulatorUseCase</code>.</li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#endpoints","title":"Endpoints","text":"<p>The library defines an APIRouter under the <code>/emulator</code> prefix with one key endpoint:</p>"},{"location":"reference/libs/ddd/adapters/controllers/#post-emulator","title":"POST <code>/emulator</code>","text":"<ul> <li> <p>Description:   Triggers the emulator process by accepting a <code>StartEmulatorDTO</code> payload and scheduling a background task that produces data using the appropriate producer (Kafka or Minio).</p> </li> <li> <p>Request Body:   Expects a DTO matching <code>StartEmulatorDTO</code>, which includes parameters such as emulator synchronization type, emulation domain, and timeout.</p> </li> <li> <p>Response:   Returns an <code>EmulationScheduledDTO</code> response containing the scheduled emulation details (including a unique emulation ID).</p> </li> <li> <p>Example Request:</p> </li> </ul> <pre><code>POST /emulator HTTP/1.1\nContent-Type: application/json\n\n{\n    \"emulator_sync\": \"kafka\",\n    \"emulation_domain\": \"transaction\",\n    \"timeout\": 60\n}\n</code></pre> <ul> <li>Example Response:</li> </ul> <pre><code>{\n  \"id\": \"generated-uuid-string\",\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/#integration-example","title":"Integration Example","text":"<p>Here\u2019s a sample integration that uses the Controllers library within a FastAPI application:</p> <pre><code>from fastapi import FastAPI\nfrom controllers.emulator_controller import router as emulator_router\n\napp = FastAPI()\napp.state.config = ...  # Initialize your Settings instance here.\n\n# Include the emulator endpoints\napp.include_router(emulator_router)\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Settings:   The controllers depend on a <code>Settings</code> class that holds configuration values such as Kafka bootstrap servers, Minio endpoints, and authentication credentials.</p> </li> <li> <p>Dependency Mapping:   The endpoint uses dependency providers to instantiate:</p> </li> <li> <p>A Minio storage client (<code>MinioStorageClient</code>),</p> </li> <li>A Kafka producer strategy (<code>KafkaProducerStrategy</code>),</li> <li> <p>A unified use case (<code>StartEmulatorUseCase</code>) that orchestrates background data emulation tasks.</p> </li> <li> <p>Background Task Scheduling:   Emulation tasks are scheduled using FastAPI\u2019s <code>BackgroundTasks</code>, enabling asynchronous processing of data production without blocking API responses.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#testing","title":"Testing","text":"<p>Unit tests for the Controllers library are provided and can be executed using your CI command:</p> <pre><code>npx nx test ddd-application-controllers\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/","title":"Emulator controller","text":""},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_config","title":"<code>get_config(request)</code>","text":"<p>Dependency to get the application configuration. This function retrieves the configuration from the request's state. It is used as a dependency in FastAPI routes to access the configuration settings. Args:     request (Request): The FastAPI request object. Returns:     Settings: The application configuration settings.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_config(request: Request) -&gt; Settings:\n    \"\"\"\n    Dependency to get the application configuration.\n    This function retrieves the configuration from the request's state.\n    It is used as a dependency in FastAPI routes to access the configuration\n    settings.\n    Args:\n        request (Request): The FastAPI request object.\n    Returns:\n        Settings: The application configuration settings.\n    \"\"\"\n    return request.app.state.config\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_minio_client","title":"<code>get_minio_client(config=Depends(get_config))</code>","text":"<p>Dependency to get the MinIO storage client. This function creates and returns a MinIO storage client instance using the configuration settings provided. Args:     config (Settings): The application configuration settings. Returns:     MinioStorageClient: The MinIO storage client instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_minio_client(\n    config: Settings = Depends(get_config),\n) -&gt; MinioStorageClient:  # noqa: B008\n    \"\"\"\n    Dependency to get the MinIO storage client.\n    This function creates and returns a MinIO storage client instance\n    using the configuration settings provided.\n    Args:\n        config (Settings): The application configuration settings.\n    Returns:\n        MinioStorageClient: The MinIO storage client instance.\n    \"\"\"\n    return MinioStorageClient(\n        endpoint=config.minio_endpoint,\n        access_key=config.minio_access_key,\n        secret_key=config.minio_secret_key,\n        secure=config.minio_secure,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_kafka_producer","title":"<code>get_kafka_producer(config=Depends(get_config))</code>","text":"<p>Dependency to get the Kafka producer. This function creates and returns a Kafka producer instance using the configuration settings provided. Args:     config (Settings): The application configuration settings. Returns:     KafkaProducerStrategy: The Kafka producer instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_kafka_producer(\n    config: Settings = Depends(get_config),\n) -&gt; KafkaProducerStrategy:  # noqa: B008\n    \"\"\"\n    Dependency to get the Kafka producer.\n    This function creates and returns a Kafka producer instance\n    using the configuration settings provided.\n    Args:\n        config (Settings): The application configuration settings.\n    Returns:\n        KafkaProducerStrategy: The Kafka producer instance.\n    \"\"\"\n    return KafkaProducerStrategy(\n        bootstrap_servers=config.kafka_bootstrap_servers,\n        kafka_username=config.kafka_username,\n        kafka_password=config.kafka_password,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_start_emulator_usecase","title":"<code>get_start_emulator_usecase(config=Depends(get_config), kafka_producer=Depends(get_kafka_producer), minio_client=Depends(get_minio_client))</code>","text":"<p>Dependency to get the StartEmulatorUseCase instance. This function creates and returns an instance of the StartEmulatorUseCase using the configuration settings, Kafka producer, and MinIO client provided. Args:     config (Settings): The application configuration settings.     kafka_producer (KafkaProducerStrategy): The Kafka producer instance.     minio_client (MinioStorageClient): The MinIO storage client instance. Returns:     StartEmulatorUseCase: The StartEmulatorUseCase instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_start_emulator_usecase(\n    config: Settings = Depends(get_config),  # noqa: B008\n    kafka_producer: KafkaProducerStrategy = Depends(get_kafka_producer),  # noqa: B008\n    minio_client: MinioStorageClient = Depends(get_minio_client),  # noqa: B008\n) -&gt; StartEmulatorUseCase:\n    \"\"\"\n    Dependency to get the StartEmulatorUseCase instance.\n    This function creates and returns an instance of the StartEmulatorUseCase\n    using the configuration settings, Kafka producer, and MinIO client provided.\n    Args:\n        config (Settings): The application configuration settings.\n        kafka_producer (KafkaProducerStrategy): The Kafka producer instance.\n        minio_client (MinioStorageClient): The MinIO storage client instance.\n    Returns:\n        StartEmulatorUseCase: The StartEmulatorUseCase instance.\n    \"\"\"\n    return StartEmulatorUseCase(\n        kafka_producer=kafka_producer,\n        kafka_brokers=config.kafka_bootstrap_servers,\n        minio_client=minio_client,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.generate_emulation","title":"<code>generate_emulation(dto, background_tasks, usecase=Depends(get_start_emulator_usecase))</code>","text":"<p>Endpoint to start the emulator. This endpoint receives a StartEmulatorDTO object, processes it using the StartEmulatorUseCase, and returns an EmulationScheduledDTO object. Args:     dto (StartEmulatorDTO): The data transfer object containing the         emulation parameters.     background_tasks (BackgroundTasks): FastAPI background tasks         instance for handling background tasks.     usecase (StartEmulatorUseCase): The use case instance for starting         the emulator. Returns:     EmulationScheduledDTO: The data transfer object containing the         emulation scheduling result. Raises:     HTTPException: If there is an error during the emulation process.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>@router.post(\"/\", response_model=EmulationScheduledDTO, status_code=201)\ndef generate_emulation(\n    dto: StartEmulatorDTO,\n    background_tasks: BackgroundTasks,\n    usecase: StartEmulatorUseCase = Depends(get_start_emulator_usecase),  # noqa: B008\n):\n    \"\"\"\n    Endpoint to start the emulator.\n    This endpoint receives a StartEmulatorDTO object, processes it using the\n    StartEmulatorUseCase, and returns an EmulationScheduledDTO object.\n    Args:\n        dto (StartEmulatorDTO): The data transfer object containing the\n            emulation parameters.\n        background_tasks (BackgroundTasks): FastAPI background tasks\n            instance for handling background tasks.\n        usecase (StartEmulatorUseCase): The use case instance for starting\n            the emulator.\n    Returns:\n        EmulationScheduledDTO: The data transfer object containing the\n            emulation scheduling result.\n    Raises:\n        HTTPException: If there is an error during the emulation process.\n    \"\"\"\n    try:\n        return usecase.execute(dto, background_tasks, num_threads=5)\n    except Exception as e:\n        raise HTTPException(status_code=HTTPStatus.BAD_REQUEST, detail=str(e)) from e\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/","title":"DTOs","text":"<p>The DTOs (Data Transfer Objects) Library provides structured representations of emulation data for your application. It simplifies data transfer between different layers and ensures data consistency and validation across the system.</p>"},{"location":"reference/libs/ddd/application/dtos/#features","title":"Features","text":"<ul> <li> <p>Immutable Data Structures:   The <code>EmulationScheduledDTO</code> is implemented as a frozen dataclass, ensuring that once created, its values remain unchanged.</p> </li> <li> <p>Data Validation:   The <code>StartEmulatorDTO</code> leverages Pydantic's <code>BaseModel</code> to validate and parse input data for starting an emulation.</p> </li> <li> <p>Consistent Data Transfer:   Both DTOs encapsulate essential information about an emulation, including synchronization details, domain, timeout settings, and a unique identifier.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#installation","title":"Installation","text":"<p>Add the DTOs library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-dtos --local\n</code></pre> <p>Ensure that your environment includes all required dependencies such as <code>pydantic</code>.</p>"},{"location":"reference/libs/ddd/application/dtos/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/application/dtos/#emulationscheduleddto","title":"EmulationScheduledDTO","text":"<p>The <code>EmulationScheduledDTO</code> is a frozen dataclass representing an emulation's scheduled details. It includes the following fields:</p> <ul> <li>id: An <code>EmulationID</code> value object representing the unique identifier of the emulation.</li> <li>emulator_sync: A string indicating the synchronization details.</li> <li>emulation_domain: A string specifying the emulation domain.</li> <li>timeout: An integer value representing the timeout duration.</li> </ul> <p>Example:</p> <pre><code>from dtos.emulation_dto import EmulationScheduledDTO\nfrom value_objects.emulator_id import EmulationID\n\n# Create an EmulationScheduledDTO instance\nemulation_dto = EmulationScheduledDTO(\n    id=EmulationID.generate(),\n    emulator_sync=\"sync_value\",\n    emulation_domain=\"domain_value\",\n    timeout=30\n)\n\nprint(emulation_dto)\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/#startemulatordto","title":"StartEmulatorDTO","text":"<p>The <code>StartEmulatorDTO</code> is a Pydantic model used for starting an emulation. It validates the following fields:</p> <ul> <li>emulator_sync: A string indicating synchronization details.</li> <li>emulation_domain: A string specifying the emulation domain.</li> <li>timeout: An integer value representing the timeout duration.</li> </ul> <p>Example:</p> <pre><code>from dtos.emulation_dto import StartEmulatorDTO\n\n# Define data for starting an emulation\ndata = {\n    \"emulator_sync\": \"sync_value\",\n    \"emulation_domain\": \"domain_value\",\n    \"timeout\": 30\n}\n\n# Parse and validate the data using StartEmulatorDTO\nstart_dto = StartEmulatorDTO(**data)\nprint(start_dto)\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Immutable DTO: <code>EmulationScheduledDTO</code> is implemented with <code>@dataclass(frozen=True)</code> to ensure immutability, promoting data integrity across your application.</p> </li> <li> <p>Robust Validation: <code>StartEmulatorDTO</code> uses Pydantic's powerful data validation and parsing features to ensure that only valid data is processed when starting an emulation.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#testing","title":"Testing","text":"<p>Unit tests are not included in this library. However, can be created if needed (remember to edit the test command in the project.json to not pass in failures). To run the tests, navigate to the library directory and execute:</p> <pre><code>npx nx test ddd-application-dtos\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/","title":"Emulation dto","text":""},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.EmulationScheduledDTO","title":"<code>EmulationScheduledDTO</code>  <code>dataclass</code>","text":"<p>Data Transfer Object representing an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>@dataclass(frozen=True)\nclass EmulationScheduledDTO:\n    \"\"\"Data Transfer Object representing an emulation.\"\"\"\n\n    id: EmulationID\n    emulator_sync: str\n    emulation_domain: str\n    timeout: int\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.StartEmulatorDTO","title":"<code>StartEmulatorDTO</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DTO for starting an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>class StartEmulatorDTO(BaseModel):\n    \"\"\"DTO for starting an emulation.\"\"\"\n\n    emulator_sync: str\n    emulation_domain: str\n    timeout: int\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/","title":"Use Cases","text":"<p>The Use Cases Library provides core application logic that orchestrates various components like messaging producers, fake data generators, and storage clients to implement real-world business scenarios. A primary example is the Start Emulator Use Case, which starts an emulation process by coordinating between Kafka/Minio producers, data factories, and FastAPI's background tasks.</p>"},{"location":"reference/libs/ddd/application/usecases/#features","title":"Features","text":"<ul> <li> <p>Abstract Sync Producer Interface:   Defines a uniform interface (<code>SyncProducer</code>) for synchronous message production across multiple systems (e.g., Kafka or Minio).</p> </li> <li> <p>Producer Wrapper Implementations:   Includes concrete implementations for Kafka (<code>KafkaFactorySyncProducerWrapper</code>) and Minio (<code>MinioFactorySyncProducerWrapper</code>). These wrappers encapsulate system-specific resource setup and message production.</p> </li> <li> <p>Flexible Data Emulation:   Uses various fake data factories to generate realistic transaction, device log, or user profile data, simulating actual operational data flows.</p> </li> <li> <p>Background Task Scheduling:   Integrates with FastAPI's <code>BackgroundTasks</code> to schedule long-running emulation tasks in a non-blocking manner.</p> </li> <li> <p>Resource Setup and Parallel Production:   Ensures that required resources (Kafka topics or Minio buckets) are created if missing. Supports parallel data production using multiple threads with proper synchronization and graceful shutdown using timers.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#installation","title":"Installation","text":"<p>Add the Use Cases library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-usecases --local\n</code></pre> <p>Ensure that all required dependencies (such as confluent-kafka, fastapi, Faker, and your logger library) are installed via your dependency manager (e.g., Poetry).</p>"},{"location":"reference/libs/ddd/application/usecases/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/application/usecases/#instantiating-producer-wrappers","title":"Instantiating Producer Wrappers","text":"<p>Depending on your infrastructure, instantiate the appropriate producer wrapper:</p>"},{"location":"reference/libs/ddd/application/usecases/#kafka-producer-wrapper","title":"Kafka Producer Wrapper","text":"<pre><code>from producers.kafka.producer import KafkaProducerStrategy\nfrom ddd.application.usecases.start_emulator import KafkaFactorySyncProducerWrapper\n\n# Create a Kafka producer strategy instance (configure bootstrap servers, etc.)\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",  # Optional, for SASL_SSL configuration\n    kafka_password=\"your_password\"   # Optional, for SASL_SSL configuration\n)\n\n# Wrap the Kafka producer\nkafka_wrapper = KafkaFactorySyncProducerWrapper(\n    kafka_producer=kafka_producer,\n    kafka_brokers=\"localhost:9092\"\n)\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#minio-producer-wrapper","title":"Minio Producer Wrapper","text":"<pre><code>from storage.minio.storage import MinioStorageClient\nfrom ddd.application.usecases.start_emulator import MinioFactorySyncProducerWrapper\n\n# Initialize the Minio client with your Minio endpoint and credentials.\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False  # Set to True if using HTTPS\n)\n\n# Wrap the Minio client\nminio_wrapper = MinioFactorySyncProducerWrapper(minio_client)\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#starting-the-emulator","title":"Starting the Emulator","text":"<p>The Start Emulator Use Case coordinates the emulation start process. It:</p> <ul> <li>Determines the target resource (Kafka topic or Minio bucket) based on the domain.</li> <li>Retrieves the corresponding fake data factory.</li> <li>Schedules a background task to continuously produce messages using multiple parallel threads until the emulation timeout is reached.</li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#example","title":"Example","text":"<pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom ddd.application.usecases.start_emulator import StartEmulatorUseCase\nfrom producers.kafka.producer import KafkaProducerStrategy\nfrom storage.minio.storage import MinioStorageClient\nfrom dtos.emulation_dto import StartEmulatorDTO\n\n# Create your FastAPI app\napp = FastAPI()\n\n# Create instances for Kafka and Minio components.\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",\n    kafka_password=\"your_password\"\n)\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False\n)\n\n# Instantiate the use case with both producer implementations.\nstart_emulator_usecase = StartEmulatorUseCase(\n    kafka_producer=kafka_producer,\n    kafka_brokers=\"localhost:9092\",\n    minio_client=minio_client\n)\n\n@app.post(\"/start-emulator\")\ndef start_emulator(dto: StartEmulatorDTO, background_tasks: BackgroundTasks):\n    # Specify the number of parallel threads (e.g., 5)\n    emulation_scheduled = start_emulator_usecase.execute(dto, background_tasks, num_threads=5)\n    return emulation_scheduled\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#key-components","title":"Key Components","text":"<ul> <li> <p>SyncProducer (Abstract Base Class):   Defines the contract for producers with methods like <code>produce()</code>, <code>flush()</code>, and <code>setup_resource()</code>. This ensures consistent behavior across different implementations.</p> </li> <li> <p>Producer Wrappers:</p> </li> <li> <p>KafkaFactorySyncProducerWrapper:     Uses Confluent Kafka's producer to send JSON messages and automatically creates topics if necessary.</p> </li> <li> <p>MinioFactorySyncProducerWrapper:     Uses Minio to upload messages as JSON objects into a bucket. Here, the <code>flush()</code> operation is a no-op.</p> </li> <li> <p>Background Emulation Task:   The use case schedules an emulation task in the background. It spawns multiple threads that invoke <code>produce_data()</code> repeatedly until a specified timeout, then flushes the producer and logs the completion of the emulation.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Resource Mapping:   The use case internally maps emulation domains (e.g., \"transaction\", \"user-profile\", \"device-log\") to target topics or buckets. A default topic is used for unsupported domains.</p> </li> <li> <p>Factory Mapping:   Associates each domain with the corresponding fake data factory to generate realistic records.</p> </li> <li> <p>Parallel Processing:   Implements thread-based parallelism for high-throughput emulation. A global stop event and timer are used to gracefully shut down production.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#testing","title":"Testing","text":"<p>Unit tests are provided for this use case within the tests folder. The test suite verifies correct producer selection, resource setup, task scheduling, and proper execution of parallel data production.</p> <p>To run the tests:</p> <pre><code>npx nx test ddd-application-usecases\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/","title":"Start emulator","text":""},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer","title":"<code>SyncProducer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for synchronous producers.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class SyncProducer(ABC):\n    \"\"\"Abstract base class for synchronous producers.\"\"\"\n\n    @abstractmethod\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"Produces a message to the given topic.\"\"\"\n        pass\n\n    @abstractmethod\n    def flush(self) -&gt; None:\n        \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n        pass\n\n    @abstractmethod\n    def setup_resource(self, topic: str) -&gt; None:\n        \"\"\"Sets up the producer resource (e.g., create a topic or bucket).\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.produce","title":"<code>produce(topic, key, value)</code>  <code>abstractmethod</code>","text":"<p>Produces a message to the given topic.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"Produces a message to the given topic.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.flush","title":"<code>flush()</code>  <code>abstractmethod</code>","text":"<p>Flushes the producer, ensuring all messages are sent.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef flush(self) -&gt; None:\n    \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.setup_resource","title":"<code>setup_resource(topic)</code>  <code>abstractmethod</code>","text":"<p>Sets up the producer resource (e.g., create a topic or bucket).</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef setup_resource(self, topic: str) -&gt; None:\n    \"\"\"Sets up the producer resource (e.g., create a topic or bucket).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper","title":"<code>KafkaFactorySyncProducerWrapper</code>","text":"<p>               Bases: <code>SyncProducer</code></p> <p>Synchronous Kafka producer wrapper implementation.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class KafkaFactorySyncProducerWrapper(SyncProducer):\n    \"\"\"Synchronous Kafka producer wrapper implementation.\"\"\"\n\n    def __init__(\n        self,\n        kafka_producer: KafkaProducerStrategy,\n        kafka_brokers: str,\n        num_partitions: int = 5,\n        replication_factor: int = 2,\n    ):\n        self.kafka_producer = kafka_producer\n        self.kafka_brokers = kafka_brokers\n        self.num_partitions = num_partitions\n        self.replication_factor = replication_factor\n\n    def setup_resource(self, topic: str) -&gt; None:\n        \"\"\"Creates the Kafka topic if it does not already exist.\"\"\"\n        admin_client = AdminClient({\"bootstrap.servers\": self.kafka_brokers})\n        metadata = admin_client.list_topics(timeout=10)\n        if topic not in metadata.topics:\n            new_topic = NewTopic(\n                topic=topic,\n                num_partitions=self.num_partitions,\n                replication_factor=self.replication_factor,\n            )\n            fs = admin_client.create_topics([new_topic])\n            for t, future in fs.items():\n                try:\n                    future.result()\n                    logger.info(f\"Topic {t} created successfully\")\n                except Exception as e:\n                    logger.error(f\"Failed to create topic {t}: {e}\")\n        else:\n            logger.info(f\"Topic {topic} already exists\")\n\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Produces a message to a Kafka topic.\n\n        Args:\n            topic (str): The Kafka topic to send the message to.\n            key (str): The key of the message.\n            value (dict[str, Any]): The message payload.\n        \"\"\"\n        try:\n            self.kafka_producer.produce(topic=topic, key=key, value=json.dumps(value))\n        except Exception as e:\n            logger.error(f\"Kafka production error: {e}\")\n\n    def flush(self) -&gt; None:\n        \"\"\"Flushes the Kafka producer.\"\"\"\n        self.kafka_producer.flush()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.setup_resource","title":"<code>setup_resource(topic)</code>","text":"<p>Creates the Kafka topic if it does not already exist.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def setup_resource(self, topic: str) -&gt; None:\n    \"\"\"Creates the Kafka topic if it does not already exist.\"\"\"\n    admin_client = AdminClient({\"bootstrap.servers\": self.kafka_brokers})\n    metadata = admin_client.list_topics(timeout=10)\n    if topic not in metadata.topics:\n        new_topic = NewTopic(\n            topic=topic,\n            num_partitions=self.num_partitions,\n            replication_factor=self.replication_factor,\n        )\n        fs = admin_client.create_topics([new_topic])\n        for t, future in fs.items():\n            try:\n                future.result()\n                logger.info(f\"Topic {t} created successfully\")\n            except Exception as e:\n                logger.error(f\"Failed to create topic {t}: {e}\")\n    else:\n        logger.info(f\"Topic {topic} already exists\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.produce","title":"<code>produce(topic, key, value)</code>","text":"<p>Produces a message to a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The Kafka topic to send the message to.</p> required <code>key</code> <code>str</code> <p>The key of the message.</p> required <code>value</code> <code>dict[str, Any]</code> <p>The message payload.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Produces a message to a Kafka topic.\n\n    Args:\n        topic (str): The Kafka topic to send the message to.\n        key (str): The key of the message.\n        value (dict[str, Any]): The message payload.\n    \"\"\"\n    try:\n        self.kafka_producer.produce(topic=topic, key=key, value=json.dumps(value))\n    except Exception as e:\n        logger.error(f\"Kafka production error: {e}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.flush","title":"<code>flush()</code>","text":"<p>Flushes the Kafka producer.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flushes the Kafka producer.\"\"\"\n    self.kafka_producer.flush()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper","title":"<code>MinioFactorySyncProducerWrapper</code>","text":"<p>               Bases: <code>SyncProducer</code></p> <p>Synchronous Minio producer wrapper implementation.</p> <p>This implementation uses a MinioClient to upload messages as individual objects.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class MinioFactorySyncProducerWrapper(SyncProducer):\n    \"\"\"Synchronous Minio producer wrapper implementation.\n\n    This implementation uses a MinioClient to upload messages as individual objects.\n    \"\"\"\n\n    def __init__(self, minio_client: MinioStorageClient):\n        self.minio_client = minio_client\n        self.bucket = None\n        self.lock = threading.Lock()\n\n    def setup_resource(self, bucket_name: str) -&gt; None:\n        \"\"\"\n        Creates a bucket on Minio if it does not already exist.\n\n        Args:\n            bucket_name (str): The name of the bucket (used as the \"topic\").\n        \"\"\"\n        self.bucket = bucket_name\n        buckets = self.minio_client.list_buckets()\n        if bucket_name not in buckets:\n            self.minio_client.create_bucket(bucket_name)\n            logger.info(f\"Bucket {bucket_name} created successfully\")\n        else:\n            logger.info(f\"Bucket {bucket_name} already exists\")\n\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Uploads a message as an object to the Minio bucket.\n\n        Args:\n            topic (str): Not used for Minio; bucket name is used instead.\n            key (str): A key used to generate a unique object name.\n            value (dict[str, Any]): The message payload.\n        \"\"\"\n        message_bytes = json.dumps(value).encode(\"utf-8\")\n        # Generate a unique object name (e.g., using key and current time)\n        object_name = f\"{key}_{int(time.time() * 1000)}.json\"\n        with self.lock:\n            self.minio_client.upload_bytes(self.bucket, object_name, message_bytes)\n            logger.info(f\"Uploaded object {object_name} to bucket {self.bucket}\")\n\n    def flush(self) -&gt; None:\n        \"\"\"Flush is a no-op for the Minio producer.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.setup_resource","title":"<code>setup_resource(bucket_name)</code>","text":"<p>Creates a bucket on Minio if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket (used as the \"topic\").</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def setup_resource(self, bucket_name: str) -&gt; None:\n    \"\"\"\n    Creates a bucket on Minio if it does not already exist.\n\n    Args:\n        bucket_name (str): The name of the bucket (used as the \"topic\").\n    \"\"\"\n    self.bucket = bucket_name\n    buckets = self.minio_client.list_buckets()\n    if bucket_name not in buckets:\n        self.minio_client.create_bucket(bucket_name)\n        logger.info(f\"Bucket {bucket_name} created successfully\")\n    else:\n        logger.info(f\"Bucket {bucket_name} already exists\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.produce","title":"<code>produce(topic, key, value)</code>","text":"<p>Uploads a message as an object to the Minio bucket.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>Not used for Minio; bucket name is used instead.</p> required <code>key</code> <code>str</code> <p>A key used to generate a unique object name.</p> required <code>value</code> <code>dict[str, Any]</code> <p>The message payload.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Uploads a message as an object to the Minio bucket.\n\n    Args:\n        topic (str): Not used for Minio; bucket name is used instead.\n        key (str): A key used to generate a unique object name.\n        value (dict[str, Any]): The message payload.\n    \"\"\"\n    message_bytes = json.dumps(value).encode(\"utf-8\")\n    # Generate a unique object name (e.g., using key and current time)\n    object_name = f\"{key}_{int(time.time() * 1000)}.json\"\n    with self.lock:\n        self.minio_client.upload_bytes(self.bucket, object_name, message_bytes)\n        logger.info(f\"Uploaded object {object_name} to bucket {self.bucket}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.flush","title":"<code>flush()</code>","text":"<p>Flush is a no-op for the Minio producer.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flush is a no-op for the Minio producer.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase","title":"<code>StartEmulatorUseCase</code>","text":"<p>Use case for starting the data emulator with flexible sync strategies.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class StartEmulatorUseCase:\n    \"\"\"Use case for starting the data emulator with flexible sync strategies.\"\"\"\n\n    def __init__(\n        self,\n        kafka_producer: KafkaProducerStrategy,\n        kafka_brokers: str,\n        minio_client: MinioStorageClient,\n    ):\n        self.topics_mapping = {\n            \"transaction\": \"transactions\",\n            \"user-profile\": \"user-profiles\",\n            \"device-log\": \"device-logs\",\n            \"default\": \"default_topic\",\n        }\n        self.fake_factories: dict[str, Any] = {\n            \"transaction\": TransactionFakeFactory,\n            \"user-profile\": UserProfileFactory,\n            \"device-log\": DeviceLogFactory,\n        }\n        # Map sync types to their corresponding producer wrapper classes.\n        self.producer_wrapper_mapping = {\n            \"kafka\": KafkaFactorySyncProducerWrapper(kafka_producer, kafka_brokers),\n            \"minio\": MinioFactorySyncProducerWrapper(minio_client),\n        }\n\n    def execute(\n        self, dto: StartEmulatorDTO, background_tasks: BackgroundTasks, num_threads: int\n    ) -&gt; EmulationScheduledDTO:\n        \"\"\"\n        Executes the emulator and schedules the background emulation task.\n\n        Args:\n            dto (StartEmulatorDTO): The DTO containing emulator parameters.\n            background_tasks (BackgroundTasks): FastAPI background tasks manager.\n            num_threads (int): Number of parallel threads to run.\n\n        Returns:\n            EmulationScheduledDTO: DTO with details about the scheduled emulation.\n\n        Raises:\n            ValueError: If the sync type or domain is not supported.\n        \"\"\"\n        emulation_id = EmulationID.generate()\n        sync_type = dto.emulator_sync.lower()\n\n        # Retrieve the producer wrapper based on the sync type.\n        producer_wrapper = self.producer_wrapper_mapping.get(sync_type)\n        if producer_wrapper is None:\n            raise ValueError(f\"Producer wrapper not found for sync type: {sync_type}\")\n\n        # Determine the target topic (or bucket name) based on the emulation domain.\n        domain = dto.emulation_domain.lower()\n        topic = self.topics_mapping.get(domain, self.topics_mapping[\"default\"])\n        producer_wrapper.setup_resource(topic)\n\n        # Retrieve the appropriate fake factory for the specified domain.\n        fake_factory_class = self.fake_factories.get(domain)\n        if fake_factory_class is None:\n            raise ValueError(f\"Domain not supported: {dto.emulation_domain}\")\n        fake_factory = fake_factory_class()\n\n        # Schedule the background emulation task.\n        background_tasks.add_task(\n            self._run_emulation_task,\n            emulation_id,\n            producer_wrapper,\n            topic,\n            fake_factory,\n            dto.timeout,\n            num_threads,\n        )\n\n        return EmulationScheduledDTO(\n            id=emulation_id,\n            emulator_sync=dto.emulator_sync,\n            emulation_domain=dto.emulation_domain,\n            timeout=dto.timeout,\n        )\n\n    def produce_data(\n        self,\n        emulation_id: Any,\n        thread_id: int,\n        producer: SyncProducer,\n        topic: str,\n        stop_event: threading.Event,\n        factory: Any,\n    ) -&gt; None:\n        \"\"\"\n        Produces data continuously until the stop event is triggered.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            thread_id (int): Identifier for the thread.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            stop_event (threading.Event): Event to signal when to stop production.\n            factory(Any): The fake data factory.\n        \"\"\"\n        while not stop_event.is_set():\n            fake_data = factory.generate()\n            if fake_data is None:\n                logger.info(f\"Thread {thread_id} - No more data to process\")\n                break\n            message_payload = {\n                \"emulation_id\": str(emulation_id),\n                \"timestamp\": time.time(),\n                \"data\": fake_data,\n            }\n            try:\n                key = fake_data.get(\"transaction_id\", str(time.time()))\n                producer.produce(topic=topic, key=key, value=message_payload)\n                logger.info(f\"Thread {thread_id} - Produced message: {message_payload}\")\n            except Exception as e:\n                logger.error(f\"Failed to produce message: {e}\")\n\n    def produce_data_in_parallel(\n        self,\n        emulation_id: Any,\n        producer: SyncProducer,\n        topic: str,\n        factory: Any,\n        stop_event: threading.Event,\n        num_threads: int,\n    ) -&gt; None:\n        \"\"\"\n        Starts multiple threads to produce data in parallel.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            factory (Any): The fake data factory.\n            stop_event (threading.Event): Event to signal when to stop production.\n            num_threads (int): Number of parallel threads to run.\n        \"\"\"\n        threads = []\n        try:\n            for i in range(num_threads):\n                thread = threading.Thread(\n                    target=self.produce_data,\n                    args=(emulation_id, i, producer, topic, stop_event, factory),\n                )\n                thread.daemon = True\n                thread.start()\n                threads.append(thread)\n            for thread in threads:\n                thread.join()\n        except Exception as e:\n            logger.error(f\"Failed to start threads: {e}\")\n\n    def _run_emulation_task(\n        self,\n        emulation_id: Any,\n        producer: SyncProducer,\n        topic: str,\n        factory: Any,\n        timeout: float,\n        num_threads: int,\n    ) -&gt; None:\n        \"\"\"\n        Runs the emulation task until the specified timeout elapses.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            factory (Any): The fake data factory.\n            timeout (float): Emulation duration in seconds.\n            num_threads (int): Number of parallel threads to run.\n        \"\"\"\n        stop_event = threading.Event()\n        timer = threading.Timer(timeout, stop_event.set)\n        timer.start()\n\n        self.produce_data_in_parallel(\n            emulation_id, producer, topic, factory, stop_event, num_threads\n        )\n        timer.cancel()\n        producer.flush()\n        logger.info(\"Emulation finished\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.execute","title":"<code>execute(dto, background_tasks, num_threads)</code>","text":"<p>Executes the emulator and schedules the background emulation task.</p> <p>Parameters:</p> Name Type Description Default <code>dto</code> <code>StartEmulatorDTO</code> <p>The DTO containing emulator parameters.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>FastAPI background tasks manager.</p> required <code>num_threads</code> <code>int</code> <p>Number of parallel threads to run.</p> required <p>Returns:</p> Name Type Description <code>EmulationScheduledDTO</code> <code>EmulationScheduledDTO</code> <p>DTO with details about the scheduled emulation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sync type or domain is not supported.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def execute(\n    self, dto: StartEmulatorDTO, background_tasks: BackgroundTasks, num_threads: int\n) -&gt; EmulationScheduledDTO:\n    \"\"\"\n    Executes the emulator and schedules the background emulation task.\n\n    Args:\n        dto (StartEmulatorDTO): The DTO containing emulator parameters.\n        background_tasks (BackgroundTasks): FastAPI background tasks manager.\n        num_threads (int): Number of parallel threads to run.\n\n    Returns:\n        EmulationScheduledDTO: DTO with details about the scheduled emulation.\n\n    Raises:\n        ValueError: If the sync type or domain is not supported.\n    \"\"\"\n    emulation_id = EmulationID.generate()\n    sync_type = dto.emulator_sync.lower()\n\n    # Retrieve the producer wrapper based on the sync type.\n    producer_wrapper = self.producer_wrapper_mapping.get(sync_type)\n    if producer_wrapper is None:\n        raise ValueError(f\"Producer wrapper not found for sync type: {sync_type}\")\n\n    # Determine the target topic (or bucket name) based on the emulation domain.\n    domain = dto.emulation_domain.lower()\n    topic = self.topics_mapping.get(domain, self.topics_mapping[\"default\"])\n    producer_wrapper.setup_resource(topic)\n\n    # Retrieve the appropriate fake factory for the specified domain.\n    fake_factory_class = self.fake_factories.get(domain)\n    if fake_factory_class is None:\n        raise ValueError(f\"Domain not supported: {dto.emulation_domain}\")\n    fake_factory = fake_factory_class()\n\n    # Schedule the background emulation task.\n    background_tasks.add_task(\n        self._run_emulation_task,\n        emulation_id,\n        producer_wrapper,\n        topic,\n        fake_factory,\n        dto.timeout,\n        num_threads,\n    )\n\n    return EmulationScheduledDTO(\n        id=emulation_id,\n        emulator_sync=dto.emulator_sync,\n        emulation_domain=dto.emulation_domain,\n        timeout=dto.timeout,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.produce_data","title":"<code>produce_data(emulation_id, thread_id, producer, topic, stop_event, factory)</code>","text":"<p>Produces data continuously until the stop event is triggered.</p> <p>Parameters:</p> Name Type Description Default <code>emulation_id</code> <code>Any</code> <p>Unique emulation identifier.</p> required <code>thread_id</code> <code>int</code> <p>Identifier for the thread.</p> required <code>producer</code> <code>SyncProducer</code> <p>The producer instance.</p> required <code>topic</code> <code>str</code> <p>Target topic or output resource.</p> required <code>stop_event</code> <code>Event</code> <p>Event to signal when to stop production.</p> required <code>factory(Any)</code> <p>The fake data factory.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce_data(\n    self,\n    emulation_id: Any,\n    thread_id: int,\n    producer: SyncProducer,\n    topic: str,\n    stop_event: threading.Event,\n    factory: Any,\n) -&gt; None:\n    \"\"\"\n    Produces data continuously until the stop event is triggered.\n\n    Args:\n        emulation_id (Any): Unique emulation identifier.\n        thread_id (int): Identifier for the thread.\n        producer (SyncProducer): The producer instance.\n        topic (str): Target topic or output resource.\n        stop_event (threading.Event): Event to signal when to stop production.\n        factory(Any): The fake data factory.\n    \"\"\"\n    while not stop_event.is_set():\n        fake_data = factory.generate()\n        if fake_data is None:\n            logger.info(f\"Thread {thread_id} - No more data to process\")\n            break\n        message_payload = {\n            \"emulation_id\": str(emulation_id),\n            \"timestamp\": time.time(),\n            \"data\": fake_data,\n        }\n        try:\n            key = fake_data.get(\"transaction_id\", str(time.time()))\n            producer.produce(topic=topic, key=key, value=message_payload)\n            logger.info(f\"Thread {thread_id} - Produced message: {message_payload}\")\n        except Exception as e:\n            logger.error(f\"Failed to produce message: {e}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.produce_data_in_parallel","title":"<code>produce_data_in_parallel(emulation_id, producer, topic, factory, stop_event, num_threads)</code>","text":"<p>Starts multiple threads to produce data in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>emulation_id</code> <code>Any</code> <p>Unique emulation identifier.</p> required <code>producer</code> <code>SyncProducer</code> <p>The producer instance.</p> required <code>topic</code> <code>str</code> <p>Target topic or output resource.</p> required <code>factory</code> <code>Any</code> <p>The fake data factory.</p> required <code>stop_event</code> <code>Event</code> <p>Event to signal when to stop production.</p> required <code>num_threads</code> <code>int</code> <p>Number of parallel threads to run.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce_data_in_parallel(\n    self,\n    emulation_id: Any,\n    producer: SyncProducer,\n    topic: str,\n    factory: Any,\n    stop_event: threading.Event,\n    num_threads: int,\n) -&gt; None:\n    \"\"\"\n    Starts multiple threads to produce data in parallel.\n\n    Args:\n        emulation_id (Any): Unique emulation identifier.\n        producer (SyncProducer): The producer instance.\n        topic (str): Target topic or output resource.\n        factory (Any): The fake data factory.\n        stop_event (threading.Event): Event to signal when to stop production.\n        num_threads (int): Number of parallel threads to run.\n    \"\"\"\n    threads = []\n    try:\n        for i in range(num_threads):\n            thread = threading.Thread(\n                target=self.produce_data,\n                args=(emulation_id, i, producer, topic, stop_event, factory),\n            )\n            thread.daemon = True\n            thread.start()\n            threads.append(thread)\n        for thread in threads:\n            thread.join()\n    except Exception as e:\n        logger.error(f\"Failed to start threads: {e}\")\n</code></pre>"},{"location":"reference/libs/ddd/entities/","title":"Entities","text":"<p>The Entities Library provides domain entities that encapsulate business logic and data for your application. It currently includes the <code>Emulation</code> entity, which represents an emulation instance with a unique identifier, a timeout value, an emulator type, and a timestamp indicating when it was created.</p>"},{"location":"reference/libs/ddd/entities/#features","title":"Features","text":"<ul> <li> <p>Unique Identification:   Each <code>Emulation</code> entity is assigned a unique identifier using the <code>EmulationID</code> value object. This ensures that every emulation instance can be uniquely referenced.</p> </li> <li> <p>Automatic Timestamping:   The entity records its creation time (<code>created_at</code>) automatically using the current UTC datetime.</p> </li> <li> <p>Flexible Data Structure:   The <code>Emulation</code> entity includes core attributes such as <code>timeout</code> and <code>emulator_type</code>, and it can be easily extended to accommodate additional properties.</p> </li> <li> <p>Easy Serialization:   The <code>to_dict</code> method converts the entity to a dictionary, making it simple to serialize data for APIs, logging, or other integrations.</p> </li> </ul>"},{"location":"reference/libs/ddd/entities/#installation","title":"Installation","text":"<p>Add the Entities library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-entities --local\n</code></pre>"},{"location":"reference/libs/ddd/entities/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/entities/#creating-an-emulation-entity","title":"Creating an Emulation Entity","text":"<p>To create an instance of an <code>Emulation</code> entity, simply import the class and instantiate it with the required parameters:</p> <pre><code>from entities.emulation import Emulation\n\n# Create an Emulation entity with a timeout of 30 and a specified emulator type.\nemulation = Emulation(timeout=30, emulator_type=\"test_emulator\")\nprint(emulation.to_dict())\n</code></pre>"},{"location":"reference/libs/ddd/entities/#understanding-the-attributes","title":"Understanding the Attributes","text":"<ul> <li> <p>id:   A unique identifier automatically generated using the <code>EmulationID</code> value object.</p> </li> <li> <p>timeout:   An integer representing the timeout period for the emulation.</p> </li> <li> <p>emulator_type:   A string specifying the type of emulator.</p> </li> <li> <p>created_at:   A UTC timestamp indicating when the emulation was created.</p> </li> </ul>"},{"location":"reference/libs/ddd/entities/#testing","title":"Testing","text":"<p>Unit tests are included to ensure that the <code>Emulation</code> entity behaves as expected. To run the tests, navigate to the library directory and execute:</p> <pre><code>npx nx test ddd-entities\n</code></pre>"},{"location":"reference/libs/ddd/entities/code_reference/entities/emulation/","title":"Emulation","text":""},{"location":"reference/libs/ddd/infra/producers/","title":"Producers","text":"<p>The Producers Library provides a unified interface for producing messages to external systems, with a focus on Kafka messaging. This library includes a base producer abstraction and a concrete Kafka producer implementation. It is designed to be extensible, allowing future support for additional messaging systems while maintaining a consistent API.</p>"},{"location":"reference/libs/ddd/infra/producers/#features","title":"Features","text":"<ul> <li>Unified Producer Interface:   Defines a standard interface (<code>BaseProducer</code>) for message production across different platforms.</li> <li> <p>Kafka Producer Implementation:   Provides a robust implementation (<code>KafkaProducerStrategy</code>) using Confluent Kafka for producing messages to Kafka topics.</p> </li> <li> <p>Configurable and Secure:   Supports both plaintext and SASL_SSL configurations for secure message production.</p> </li> <li> <p>Callback and Delivery Reporting:   Includes a delivery report mechanism that logs successful and failed message deliveries.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/producers/#installation","title":"Installation","text":"<p>To add the Producers library to your monorepo, run:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-infra-producers --local\n</code></pre> <p>Ensure your environment includes all required dependencies, such as <code>confluent_kafka</code> and your logger library.</p>"},{"location":"reference/libs/ddd/infra/producers/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/infra/producers/#instantiating-the-kafka-producer","title":"Instantiating the Kafka Producer","text":"<p>Import and create an instance of the <code>KafkaProducerStrategy</code> with the necessary configuration:</p> <pre><code>from producers.infra.producers.kafka.producer import KafkaProducerStrategy\n\n# Initialize the Kafka producer with bootstrap servers and, optionally, SASL credentials.\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",  # Optional\n    kafka_password=\"your_password\"   # Optional\n)\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/#producing-messages","title":"Producing Messages","text":"<p>To produce a message to a Kafka topic, use the <code>produce</code> method. The message value should be a dictionary that will be JSON-encoded before sending:</p> <pre><code>topic = \"my_topic\"\nkey = \"unique_key\"\nvalue = {\"event\": \"user_signup\", \"user_id\": 12345}\n\n# Produce a message to the specified topic.\nkafka_producer.produce(topic, key, value)\n</code></pre> <p>The producer immediately polls for delivery events and logs the outcome using the configured callback.</p>"},{"location":"reference/libs/ddd/infra/producers/#flushing-the-producer","title":"Flushing the Producer","text":"<p>When you need to ensure that all queued messages have been sent, call the <code>flush</code> method:</p> <pre><code># Flush any remaining messages with a timeout (in seconds).\nkafka_producer.flush(timeout=30)\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/#delivery-reporting","title":"Delivery Reporting","text":"<p>The producer's delivery report callback logs whether messages are successfully delivered or if they encountered errors. You can customize this behavior by extending or overriding the default callback.</p>"},{"location":"reference/libs/ddd/infra/producers/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Producer Configuration:   The Kafka producer is configured with options for batching, compression, and buffering. By default, it uses gzip compression and a client ID of \"emulator-producer\".</p> </li> <li> <p>If SASL credentials are provided, the producer is configured to use SASL_SSL.</p> </li> <li> <p>Otherwise, the producer uses PLAINTEXT.</p> </li> <li> <p>Callback Function:   The <code>delivery_report</code> method is used as a callback for reporting message delivery status. Successful deliveries and errors are both logged for troubleshooting and auditing.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/producers/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that all functionality works as expected. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test ddd-infra-producers\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/code_reference/producers/base_producer/","title":"Base producer","text":""},{"location":"reference/libs/ddd/infra/storage/","title":"Storage","text":"<p>The Storage Library provides a unified interface for interacting with bucket-based storage systems. Currently, it includes an implementation for Minio via the <code>MinioStorageClient</code>. The design is extensible so that future implementations (e.g., AWS S3, Google Cloud Storage) can be added seamlessly by conforming to the <code>StorageClient</code> interface.</p>"},{"location":"reference/libs/ddd/infra/storage/#features","title":"Features","text":"<ul> <li>Unified Interface:   Implements a common interface (<code>StorageClient</code>) for storage operations, ensuring consistent usage across different providers.</li> <li> <p>Bucket Operations:   Supports creating buckets, listing buckets, and listing objects within buckets.</p> </li> <li> <p>Object Operations:   Provides methods to upload files or raw bytes (or BytesIO data) and to download files either as local files or as bytes.</p> </li> <li> <p>URI Generation:   Generates URIs for accessing stored objects, making integration with external systems easier.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/storage/#installation","title":"Installation","text":"<p>Add the Storage library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-infra-storage --local\n</code></pre> <p>Ensure that your environment has all required dependencies, including <code>minio</code> and your logger library.</p>"},{"location":"reference/libs/ddd/infra/storage/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/infra/storage/#instantiating-the-minio-storage-client","title":"Instantiating the Minio Storage Client","text":"<p>To begin using the Minio storage client, import and instantiate <code>MinioStorageClient</code> with the appropriate connection parameters:</p> <pre><code>from storage.minio.storage import MinioStorageClient\n\n# Initialize the client with your Minio endpoint and credentials.\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False\n)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#creating-a-bucket","title":"Creating a Bucket","text":"<p>Create a new bucket on the Minio server:</p> <pre><code>bucket_name = \"my_bucket\"\nminio_client.create_bucket(bucket_name)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#listing-buckets","title":"Listing Buckets","text":"<p>Retrieve a list of all buckets available on the Minio server:</p> <pre><code>buckets = minio_client.list_buckets()\nprint(\"Buckets:\", buckets)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#uploading-files-and-bytes","title":"Uploading Files and Bytes","text":""},{"location":"reference/libs/ddd/infra/storage/#upload-a-file","title":"Upload a File","text":"<p>Upload a local file to a specified bucket and obtain its URI:</p> <pre><code>uri = minio_client.upload_file(\"my_bucket\", \"example.txt\", \"path/to/local/file.txt\")\nprint(\"File uploaded to:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#upload-raw-bytes","title":"Upload Raw Bytes","text":"<p>Upload data as bytes (or using a BytesIO instance) and get its URI:</p> <pre><code># Upload using raw bytes\nuri = minio_client.upload_bytes(\"my_bucket\", \"data.txt\", b\"Sample data\")\nprint(\"Data uploaded to:\", uri)\n\n# Upload using a BytesIO instance\nfrom io import BytesIO\nbytes_io = BytesIO(b\"Sample data from BytesIO\")\nuri = minio_client.upload_bytes(\"my_bucket\", \"data_bytesio.txt\", bytes_io)\nprint(\"Data uploaded to:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#downloading-files-and-bytes","title":"Downloading Files and Bytes","text":""},{"location":"reference/libs/ddd/infra/storage/#download-a-file-locally","title":"Download a File Locally","text":"<p>Download an object from a bucket and save it to a local file:</p> <pre><code>minio_client.download_file(\"my_bucket\", \"example.txt\", \"path/to/save/example.txt\")\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#download-as-bytes","title":"Download as Bytes","text":"<p>Download an object from a bucket and obtain its data as bytes:</p> <pre><code>data = minio_client.download_file_as_bytes(\"my_bucket\", \"example.txt\")\nprint(\"Downloaded data:\", data)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#listing-objects-in-a-bucket","title":"Listing Objects in a Bucket","text":"<p>Retrieve a list of object names stored in a specific bucket:</p> <pre><code>objects = minio_client.list_objects(\"my_bucket\")\nprint(\"Objects in bucket:\", objects)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#generating-object-uris","title":"Generating Object URIs","text":"<p>Generate a URI to access an object stored in a bucket:</p> <pre><code>uri = minio_client.get_uri(\"my_bucket\", \"example.txt\")\nprint(\"Access URI:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Minio Connection:   The client is configured with the Minio server\u2019s endpoint and credentials. Use the <code>secure</code> flag to toggle between HTTP and HTTPS.</p> </li> <li> <p>Logging:   All operations log detailed messages using the integrated logger. Adjust or replace the logger if needed.</p> </li> <li> <p>Extensibility:   The library is designed around the <code>StorageClient</code> interface. Future implementations for other storage providers can be added with minimal changes to the application code.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/storage/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that all functionality works as expected. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test ddd-infra-storage\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/","title":"Bucket storage","text":""},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient","title":"<code>StorageClient</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>class StorageClient(ABC):\n    @abstractmethod\n    def create_bucket(self, bucket_name: str) -&gt; None:\n        \"\"\"Create a new bucket.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_buckets(self) -&gt; List[str]:\n        \"\"\"List all buckets available.\"\"\"\n        pass\n\n    @abstractmethod\n    def upload_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; str:\n        \"\"\"Upload a file to a bucket and return its URI.\"\"\"\n        pass\n\n    @abstractmethod\n    def upload_bytes(\n        self, bucket_name: str, object_name: str, bytes_data: bytes\n    ) -&gt; str:\n        \"\"\"Upload bytes data to a bucket and return its URI.\"\"\"\n        pass\n\n    @abstractmethod\n    def download_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; None:\n        \"\"\"Download an object from a bucket and save it locally.\"\"\"\n        pass\n\n    @abstractmethod\n    def download_file_as_bytes(self, bucket_name: str, object_name: str) -&gt; bytes:\n        \"\"\"Download an object from a bucket and return its data as bytes.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_objects(self, bucket_name: str) -&gt; List[str]:\n        \"\"\"List object names in the specified bucket.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_uri(self, bucket_name: str, object_name: str) -&gt; str:\n        \"\"\"Generate a URI to access an object.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.create_bucket","title":"<code>create_bucket(bucket_name)</code>  <code>abstractmethod</code>","text":"<p>Create a new bucket.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef create_bucket(self, bucket_name: str) -&gt; None:\n    \"\"\"Create a new bucket.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.list_buckets","title":"<code>list_buckets()</code>  <code>abstractmethod</code>","text":"<p>List all buckets available.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef list_buckets(self) -&gt; List[str]:\n    \"\"\"List all buckets available.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.upload_file","title":"<code>upload_file(bucket_name, object_name, file_path)</code>  <code>abstractmethod</code>","text":"<p>Upload a file to a bucket and return its URI.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef upload_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; str:\n    \"\"\"Upload a file to a bucket and return its URI.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.upload_bytes","title":"<code>upload_bytes(bucket_name, object_name, bytes_data)</code>  <code>abstractmethod</code>","text":"<p>Upload bytes data to a bucket and return its URI.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef upload_bytes(\n    self, bucket_name: str, object_name: str, bytes_data: bytes\n) -&gt; str:\n    \"\"\"Upload bytes data to a bucket and return its URI.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.download_file","title":"<code>download_file(bucket_name, object_name, file_path)</code>  <code>abstractmethod</code>","text":"<p>Download an object from a bucket and save it locally.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef download_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; None:\n    \"\"\"Download an object from a bucket and save it locally.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.download_file_as_bytes","title":"<code>download_file_as_bytes(bucket_name, object_name)</code>  <code>abstractmethod</code>","text":"<p>Download an object from a bucket and return its data as bytes.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef download_file_as_bytes(self, bucket_name: str, object_name: str) -&gt; bytes:\n    \"\"\"Download an object from a bucket and return its data as bytes.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.list_objects","title":"<code>list_objects(bucket_name)</code>  <code>abstractmethod</code>","text":"<p>List object names in the specified bucket.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef list_objects(self, bucket_name: str) -&gt; List[str]:\n    \"\"\"List object names in the specified bucket.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.get_uri","title":"<code>get_uri(bucket_name, object_name)</code>  <code>abstractmethod</code>","text":"<p>Generate a URI to access an object.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef get_uri(self, bucket_name: str, object_name: str) -&gt; str:\n    \"\"\"Generate a URI to access an object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/","title":"Value Objects","text":"<p>The Value Objects Library provides a set of immutable objects to represent domain values consistently and reliably. It includes the <code>EmulationID</code> value object, which encapsulates a unique identifier for emulation instances using a UUID format.</p>"},{"location":"reference/libs/ddd/value-objects/#features","title":"Features","text":"<ul> <li>Immutable Data Structures: Uses Python\u2019s <code>@dataclass(frozen=True)</code> to ensure immutability.</li> <li>UUID Validation: Automatically validates that any provided identifier conforms to the UUID format.</li> <li>Convenience Method: Includes a class method to generate a new, valid <code>EmulationID</code>.</li> </ul>"},{"location":"reference/libs/ddd/value-objects/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name ddd-value-objects --local\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/value-objects/#creating-an-emulationid","title":"Creating an EmulationID","text":"<p>Instantiate an <code>EmulationID</code> with a valid UUID string:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\n# Create an EmulationID using a valid UUID string\nemulation_id = EmulationID(\"123e4567-e89b-12d3-a456-426614174000\")\nprint(emulation_id.value)\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#generating-a-new-emulationid","title":"Generating a New EmulationID","text":"<p>Generate a new unique <code>EmulationID</code> using the provided class method:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\n# Generate a new EmulationID\nnew_id = EmulationID.generate()\nprint(new_id.value)\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#handling-invalid-ids","title":"Handling Invalid IDs","text":"<p>If an invalid UUID is provided, the class raises a <code>ValueError</code> during initialization:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\ntry:\n    invalid_id = EmulationID(\"invalid-uuid\")\nexcept ValueError as e:\n    print(e)  # Output: \"Invalid EmulationID: invalid-uuid\"\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that the value objects behave as expected. To run the tests, navigate to the <code>libs/shared/value-objects</code> directory and execute:</p> <pre><code>npx nx test ddd-value-objects\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/","title":"Emulator id","text":""},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID","title":"<code>EmulationID</code>  <code>dataclass</code>","text":"<p>Value Object representing an emulation's unique identifier.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>@dataclass(frozen=True)\nclass EmulationID:\n    \"\"\"Value Object representing an emulation's unique identifier.\"\"\"\n\n    value: str\n\n    def __post_init__(self):\n        \"\"\"Ensure the ID is a valid UUID.\"\"\"\n        try:\n            uuid.UUID(self.value)  # Validate UUID format\n        except ValueError:\n            raise ValueError(f\"Invalid EmulationID: {self.value}\") from None\n\n    @classmethod\n    def generate(cls) -&gt; \"EmulationID\":\n        \"\"\"Generate a new AgentID.\"\"\"\n        return cls(value=str(uuid.uuid4()))\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Ensure the ID is a valid UUID.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure the ID is a valid UUID.\"\"\"\n    try:\n        uuid.UUID(self.value)  # Validate UUID format\n    except ValueError:\n        raise ValueError(f\"Invalid EmulationID: {self.value}\") from None\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID.generate","title":"<code>generate()</code>  <code>classmethod</code>","text":"<p>Generate a new AgentID.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>@classmethod\ndef generate(cls) -&gt; \"EmulationID\":\n    \"\"\"Generate a new AgentID.\"\"\"\n    return cls(value=str(uuid.uuid4()))\n</code></pre>"},{"location":"reference/libs/fake-factory/","title":"Fake Factory","text":"<p>The Fake Factory Library provides a collection of data generation factories designed to simulate realistic, synthetic data for testing, development, and feature store ingestion. Built on top of the Faker library, it offers a flexible base factory and specialized implementations that generate fraud-related records, such as device logs, transactions, and user profiles.</p>"},{"location":"reference/libs/fake-factory/#features","title":"Features","text":"<ul> <li> <p>Extensible Base Factory:   The library defines a <code>BaseFakeFactory</code> interface that can be extended to create new factories with custom logic.</p> </li> <li> <p>Realistic Data Generation:   Leveraging the Faker library, each factory produces synthetic yet realistic data, including names, emails, timestamps, and more.</p> </li> <li> <p>Fraud Simulation:   Specialized factories (e.g., for device logs, transactions, and user profiles) implement fraud rules to simulate various risk scenarios, enabling testing of fraud detection systems and data pipelines.</p> </li> <li> <p>Configurability:   Factories include configurable parameters (e.g., user ID ranges) and randomness to mimic real-world variability and edge cases.</p> </li> </ul>"},{"location":"reference/libs/fake-factory/#installation","title":"Installation","text":"<p>Add the Fake-Factory library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name fake-factory --local\n</code></pre> <p>Ensure that your environment includes the required dependencies (e.g., Faker) as defined in the <code>pyproject.toml</code> and <code>poetry.toml</code> files.</p>"},{"location":"reference/libs/fake-factory/#usage","title":"Usage","text":""},{"location":"reference/libs/fake-factory/#base-factory","title":"Base Factory","text":"<p>The <code>BaseFakeFactory</code> is the abstract base class that all specific factories extend. It defines the contract for generating fake records.</p> <pre><code>from fake_factory.base_factory import BaseFakeFactory\n\nclass MyFactory(BaseFakeFactory):\n    def generate(self) -&gt; dict[str, Any]:\n        # Implement record generation logic here.\n        return {\"dummy\": \"data\"}\n</code></pre>"},{"location":"reference/libs/fake-factory/#device-log-factory","title":"Device Log Factory","text":"<p>The <code>DeviceLogFactory</code> generates fake device log records, including unique log IDs, device details, and timestamps. For example:</p> <pre><code>from fake_factory.fraud.device_factory import DeviceLogFactory\n\nfactory = DeviceLogFactory()\ndevice_log = factory.generate()\nprint(device_log)\n</code></pre>"},{"location":"reference/libs/fake-factory/#transaction-factory","title":"Transaction Factory","text":"<p>The <code>TransactionFakeFactory</code> simulates transaction data with optional fraud labeling. It applies conditional fraud rules to raw transaction data for realistic risk simulation:</p> <pre><code>from fake_factory.fraud.transaction_factory import TransactionFakeFactory\n\ntransaction_factory = TransactionFakeFactory()\ntransaction = transaction_factory.generate(with_label=True)\nprint(transaction)\n</code></pre>"},{"location":"reference/libs/fake-factory/#user-profile-factory","title":"User Profile Factory","text":"<p>The <code>UserProfileFactory</code> generates user profiles with unique IDs and calculates a risk level based on factors such as credit score, signup date, and country:</p> <pre><code>from fake_factory.fraud.user_profile_factory import UserProfileFactory\n\nprofile_factory = UserProfileFactory()\nuser_profile = profile_factory.generate()\nprint(user_profile)\n</code></pre>"},{"location":"reference/libs/fake-factory/#configuration-details","title":"Configuration Details","text":"<ul> <li>Data Realism:   Each factory uses the Faker library to produce data that closely resembles real-world records.</li> <li>Fraud Rules:   The specialized fraud factories include multiple conditional branches to simulate different risk scenarios. For example, the TransactionFakeFactory can label transactions as fraudulent based on user compromise, card testing, or geographical anomalies.</li> <li>User ID Management:   The UserProfileFactory maintains a thread-safe queue of user IDs to ensure unique identifiers across generated profiles.</li> </ul>"},{"location":"reference/libs/fake-factory/#testing","title":"Testing","text":"<p>Unit tests are provided to validate the functionality and consistency of the data generators. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test fake-factory\n</code></pre>"},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/","title":"Base factory","text":""},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/#libs.fake-factory.fake_factory.base_factory.BaseFakeFactory","title":"<code>BaseFakeFactory</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>libs/fake-factory/fake_factory/base_factory.py</code> <pre><code>class BaseFakeFactory(ABC):\n    @abstractmethod\n    def generate(self) -&gt; dict[str, Any]:\n        \"\"\"Generates a fake data record.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/#libs.fake-factory.fake_factory.base_factory.BaseFakeFactory.generate","title":"<code>generate()</code>  <code>abstractmethod</code>","text":"<p>Generates a fake data record.</p> Source code in <code>libs/fake-factory/fake_factory/base_factory.py</code> <pre><code>@abstractmethod\ndef generate(self) -&gt; dict[str, Any]:\n    \"\"\"Generates a fake data record.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/settings/emulator-settings/","title":"settings-emulator-settings","text":"<p>Project description here.</p>"},{"location":"reference/libs/settings/emulator-settings/code_reference/emulator_settings/settings/","title":"Settings","text":""},{"location":"reference/libs/shared/cliargs/","title":"CLI Arguments Parser","text":"<p>The CLI Arguments Parser Library provides a convenience function to generate a standard <code>argparse.ArgumentParser</code> with common command-line options such as verbose output, debug mode, logging level, and version information. It is designed to simplify the creation of consistent CLI interfaces across multiple applications.</p>"},{"location":"reference/libs/shared/cliargs/#features","title":"Features","text":"<ul> <li>Standard CLI Options: Pre-configured with commonly used command-line options.</li> <li>Consistent Interface: Ensures a uniform command-line interface for your applications.</li> <li>Ease of Integration: Quickly integrate the parser into any project with minimal configuration.</li> </ul>"},{"location":"reference/libs/shared/cliargs/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name shared-cliargs --local\n</code></pre>"},{"location":"reference/libs/shared/cliargs/#usage","title":"Usage","text":""},{"location":"reference/libs/shared/cliargs/#basic-setup","title":"Basic Setup","text":"<p>To create a standard argument parser for your application, import the <code>new_args_parser</code> function:</p> <pre><code>from cliargs.cliargs import new_args_parser\n\n# Create the argument parser with a brief description of your application\nparser = new_args_parser(\"Description of your application\")\nargs = parser.parse_args()\n\nif args.verbose:\n    print(\"Verbose mode enabled\")\n</code></pre>"},{"location":"reference/libs/shared/cliargs/#standard-cli-options","title":"Standard CLI Options","text":"<p>The parser includes the following command-line options:</p> <ul> <li><code>--verbose</code>: Enable verbose output.</li> <li><code>--debug</code>: Activate debug mode with detailed logging.</li> <li><code>--log-level</code>: Set the logging level. Acceptable values are \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".</li> <li><code>--version</code>: Display the application's version and exit.</li> </ul>"},{"location":"reference/libs/shared/cliargs/#configuration-details","title":"Configuration Details","text":"<p>The <code>new_args_parser</code> function initializes an <code>argparse.ArgumentParser</code> with the provided description and adds the standard CLI options listed above. You can easily extend the parser with additional arguments as required by your application.</p>"},{"location":"reference/libs/shared/cliargs/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure the argument parser functions correctly. To run the tests, navigate to the <code>libs/shared/cliargs</code> directory and execute:</p> <pre><code>npx nx test shared-cliargs\n</code></pre>"},{"location":"reference/libs/shared/cliargs/code_reference/cliargs/cli/","title":"Cli","text":"<p>Module for Creating a Standard Argument Parser</p> <p>This module provides a convenience function to generate an argparse.ArgumentParser instance with common command-line options such as verbose output, debug mode, logging level, and version information. It is designed to simplify the creation of consistent CLI interfaces across multiple applications.</p>"},{"location":"reference/libs/shared/cliargs/code_reference/cliargs/cli/#libs.shared.cliargs.cliargs.cli.new_args_parser","title":"<code>new_args_parser(description)</code>","text":"<p>Create and return an ArgumentParser with standard CLI options.</p> <p>This function initializes an argparse.ArgumentParser with the provided description and adds the following command-line arguments:</p> <pre><code>--verbose:\n    Enable verbose output.\n\n--debug:\n    Activate debug mode with detailed logging.\n\n--log-level:\n    Set the logging level. Acceptable values are \"DEBUG\", \"INFO\",\n    \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".\n\n--version:\n    Display the application's version and exit. The version is\n    hardcoded as \"0.1.0\".\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A brief description of the application, which is displayed in the help message.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>argparse.ArgumentParser: An ArgumentParser object configured with the</p> <code>ArgumentParser</code> <p>standard command-line arguments.</p> Source code in <code>libs/shared/cliargs/cliargs/cli.py</code> <pre><code>def new_args_parser(description: str) -&gt; argparse.ArgumentParser:\n    \"\"\"Create and return an ArgumentParser with standard CLI options.\n\n    This function initializes an argparse.ArgumentParser with the provided\n    description and adds the following command-line arguments:\n\n        --verbose:\n            Enable verbose output.\n\n        --debug:\n            Activate debug mode with detailed logging.\n\n        --log-level:\n            Set the logging level. Acceptable values are \"DEBUG\", \"INFO\",\n            \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".\n\n        --version:\n            Display the application's version and exit. The version is\n            hardcoded as \"0.1.0\".\n\n    Args:\n        description (str): A brief description of the application, which is\n            displayed in the help message.\n\n    Returns:\n        argparse.ArgumentParser: An ArgumentParser object configured with the\n        standard command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output.\")\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Activate debug mode with detailed logging.\",\n    )\n    parser.add_argument(\n        \"--log-level\",\n        type=str,\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Set the logging level. Defaults to INFO.\",\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s 0.1.0\",\n        help=\"Show the application's version and exit.\",\n    )\n    return parser\n</code></pre>"},{"location":"reference/libs/shared/logger/","title":"Logger","text":"<p>The Logger Library provides a simple and effective way to configure logging for your Python applications. It uses JSON formatting for log messages, making it easier to integrate with log management systems and enabling structured logging.</p>"},{"location":"reference/libs/shared/logger/#features","title":"Features","text":"<ul> <li>JSON Formatted Logging: Logs are output in a structured JSON format.</li> <li>Configurable Log Levels: Easily set the desired log level via function parameters or environment variables.</li> <li>Modular Integration: Quickly integrate the logger into any module by providing the module name.</li> <li>Propagation Control: Option to propagate log messages to parent loggers.</li> </ul>"},{"location":"reference/libs/shared/logger/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name shared-logger --local\n</code></pre>"},{"location":"reference/libs/shared/logger/#usage","title":"Usage","text":""},{"location":"reference/libs/shared/logger/#basic-setup","title":"Basic Setup","text":"<p>To set up logging with a custom module name, import the <code>setup_logging</code> function:</p> <pre><code>from logger.log import setup_logging\n\n# Create a logger for your module\nlogger = setup_logging(__name__)\n\n# Log an informational message\nlogger.info(\"This is an info message.\")\n</code></pre>"},{"location":"reference/libs/shared/logger/#environment-based-setup","title":"Environment-Based Setup","text":"<p>You can also create a logger that automatically reads the log level from the <code>LOG_LEVEL</code> environment variable. Use the <code>get_logger_from_env</code> function as follows:</p> <pre><code>from logger.log import get_logger_from_env\n\n# Create a logger that uses the LOG_LEVEL environment variable\nlogger = get_logger_from_env(__name__)\n\n# Log a debug message (if LOG_LEVEL is set to DEBUG)\nlogger.debug(\"This is a debug message.\")\n</code></pre> <p>Before running your application, set the <code>LOG_LEVEL</code> environment variable if needed:</p> <pre><code>export LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/libs/shared/logger/#configuration-details","title":"Configuration Details","text":"<ul> <li>JSON Formatter:   The logger uses the following format for JSON logs:</li> </ul> <pre><code>%(levelname)s %(filename)s %(message)s\n</code></pre> <p>You can modify this format directly in the code by adjusting the formatter setup if a different structure is required.</p> <ul> <li>Propagation:   The <code>setup_logging</code> function accepts a <code>propagate</code> parameter. Set it to <code>True</code> if you want log messages to propagate to the parent logger.</li> </ul>"},{"location":"reference/libs/shared/logger/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that the logger functions correctly. To run the tests, navigate to the <code>libs/shared/logger</code> directory and execute:</p> <pre><code>npx nx test shared-logger\n</code></pre>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/","title":"Log","text":"<p>Logging module.</p>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/#libs.shared.logger.logger.log.setup_logging","title":"<code>setup_logging(module_name, propagate=False, log_level=os.getenv('LOG_LEVEL', 'INFO').upper())</code>","text":"<p>Set up logging using JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The module name.</p> required <code>propagate</code> <code>bool</code> <p>Whether to propagate the logging to the parent logger.</p> <code>False</code> <code>log_level</code> <code>str</code> <p>The log level.</p> <code>upper()</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger.</p> Source code in <code>libs/shared/logger/logger/log.py</code> <pre><code>def setup_logging(\n    module_name: str,\n    propagate: bool = False,\n    log_level: str = os.getenv(\"LOG_LEVEL\", \"INFO\").upper(),\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up logging using JSON format.\n\n    Args:\n        module_name (str): The module name.\n        propagate (bool): Whether to propagate the logging to the parent logger.\n        log_level (str): The log level.\n\n    Returns:\n        The logger.\n    \"\"\"\n    log_handler = logging.StreamHandler()\n    formatter = json.JsonFormatter(\"%(levelname)s %(filename)s %(message)s\")\n    log_handler.setFormatter(formatter)\n\n    logger = logging.getLogger(module_name)\n    logger.addHandler(log_handler)\n    logger.propagate = propagate\n    logger.setLevel(logging.getLevelName(log_level))\n    return logger\n</code></pre>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/#libs.shared.logger.logger.log.get_logger_from_env","title":"<code>get_logger_from_env(module_name)</code>","text":"<p>Get a logger using the <code>LOG_LEVEL</code> environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The module name.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>The logger.</p> Source code in <code>libs/shared/logger/logger/log.py</code> <pre><code>def get_logger_from_env(module_name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger using the `LOG_LEVEL` environment variable.\n\n    Args:\n        module_name (str): The module name.\n\n    Returns:\n        The logger.\n    \"\"\"\n    log_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n    return setup_logging(module_name, log_level=log_level)\n</code></pre>"},{"location":"reference/services/data-emulator/","title":"Data Emulator Service","text":"<p>The Data Emulator Service orchestrates the emulation of data ingestion workflows. It provides a REST API\u2014built on FastAPI\u2014that enables external clients to trigger and manage emulator processes. The service integrates configuration management, logging, and resource setup (using Kafka and Minio) to simulate realistic data production. It leverages asynchronous background tasks and graceful signal handling for robust operation.</p>"},{"location":"reference/services/data-emulator/#overview","title":"Overview","text":"<p>The Data Emulator Service is designed to generate synthetic data for testing, development, and integration purposes. It consists of a primary process that:</p> <ul> <li>Parses command-line arguments using a custom CLI argument parser.</li> <li>Loads configuration settings via a custom <code>Settings</code> class.</li> <li>Sets up centralized logging.</li> <li>Spawns a child process to run the REST API server (using Uvicorn).</li> </ul> <p>The service uses an asynchronous signal handler to listen for termination signals (such as SIGINT and SIGTERM), ensuring that both the REST API server and any running tasks are shutdown gracefully.</p>"},{"location":"reference/services/data-emulator/#features","title":"Features","text":"<ul> <li> <p>RESTful API Interface:   Provides endpoints to interact with emulator use cases, allowing external triggering of long-running data emulation tasks.</p> </li> <li> <p>Modular Architecture:   Separates concerns by delegating configuration, logging, and resource initialization to dedicated modules. The REST API is run in an isolated child process.</p> </li> <li> <p>Configuration and Dependency Injection:   Uses a custom <code>Settings</code> class to configure external resources (Kafka, Minio) and logging. Command-line arguments control verbosity, debug mode, and log levels.</p> </li> <li> <p>Asynchronous Background Tasks:   Emulation tasks are scheduled asynchronously using FastAPI\u2019s BackgroundTasks, ensuring that API responses remain quick while the heavy lifting occurs in the background.</p> </li> <li> <p>Graceful Shutdown Handling:   A dedicated signal handler captures termination signals and coordinates the proper shutdown of the REST API process and any background tasks.</p> </li> </ul>"},{"location":"reference/services/data-emulator/#architecture","title":"Architecture","text":"<p>The service\u2019s main entry point performs the following steps:</p> <ol> <li> <p>Setup Service:</p> </li> <li> <p>Parses CLI arguments using <code>new_args_parser</code>.</p> </li> <li>Loads configuration settings from both CLI parameters and environment variables.</li> <li> <p>Sets up logging and propagates the log level to the application\u2019s environment.</p> </li> <li> <p>Start REST API Server:</p> </li> <li> <p>Spawns a child process to run the FastAPI REST server with Uvicorn.</p> </li> <li> <p>The REST API, which is built in a separate module (<code>api.emulator_rest_api</code>), receives the configuration via the app state.</p> </li> <li> <p>Signal Handling:</p> </li> <li> <p>Instantiates a <code>SignalHandler</code> that registers for termination signals.</p> </li> <li> <p>Asynchronously waits for a shutdown event; upon receiving a signal, it terminates the REST API child process and cleans up resources.</p> </li> <li> <p>Shutdown:</p> </li> <li>The main process joins the REST API process and exits gracefully.</li> </ol>"},{"location":"reference/services/data-emulator/#usage","title":"Usage","text":"<p>To build the current image for the Data Emulator Service, use:</p> <pre><code>npx nx image service-data-emulator\n</code></pre>"},{"location":"reference/services/data-emulator/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The service supports CLI arguments (via the custom <code>new_args_parser</code>) for configuring log levels, verbosity, and debug modes. For example:</p> <pre><code>python main.py --log-level DEBUG --verbose\n</code></pre>"},{"location":"reference/services/data-emulator/#environment-configuration","title":"Environment Configuration","text":"<p>The service reads external resource settings (e.g., Kafka, Minio) from environment variables as well as the provided CLI arguments. For instance:</p> <ul> <li>Kafka:</li> <li><code>KAFKA_BOOTSTRAP_SERVERS</code></li> <li><code>KAFKA_USERNAME</code></li> <li><code>KAFKA_PASSWORD</code></li> <li>Minio:</li> <li><code>MINIO_ENDPOINT</code></li> <li><code>MINIO_ACCESS_KEY</code></li> <li><code>MINIO_SECRET_KEY</code></li> <li><code>MINIO_SECURE</code></li> </ul> <p>These values are loaded into the custom <code>Settings</code> class and assigned to the FastAPI app state before the REST API process starts.</p>"},{"location":"reference/services/data-emulator/#running-the-rest-api","title":"Running the REST API","text":"<p>The REST API server is launched as a child process using Uvicorn:</p> <pre><code>rest_app.state.config = config\nuvicorn.run(rest_app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n</code></pre> <p>This isolates the API layer while enabling the main process to manage service lifecycle and signal handling.</p>"},{"location":"reference/services/data-emulator/#signal-handling-and-graceful-shutdown","title":"Signal Handling and Graceful Shutdown","text":"<p>The service registers signal handlers for SIGINT and SIGTERM using an asynchronous <code>SignalHandler</code>. When a termination signal is received:</p> <ul> <li>The shutdown event is triggered.</li> <li>The REST API child process is terminated and joined.</li> <li>Any background tasks are allowed to complete, and resources are released before the service exits.</li> </ul>"},{"location":"reference/services/data-emulator/#logging","title":"Logging","text":"<p>The service configures logging early in the startup sequence. It sets the <code>LOG_LEVEL</code> environment variable and uses a custom logging setup to propagate messages. Verbose and debug modes are supported via CLI arguments.</p>"},{"location":"reference/services/data-emulator/#testing","title":"Testing","text":"<p>Unit tests are provided for the various modules including CLI argument parsing, configuration loading, REST API endpoints, and signal handling. To run the tests, execute:</p> <pre><code>npx nx test services-data-emulator\n</code></pre>"}]}