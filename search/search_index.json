{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lakehouse Lab: Data Emulator &amp; Pipeline Architecture Documentation","text":"<p>Lakehouse Lab is an open, extensible platform designed to simulate realistic data ingestion workflows. It enables users to generate synthetic data across multiple domains, stream data into storage systems, and process it through data transformation pipelines. This documentation details the system's architecture, components, data flows, and relational models.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>System Architecture Overview</li> <li>Data Factories<ul> <li>DeviceLogFactory</li> <li>TransactionFakeFactory</li> <li>UserProfileFactory</li> </ul> </li> <li>API Workflow &amp; Data Ingestion</li> <li>Orchestration and Spark Processing</li> <li>Diagrams<ul> <li>Sequence Diagram</li> <li>ER Diagram (Raw Data Model)</li> </ul> </li> </ol>"},{"location":"#introduction","title":"Introduction","text":"<p>Lakehouse Lab emulates a robust data ingestion and processing environment for modern data architectures. The platform simulates the data production lifecycle\u2014from synthetic record generation to advanced processing\u2014facilitating testing, development, and analytics at scale. The solution is built using modern technologies such as Kafka, Minio, Apache Airflow, and Apache Spark.</p>"},{"location":"#system-architecture-overview","title":"System Architecture Overview","text":"<p>Lakehouse Lab\u2019s architecture is segmented into three primary areas:</p> <ul> <li>Data Generation: Synthetic data is produced using dedicated data factories, each targeting a specific domain (e.g., user profiles, transactions, device logs).</li> <li>Data Ingestion: The synthetic data is ingested into raw storage via either message brokers (Kafka) or object storage (Minio).</li> <li>Data Processing: Downstream processes orchestrated by Apache Airflow trigger Apache Spark jobs. These jobs transform raw data into analytics-ready layers (Bronze, Silver, and Gold).</li> </ul> <p>This modular design ensures extensibility and ease of maintenance while replicating realistic data ingestion workflows.</p>"},{"location":"#data-factories","title":"Data Factories","text":"<p>Each factory is responsible for generating realistic synthetic data for a specific domain. All factories expose similar metadata so that downstream processes can treat data uniformly.</p>"},{"location":"#devicelogfactory","title":"DeviceLogFactory","text":"<ul> <li>Purpose:   Generates synthetic device log records capturing information such as user device details, login/logout timestamps, and network specifics.</li> <li>Key Fields:</li> <li><code>log_id</code></li> <li><code>user_id</code></li> <li><code>device_id</code></li> <li><code>device_type</code></li> <li><code>os</code></li> <li><code>ip_address</code></li> <li><code>location</code></li> <li><code>user_agent</code></li> <li><code>login_timestamp</code></li> <li><code>logout_timestamp</code> (optional)</li> </ul>"},{"location":"#transactionfakefactory","title":"TransactionFakeFactory","text":"<ul> <li>Purpose:   Produces synthetic financial transaction data, including optional fraud indicators.</li> <li>Key Fields:</li> <li><code>transaction_id</code></li> <li><code>user_id</code></li> <li><code>amount</code></li> <li><code>currency</code></li> <li><code>merchant</code></li> <li><code>timestamp</code></li> <li><code>location</code></li> <li><code>is_fraud</code> (optional)</li> <li>Fraud Rules:   Incorporates logic based on user compromise, card testing, merchant collusion, geographical anomalies, and an inherent fraud probability.</li> </ul>"},{"location":"#userprofilefactory","title":"UserProfileFactory","text":"<ul> <li>Purpose:   Generates synthetic user profiles, incorporating demographic and risk assessment data.</li> <li>Key Fields:</li> <li><code>user_id</code></li> <li><code>name</code></li> <li><code>email</code></li> <li><code>phone</code></li> <li><code>date_of_birth</code></li> <li><code>address</code></li> <li><code>country</code></li> <li><code>signup_date</code></li> <li><code>credit_score</code></li> <li><code>risk_level</code></li> </ul>"},{"location":"#api-workflow-data-ingestion","title":"API Workflow &amp; Data Ingestion","text":"<p>The Data Emulator Service provides a RESTful API to initiate data emulation. The following outlines the typical workflow:</p>"},{"location":"#request-initiation","title":"Request Initiation","text":"<p>An external client sends a POST request to the FastAPI endpoint. For example:</p> <pre><code>curl -X 'POST' 'http://localhost:8000/emulator/' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"emulator_sync\": \"minio\", \"emulation_domain\": \"user-profile\", \"timeout\": 120}'\n</code></pre>"},{"location":"#processing-steps","title":"Processing Steps","text":"<ol> <li> <p>Endpoint Invocation:    The FastAPI receives a <code>StartEmulatorDTO</code> payload with parameters such as the destination type (Kafka or Minio), the data domain, and an optional timeout.</p> </li> <li> <p>Factory Selection &amp; Task Scheduling:    Based on the <code>emulation_domain</code>, the correct data factory (e.g., <code>UserProfileFactory</code>) is selected. A background task is scheduled with the specified timeout.</p> </li> <li> <p>Data Generation &amp; Ingestion:    The factory continuously generates synthetic data:</p> </li> <li>For <code>emulator_sync</code> set to <code>\"kafka\"</code>: Data is published as JSON messages to Kafka.</li> <li> <p>For <code>emulator_sync</code> set to <code>\"minio\"</code>: Raw records are stored within a Minio bucket.</p> </li> <li> <p>Response Generation:    The service responds with an <code>EmulationScheduledDTO</code> that includes a unique emulation ID and execution details.</p> </li> <li> <p>Triggering Downstream Processing:    An Airflow operator may later trigger the API to indicate completion. Subsequently, Apache Spark jobs are dispatched to read, transform, and store the processed data.</p> </li> </ol>"},{"location":"#orchestration-and-spark-processing","title":"Orchestration and Spark Processing","text":""},{"location":"#airflow-orchestration","title":"Airflow Orchestration","text":"<ul> <li>Role:   Airflow is responsible for scheduling and managing the lifecycle of data emulation tasks. Once a task is completed, Airflow initiates subsequent Spark processing jobs.</li> </ul>"},{"location":"#apache-spark-processing","title":"Apache Spark Processing","text":"<ul> <li>Role:   Apache Spark is used for both batch and stream processing:</li> <li>Batch Jobs: Regularly process raw data stored in Minio or received via Kafka.</li> <li>Streaming Jobs: Continuously process incoming data (if required).</li> <li>Pipeline Layers:   The processed data is organized into multiple layers:</li> <li>Bronze: Raw ingested data.</li> <li>Silver: Cleaned and pre-aggregated data.</li> <li>Gold: Curated, analytics-ready datasets.</li> </ul>"},{"location":"#diagrams","title":"Diagrams","text":""},{"location":"#sequence-diagram","title":"Sequence Diagram","text":"<p>This diagram shows the interactions between system components during a typical data emulation workflow.</p> <p></p>"},{"location":"#er-diagram-raw-data-model","title":"ER Diagram (Raw Data Model)","text":"<p>The following ER diagram represents the relational structure of the raw data generated by the data factories.</p> <p></p>"},{"location":"getting-start/","title":"Getting Started with Lakehouse Lab","text":"<p>This guide will walk you through the prerequisites, installation, configuration, and usage of Lakehouse Lab.</p>"},{"location":"getting-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Node 23.5.0 or higher</li> <li>Golang 1.24 or higher</li> <li>Python 3.12 or higher</li> <li>Poetry 1.8.5 \u2013 a dependency management tool for Python</li> <li>Docker 27.4.0 or higher</li> </ul>"},{"location":"getting-start/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Clone the repository from GitHub and change to the project directory:</p> <pre><code>git clone https://github.com/FabioCaffarello/lakehouse-lab.git\ncd lakehouse-lab\n</code></pre>"},{"location":"getting-start/#2-set-up-the-virtual-environment","title":"2. Set Up the Virtual Environment","text":"<p>Set up the project with a single command:</p> <pre><code>make setup\n</code></pre> <p>This command will:</p> <ul> <li>Create a virtual environment in the <code>.venv</code> directory.</li> <li>Install all dependencies (including development and documentation extras).</li> <li>Set up pre-commit hooks for both commit and push stages.</li> </ul>"},{"location":"getting-start/#3-running-the-service","title":"3. Running the Service","text":"<p>To start the development server with auto-reload enabled, run:</p> <pre><code>make run\n</code></pre> <p>This command launches the Lakehouse Lab service, making the REST API available for testing and integration.</p>"},{"location":"getting-start/#4-running-tests","title":"4. Running Tests","text":"<p>To run the entire test suite, use the following command:</p> <pre><code>make check-all\n</code></pre> <p>This command executes all tests with <code>pytest</code> in the controlled environment and outputs the results along with a coverage report.</p>"},{"location":"getting-start/#5-linting-and-code-quality","title":"5. Linting and Code Quality","text":"<p>To keep your code clean and consistent, use these commands:</p> <ul> <li>Lint Code (using Ruff):</li> </ul> <pre><code>make lint\n</code></pre> <ul> <li>Run Pre-commit Hooks:</li> </ul> <pre><code>make precommit\n</code></pre>"},{"location":"getting-start/#6-documentation","title":"6. Documentation","text":""},{"location":"getting-start/#serve-documentation-locally","title":"Serve Documentation Locally","text":"<p>The project documentation is managed with MkDocs. To serve it locally with live reload, run:</p> <pre><code>make server-docs\n</code></pre> <p>Then visit http://127.0.0.1:8000 in your browser.</p>"},{"location":"getting-start/#deploy-documentation","title":"Deploy Documentation","text":"<p>To build and deploy the documentation to GitHub Pages:</p> <pre><code>make deploy-docs\n</code></pre> <p>This command builds the docs and pushes them to the appropriate branch for GitHub Pages hosting.</p>"},{"location":"getting-start/#7-additional-commands","title":"7. Additional Commands","text":"<p>For more tasks and a complete list of available commands, run:</p> <pre><code>make help\n</code></pre> <p>This will display a summary of all available Makefile targets and their descriptions.</p>"},{"location":"getting-start/#8-troubleshooting","title":"8. Troubleshooting","text":"<ul> <li>Virtual Environment Issues:   If you experience problems with the virtual environment, try clearing cached files by running:</li> </ul> <pre><code>make clean\nmake setup\n</code></pre> <ul> <li>Pre-commit Hooks Not Running:   If pre-commit hooks aren\u2019t working as expected, install them manually:</li> </ul> <pre><code>pre-commit install\n</code></pre> <ul> <li>Dependency Updates:   When updating dependencies in <code>pyproject.toml</code>, run the following commands to refresh the lock file:</li> </ul> <pre><code>npx nx reset\npoetry update\n</code></pre>"},{"location":"summary/","title":"Summary","text":"<ul> <li>Home</li> <li>Articles</li> <li>Services</li> <li>Libs</li> <li>Dependency Graph</li> <li>Getting Start</li> </ul>"},{"location":"papers/01-article/","title":"Lakehouse Lab: A Modern Blueprint for End-to-End Data Engineering","text":"<p>In today\u2019s rapidly evolving data landscape, organizations need more than just robust production systems\u2014they need a secure, realistic lab where innovation can thrive without jeopardizing live environments. Lakehouse Lab is that dynamic playground where data architects, engineers, and ML practitioners can simulate end-to-end data workflows, experiment with cutting-edge technologies, and build production-grade solutions\u2014all within a safe sandbox.</p>"},{"location":"papers/01-article/#motivation-and-project-objectives","title":"Motivation and Project Objectives","text":"<p>In this data-driven era, robust, scalable, and efficient architectures are no longer optional\u2014they\u2019re essential. Organizations must juggle vast historical datasets alongside a continuous stream of real-time information. Yet, finding a secure environment to test and refine these systems is a constant challenge. That\u2019s where Lakehouse Lab comes in.</p> <ul> <li> <p>Accelerated Skill Development:   By emulating enterprise-scale operations, Lakehouse Lab empowers you to explore innovative ingestion patterns, advanced data modeling, and sophisticated ETL strategies in a hands-on, real-world environment. This fast-tracks learning and deepens your expertise in today\u2019s dynamic data ecosystems.</p> </li> <li> <p>Authentic Data Simulations:   The platform creates synthetic data streams\u2014ranging from simulated financial transactions to virtual IoT telemetry\u2014that rigorously stress-test your pipelines for throughput, latency, and resilience. With Lakehouse Lab, you\u2019re always prepared for the unpredictable nature of live environments.</p> </li> <li> <p>Industry Best Practices in Action:   Leveraging top-tier tools such as Apache Spark, Apache Kafka, Delta Lake/Iceberg, and Nessie, Lakehouse Lab demonstrates not only how modern data lakehouses are built but also how to implement best practices in contemporary data engineering.</p> </li> </ul> <p>At its core, Lakehouse Lab is designed to empower you to experiment, iterate, and refine complex data pipelines in a risk-free setting\u2014transforming theoretical ideas into tangible, production-ready applications.</p>"},{"location":"papers/01-article/#a-360-journey-through-modern-data-engineering","title":"A 360\u00b0 Journey Through Modern Data Engineering","text":"<p>Lakehouse Lab guides you through every stage of the data lifecycle\u2014from ingestion to machine learning\u2014by combining rigorous theory with practical, hands-on demonstrations.</p> <ul> <li> <p>Multi-Mode Data Ingestion:   The platform supports a variety of ingestion methods. It seamlessly integrates traditional batch processes, using file systems and cloud storage solutions like Minio, with agile, near-real-time event streaming powered by Kafka. This dual approach builds resilient data pipelines capable of adapting to fluctuating workloads.</p> </li> <li> <p>Advanced ETL and Data Transformation:   With Apache Spark at its core, Lakehouse Lab introduces modern ETL techniques. Discover dynamic partitioning, efficient broadcasting, and intelligent shuffling\u2014all designed to ensure consistent performance, whether processing high-frequency streams or massive data backfills.</p> </li> <li> <p>Real-Time Processing for Immediate Insights:   The combination of Kafka with streaming tools such as Spark Structured Streaming or Apache Druid unlocks near real-time analytics. This empowers rapid, data-driven decision-making in fast-changing environments.</p> </li> <li> <p>Robust Data Governance and Quality:   Build your data on a solid foundation with dynamic schema evolution, rigorous validation frameworks (like Great Expectations), and comprehensive lineage tracking. These measures guarantee that your data remains reliable and compliant.</p> </li> <li> <p>Modern Infrastructure &amp; Seamless Orchestration:   Future-proof your operations using orchestration tools like Apache Airflow, containerization with Docker, and scalable deployments with Kubernetes. Lakehouse Lab provides the agile backbone that today\u2019s production-grade systems demand.</p> </li> <li> <p>End-to-End Machine Learning:   From data wrangling to model deployment, MLflow integrates your entire machine learning lifecycle. The tight integration of ML into the overall data pipeline ensures that every stage\u2014from experimentation to deployment\u2014is synchronized and efficient.</p> </li> </ul> <p>Each component of Lakehouse Lab is designed not only to educate but also to empower, ensuring that you have both the theoretical and practical skills to excel in a rapidly changing data landscape.</p>"},{"location":"papers/01-article/#architectural-overview","title":"Architectural Overview","text":"<p>Lakehouse Lab is built on a flexible, modular pipeline that combines the proven benefits of a traditional data lakehouse with modern, scalable design principles. The architecture supports every stage\u2014from realistic data simulation through to actionable analytics and machine learning\u2014while maintaining resilience, fault tolerance, and flexibility.</p> <p>Overall Pipeline Architecture</p> <p></p>"},{"location":"papers/01-article/#1-emulatorsource","title":"1. Emulator/Source","text":"<p>Purpose: Our custom data factories generate realistic event streams\u2014including synthetic device logs, financial transactions, and user profiles\u2014that closely mimic real-world production conditions. Whether delivered to streaming or batch ingestion systems, these events rigorously test every aspect of your data pipeline.</p> <p>Key Considerations:</p> <ul> <li> <p>Realism:   The emulator faithfully replicates both everyday activity and peak load scenarios, providing a stringent testing ground for system resilience.</p> </li> <li> <p>Flexibility:   By generating diverse data types\u2014such as device logs, transactions, and user profiles\u2014the emulator supplies a versatile foundation that supports a wide range of data scenarios across both streaming and batch modes.</p> </li> </ul> <p></p>"},{"location":"papers/01-article/#2-ingestion-layer","title":"2. Ingestion Layer","text":"<p>Core Technologies:</p> <ul> <li> <p>Apache Kafka:   Provides high-throughput, low-latency streaming to efficiently handle real-time data.</p> </li> <li> <p>Minio:   Acts as an S3-compatible storage solution that stages large volumes of raw data for batch processing.</p> </li> </ul> <p>Design Features:</p> <ul> <li> <p>Fault Tolerance:   Engineered to recover gracefully, ensuring data continuity without loss.</p> </li> <li> <p>Scalability:   Seamlessly adapts to both on-premise and cloud environments, scaling as your data volumes grow.</p> </li> </ul> <p>Benefits:</p> <ul> <li>Hybrid Approach:   Combines the immediacy of real-time streaming (via Kafka) with the reliability of batch ingestion (via Minio) for maximum flexibility and resilience.</li> </ul> <p>Detailed Ingestion Diagram</p> <p></p>"},{"location":"papers/01-article/#3-processing","title":"3. Processing","text":"<p>Technology Backbone: Powered by Apache Spark, the processing layer transforms both batch and streaming data into actionable insights.</p> <p>Key Processing Characteristics:</p> <ul> <li> <p>Schema Enforcement:   Ensures that all incoming data adheres strictly to predefined structures, maintaining consistency across the pipeline.</p> </li> <li> <p>Data Quality Validations:   Implements comprehensive checks to verify the accuracy and integrity of the data before further processing.</p> </li> <li> <p>Transformation Flexibility:   From simple cleanses to complex aggregations and joins, Spark adeptly handles all types of transformations.</p> </li> </ul> <p>Operational Insights:</p> <ul> <li>Real-Time Capabilities:   When combined with streaming ingestion, Spark delivers near real-time analytics that empower rapid, data-driven decision-making.</li> </ul> <p>Processing Flow Diagram</p> <p></p>"},{"location":"papers/01-article/#4-storage","title":"4. Storage","text":"<p>Storage Technologies:</p> <ul> <li> <p>Delta Lake / Iceberg:   These modern table formats provide ACID guarantees, time travel capabilities, and dynamic schema evolution, ensuring reliable and high-performance storage.</p> </li> <li> <p>Nessie:   Nessie is the Git for your data lake. It delivers robust version control\u2014enabling branching, merging, and precise change tracking in your lakehouse storage. With fine-grained lineage and auditability, Nessie ensures seamless rollbacks, fosters collaborative development, and enforces strict compliance.</p> </li> </ul> <p>Data Tiering Strategy:</p> <ul> <li>Bronze (Raw): Holds unprocessed data.</li> <li>Silver (Cleaned/Enriched): Stores data after transformation and quality validation.</li> <li>Gold (Analytics-Ready): Contains curated datasets optimized for fast, reliable analytics.</li> </ul> <p>Benefits:</p> <ul> <li>Reliability:   ACID compliance and time travel in Delta Lake / Iceberg ensure data consistency and facilitate easy rollbacks.</li> <li>Lifecycle Management:   A tiered strategy streamlines the progression from raw data to actionable insights.</li> <li>Enhanced Governance with Nessie:   Nessie's unique capability to version and control data changes empowers teams to experiment, collaborate, and audit the data lifecycle seamlessly. It ensures that every change\u2014be it schema evolution, data corrections, or historical trends\u2014is properly recorded and can be reviewed or reversed if necessary.</li> </ul> <p>Data Tiering Diagram (Including Nessie)</p> <p></p>"},{"location":"papers/01-article/#why-nessie-matters","title":"Why Nessie Matters","text":"<p>By integrating Nessie, Lakehouse Lab gains a crucial layer of data governance and version control that is unique in today's data architectures:</p> <ul> <li> <p>Version Control for Data:   Like Git for source code, Nessie offers branching, merging, and history tracking for your data. This allows you to experiment without fear, knowing that you can always revert or compare different versions of your datasets.</p> </li> <li> <p>Enhanced Collaboration:   Nessie facilitates collaboration among data engineers and scientists by managing concurrent changes, making it easier to integrate, review, and synchronize modifications across teams.</p> </li> <li> <p>Improved Auditability and Compliance:   With full traceability of data changes, Nessie supports rigorous audit trails, enabling organizations to comply with regulatory requirements and ensuring data integrity over time.</p> </li> <li> <p>Streamlined Data Evolution:   As datasets evolve through refinements and enhancements, Nessie ensures that these changes are captured systematically\u2014providing a robust metadata layer that simplifies rollback, troubleshooting, and historical analysis.</p> </li> </ul>"},{"location":"papers/01-article/#5-consumption","title":"5. Consumption","text":"<p>End-User Technologies:</p> <ul> <li>BI &amp; Analytics:   Tools like Apache Superset, Dremio, and Apache Druid empower you to build interactive dashboards and perform real\u2011time reporting.</li> <li>Machine Learning: MLflow ties together the entire machine learning lifecycle\u2014from training to deployment.</li> </ul> <p>Consumption Pathways:</p> <ul> <li>Operational Dashboards:   Provide immediate, interactive insights for everyday decision-making.</li> <li>Predictive Analytics:   Leverage both historical and real-time data to forecast future trends and outcomes.</li> </ul> <p>Unified Access: Data is transformed and curated before reaching end-user systems, ensuring that platforms like Apache Superset have a complete and accurate analytical view.</p> <p>Consumption Flow Diagram</p> <p></p>"},{"location":"papers/01-article/#additional-diagrams-and-insights","title":"Additional Diagrams and Insights","text":"<p>To provide a deeper understanding of the system, the following diagrams illustrate data flow, processing stages, and error handling:</p>"},{"location":"papers/01-article/#sequence-diagram-data-event-flow","title":"Sequence Diagram \u2013 Data Event Flow","text":"<p>This diagram shows the journey of a single data event from generation through transformation to consumption.</p> <p></p>"},{"location":"papers/01-article/#flowchart-data-processing-pipeline","title":"Flowchart \u2013 Data Processing Pipeline","text":"<p>This flowchart captures the major processing stages from raw data ingestion to final delivery of actionable insights.</p> <p></p>"},{"location":"papers/01-article/#sequence-diagram-error-handling","title":"Sequence Diagram \u2013 Error Handling","text":"<p>This diagram demonstrates how errors are managed within the processing pipeline.</p> <p></p>"},{"location":"papers/01-article/#architectural-rationale-and-benefits","title":"Architectural Rationale and Benefits","text":"<p>Lakehouse Lab\u2019s layered design is more than just a collection of state-of-the-art technologies\u2014it\u2019s a strategic blueprint that delivers:</p> <ul> <li>Modularity:   Every component, from data generation to consumption, is decoupled and replaceable, allowing the architecture to evolve seamlessly with new innovations.</li> <li>Resilience:   With robust error handling, fault tolerance, and recovery mechanisms, the system is designed to maintain continuous data flow\u2014even when issues arise.</li> <li>Scalability:   The hybrid integration of batch and real-time ingestion ensures the system scales effortlessly to meet growing data demands across diverse workloads.</li> </ul> <p>Expanded Technology Ecosystem Diagram</p> <p></p>"},{"location":"papers/02-article/","title":"Monorepos with Nx in Lakehouse Lab: Why and How","text":"<p>At Lakehouse Lab, our repository isn\u2019t simply a collection of isolated projects\u2014it\u2019s a unified ecosystem that spans data emulation, ingestion, processing, storage, monitoring, and deployment. Consolidating these components into a single monorepo enables us to work in an integrated workspace where libraries have clearly defined scopes and dependency management is both transparent and concise. In this article, we explain why we chose a monorepo strategy for Lakehouse Lab, how Nx powers our workflow, and the tangible benefits we\u2019ve realized.</p>"},{"location":"papers/02-article/#why-a-monorepo-for-lakehouse-lab","title":"Why a Monorepo for Lakehouse Lab?","text":""},{"location":"papers/02-article/#consolidating-complex-data-pipelines","title":"Consolidating Complex Data Pipelines","text":"<p>The Lakehouse Lab repository is designed as a one-stop ecosystem that integrates all components critical to modern data engineering. By keeping everything in a single repository, every piece of the puzzle is immediately accessible, synchronized, and consistently maintained. Key elements include:</p> <ul> <li> <p>Services:   Core applications such as <code>services/airflow-app</code> (for workflow orchestration) and <code>services/data-emulator</code> (for synthetic data generation) form the backbone of our pipelines.</p> </li> <li> <p>Libraries:   Under the <code>libs</code> directory, modules focused on domain-driven design (DDD), fake data factories, configuration settings, and shared utilities coexist. This layered architecture encourages code reuse and allows us to enforce closed scopes of responsibility for each library. Each team can work in the language best suited for the task (Python, TypeScript, Go, etc.), while clear dependency boundaries minimize accidental coupling.</p> </li> <li> <p>DevOps &amp; Infrastructure:   Directories like <code>deploy</code>, <code>monitoring</code>, and <code>scripts</code> encapsulate all deployment configurations, CI/CD pipelines, and infrastructure-as-code artifacts. This streamlined structure ensures every change is immediately reflected across production workflows.</p> </li> <li> <p>Documentation:   A centralized <code>docs</code> folder provides guides, architectural overviews, and onboarding materials\u2014making it easier for new team members to quickly understand and contribute to the ecosystem.</p> </li> </ul>"},{"location":"papers/02-article/#eliminating-cross-repository-overhead","title":"Eliminating Cross-Repository Overhead","text":"<p>A monorepo minimizes overheads typically encountered with managing multiple repositories by:</p> <ul> <li> <p>Enforcing Consistent Coding Practices:   With a single repository, uniform standards, testing frameworks, and build configurations are applied throughout, which greatly improves code quality and simplifies maintenance.</p> </li> <li> <p>Enhancing Collaboration:   When data engineers, machine learning specialists, and DevOps teams work within the same codebase, knowledge sharing happens more naturally, and troubleshooting becomes faster due to transparent workflows.</p> </li> <li> <p>Enabling Atomic and Synchronized Changes:   Atomic commits across multiple modules ensure that updates (from global configuration changes to extensive refactoring) are synchronized throughout the repository, reducing version conflicts and integration issues.</p> </li> </ul>"},{"location":"papers/02-article/#how-nx-empowers-our-monorepo-strategy","title":"How Nx Empowers Our Monorepo Strategy","text":"<p>Nx is much more than a repository manager\u2014it is a comprehensive toolkit that brings structure, efficiency, and scalability to our monorepo. Here\u2019s how Nx transforms our workflow at Lakehouse Lab:</p>"},{"location":"papers/02-article/#enabling-a-clean-and-concise-workspace","title":"Enabling a Clean and Concise Workspace","text":"<ul> <li> <p>Scoped Libraries:   Nx helps us enforce clear boundaries between libraries by encouraging a closed-scope design. Each library is responsible for a specific domain of our application, making dependency management straightforward and avoiding unintended cross-cutting concerns.</p> </li> <li> <p>Clear Dependency Management:   With Nx, every module\u2019s dependencies are visually mapped and strictly controlled. This transparency not only simplifies maintenance but also accelerates collaboration across teams.</p> </li> </ul>"},{"location":"papers/02-article/#advanced-features-in-action","title":"Advanced Features in Action","text":"<ul> <li> <p>Advanced Dependency Graph:   Nx automatically visualizes interdependencies across projects. When you update a shared utility in <code>libs/shared</code> or a configuration module in <code>libs/settings</code>, the dependency graph reveals the ripple effects throughout the repository. This clarity is vital for safe refactoring and informed decision-making. You can even explore our interactive dependency graph here.</p> </li> <li> <p>Incremental Builds and Tests:   Rather than rebuilding the entire codebase for every change, Nx intelligently caches outputs and tracks affected projects at a granular level. Commands like <code>nx affected:build</code> and <code>nx affected:test</code> ensure that only the modules impacted by recent changes are rebuilt or retested. This dramatically reduces build times and accelerates the development cycle.</p> </li> <li> <p>Integrated Task Orchestration and Consistent Code Generation:   Nx\u2019s built-in task runner coordinates complex workflows\u2014testing, linting, and deployment\u2014ensuring that tasks run in the correct order and parallelize whenever possible. Additionally, its powerful code generators and schematics enable developers to quickly scaffold new applications and libraries that adhere to best practices, minimizing onboarding friction.</p> </li> <li> <p>Exploring the Nx Affected Feature:   One of Nx\u2019s most powerful capabilities is identifying and visualizing the projects affected by recent changes in your monorepo. Rather than rebuilding or retesting everything, Nx enables targeted operations, such as:</p> </li> <li> <p>nx affected:test \u2013 Runs tests only for modified projects and their dependents.</p> </li> <li>nx affected:lint \u2013 Applies lint checks only where they\u2019re needed.</li> <li>nx affected:build \u2013 Rebuilds only the relevant modules.</li> <li>nx affected:image \u2013 Creates container images solely for changed projects.</li> </ul> <p>This selective approach optimizes your CI/CD pipeline by reducing build times and resource consumption. The screenshot below illustrates an \u201caffected\u201d graph generated by running:</p> <pre><code>npx nx graph --affected\n</code></pre> <p></p> <p>In this visual representation, only the projects impacted by your recent commits appear highlighted, along with their direct or indirect dependencies. You can immediately see which modules need attention and focus your efforts accordingly. By zeroing in on just the affected projects, you streamline your workflow, shorten feedback loops, and keep the entire development process more efficient.</p>"},{"location":"papers/02-article/#visualizing-the-monorepo-structure","title":"Visualizing the Monorepo Structure","text":""},{"location":"papers/02-article/#1-structural-overview-of-the-monorepo","title":"1. Structural Overview of the Monorepo","text":"<p>This diagram categorizes our repository into Services, Libraries, and DevOps &amp; Infrastructure.</p> <p></p>"},{"location":"papers/02-article/#2-nx-orchestration-and-inter-module-relationships","title":"2. Nx Orchestration and Inter-Module Relationships","text":"<p>This diagram details how Nx orchestrates core processes and integrations across modules.</p> <p></p>"},{"location":"papers/02-article/#the-impact-on-lakehouse-lab","title":"The Impact on Lakehouse Lab","text":"<p>Nx has transformed the way we develop and deploy our systems:</p> <ul> <li> <p>Enhanced Developer Productivity:   Unified workflows, closed-scope libraries, and incremental builds allow developers to navigate and modify interconnected components swiftly, reducing downtime and speeding up development.</p> </li> <li> <p>Stronger Consistency and Quality:   A single repository ensures uniform coding practices and immediate propagation of shared changes, which minimizes integration friction and maintains high code quality.</p> </li> <li> <p>Efficient, Scalable Workflows:   Intelligent dependency tracking and the \u201caffected\u201d feature keep build and test cycles lean, enabling scalable development as the repository grows.</p> </li> </ul> <p>With Nx at the core of our monorepo, every code change becomes part of a cohesive ecosystem, streamlining development while laying a robust foundation for future innovation.</p>"},{"location":"papers/02-article/#practical-benefits-realized-in-lakehouse-lab","title":"Practical Benefits Realized in Lakehouse Lab","text":""},{"location":"papers/02-article/#enhanced-developer-productivity","title":"Enhanced Developer Productivity","text":"<ul> <li>Unified Workflows:   Developers work seamlessly across services and libraries, accelerating onboarding and fostering collaboration.</li> <li>Efficient Integration:   Immediate propagation of shared changes minimizes version drift and synchronizes development across all modules.</li> </ul>"},{"location":"papers/02-article/#agile-and-consistent-delivery","title":"Agile and Consistent Delivery","text":"<ul> <li>Faster Builds and Tests:   Incremental builds and targeted testing\u2014especially with the \u201caffected\u201d feature\u2014significantly cut build times.</li> <li>Robust Task Scheduling:   Automated, dependency-aware orchestration ensures that builds, tests, and deployments run smoothly, reinforcing high-quality code.</li> </ul>"},{"location":"papers/02-article/#scalability-for-complex-data-ecosystems","title":"Scalability for Complex Data Ecosystems","text":"<p>As Lakehouse Lab evolves with new microservices, libraries, and infrastructure enhancements, our Nx-managed monorepo scales effortlessly, meeting growing data demands without sacrificing stability or efficiency.</p>"},{"location":"papers/03-article/","title":"Data Emulator Service in Lakehouse Lab: An Architectural Deep Dive","text":"<p>In today\u2019s complex data ecosystems, validating pipelines with realistic, production-grade data is critical before deployment. As experienced data architects, we know that rigorous testing is the cornerstone of operational excellence. At Lakehouse Lab, our Data Emulator Service is engineered to generate synthetic data that mirrors real-world scenarios\u2014from financial transactions and device logs to comprehensive user profiles. This article provides an in-depth exploration of the Data Emulator Service, detailing its architectural design, key components, and how it supports both streaming and batch ingestion pipelines.</p>"},{"location":"papers/03-article/#why-a-data-emulator-is-essential","title":"Why a Data Emulator Is Essential","text":""},{"location":"papers/03-article/#bridging-simulation-and-production","title":"Bridging Simulation and Production","text":"<p>Real-world data is unpredictable\u2014it features surges, anomalies, and a wide spectrum of conditions that static datasets simply cannot capture. Relying solely on sanitized, static data can mask critical issues, resulting in unexpected bottlenecks and failures when systems go live. Our Data Emulator Service overcomes these challenges by:</p> <ul> <li> <p>Simulating Real-World Variability:   It continuously produces dynamic synthetic data streams that replicate everyday operations and rare edge-case scenarios, rigorously stressing both ingestion and processing pipelines.</p> </li> <li> <p>Enabling Continuous Experimentation:   By generating production-like data on an ongoing basis, our service empowers teams to iterate on data models, refine ETL processes, and experiment with complex machine learning algorithms without impacting live systems.</p> </li> <li> <p>Validating End-to-End Workflows:   The emulator supplies realistic inputs at every stage\u2014from ingestion through processing, storage, and analytics\u2014ensuring that the entire data pipeline is primed to handle the unpredictability of real-world data.</p> </li> </ul> <p>Print Opportunity: Include a visual that contrasts static test datasets with dynamic, synthetic production-like data to highlight the importance of realistic data simulation.</p>"},{"location":"papers/03-article/#system-architecture-overview","title":"System Architecture Overview","text":"<p>Our Data Emulator Service is built with modularity, resilience, and scalability in mind\u2014principles that guide every decision we make. The architecture is structured around three key pillars, each addressing a critical component of the system's operation.</p>"},{"location":"papers/03-article/#1-rest-api-and-service-orchestration","title":"1. REST API and Service Orchestration","text":"<p>At the heart of the service is a robust REST API, built with FastAPI. This API is the primary interface for triggering, monitoring, and managing emulation tasks. Deployment in an isolated Uvicorn process enhances resilience and simplifies lifecycle management. Supporting this core functionality are several critical components:</p> <ul> <li> <p>Centralized Configuration:   A custom <code>Settings</code> class aggregates configuration from environment variables and command-line arguments. This ensures that critical parameters (e.g., Kafka and Minio credentials) are consistently injected into the application.</p> </li> <li> <p>Graceful Signal Handling:   An asynchronous <code>SignalHandler</code> listens for termination signals (such as SIGINT and SIGTERM) and orchestrates a controlled shutdown of the API and background tasks, preventing data loss and ensuring proper resource cleanup.</p> </li> </ul> <p>Below is a diagram that visualizes how these components interact:</p> <p></p> <p>Diagram Annotation:</p> <ul> <li>FastAPI REST API: Main endpoint for client requests.</li> <li>Configuration Management: Ensures consistency in configuration values across the system.</li> <li>Signal Handler: Manages shutdown sequences to ensure smooth termination.</li> <li>Uvicorn Process: Hosts the FastAPI application in an isolated, resilient environment.</li> </ul>"},{"location":"papers/03-article/#2-data-generation-with-producer-wrappers","title":"2. Data Generation with Producer Wrappers","text":"<p>Our Data Emulator Service employs a factory-based mechanism to generate synthetic data through various producer wrappers. This design enables the system to select the most suitable producer at runtime\u2014either Kafka for real-time streaming or Minio for batch mode\u2014thereby decoupling core business logic from the data publishing process.</p> <ul> <li> <p>Producer Abstraction:   A factory pattern underpins this abstraction, receiving synchronization parameters from the incoming request (e.g., \u201cKafka\u201d or \u201cMinio\u201d) and returning the correct producer wrapper. By cleanly separating publishing strategies, the service gains exceptional flexibility and maintainability.</p> </li> <li> <p>Kafka Integration:   When configured for streaming, the <code>KafkaFactorySyncProducerWrapper</code> is used to dispatch JSON-formatted synthetic events directly to Kafka topics. This approach is optimized for low-latency, high-throughput data pipelines, ensuring that downstream consumers receive updates in near real-time.</p> </li> <li> <p>Minio Integration:   For batch-oriented scenarios, the <code>MinioFactorySyncProducerWrapper</code> manages the uploading of synthetic data as objects into Minio. Whether uploading single records or handling large, chunked backfills, this wrapper ensures robust batch processing for later ingestion and analysis.</p> </li> </ul>"},{"location":"papers/03-article/#producer-wrappers-branching-logic","title":"Producer Wrappers Branching Logic","text":"<p>Diagram Annotation:</p> <ul> <li>Start: The system receives the desired synchronization method (e.g., \u201cKafka\u201d or \u201cMinio\u201d).</li> <li>Branching Decision:<ul> <li>Kafka Producer: If the mode is \u201cstreaming,\u201d JSON-formatted synthetic events are immediately dispatched to Kafka topics.</li> <li>Minio Producer: If the mode is \u201cbatch,\u201d synthetic data objects\u2014either single or chunked\u2014are uploaded to Minio.</li> </ul> </li> <li>End: The chosen producer executes the data publishing accordingly.</li> </ul>"},{"location":"papers/03-article/#real-world-example-producer-selection-in-action","title":"Real-World Example: Producer Selection in Action","text":"<p>Below is a screenshot captured from a live environment, demonstrating how the Data Emulator\u2019s logs reveal both the branching logic and the resulting published data:</p> <p></p> <p>Screenshot Highlights:</p> <ul> <li>CLI Commands: The upper portion shows a sample <code>curl</code> command specifying <code>\"emulator_sync\": \"grouped\", \"emulation_domain\": \"user-profile\"</code> and other parameters. These parameters drive the decision to use Minio for batch uploads and specify the shape and size of the dataset.</li> <li>Log Messages: The logs reflect how the service identifies the correct producer, spawns threads for parallel data generation, and posts confirmation messages such as <code>Produced message</code> or <code>Emulation finished</code>. These details offer a transparent look into the background tasks orchestrated by the Data Emulator.</li> </ul>"},{"location":"papers/03-article/#3-asynchronous-processing-and-parallel-generation","title":"3. Asynchronous Processing and Parallel Generation","text":"<p>To simulate the high-throughput conditions typical of production systems, the Data Emulator Service leverages asynchronous processing and parallel generation. This ensures that synthetic data is generated concurrently, providing a realistic stress test for downstream pipelines.</p> <ul> <li> <p>Concurrent Data Production:   The service spawns multiple threads using FastAPI\u2019s BackgroundTasks, generating synthetic data concurrently. This simulates a high-load environment, allowing us to mimic the performance characteristics of real-world systems.</p> </li> <li> <p>Specialized Data Factories:   Based on the emulation domain, the service dynamically selects a dedicated factory to generate the appropriate synthetic data:</p> <ul> <li>DeviceLogFactory: Generates detailed device logs capturing user, device, and session information.</li> <li>TransactionFakeFactory: Produces synthetic financial transactions enriched with robust fraud-detection logic.</li> <li>UserProfileFactory: Creates comprehensive user profiles including demographic details and risk assessments.</li> </ul> </li> <li> <p>Resource and Status Management:   An in-memory repository tracks the progress and status of each task at a granular level. This ensures that once data generation completes, downstream processes\u2014such as Apache Spark jobs, analytics, and ML pipelines\u2014are triggered without delay.</p> </li> </ul> <p>Below is a refined flowchart that visually encapsulates the asynchronous data generation process:</p> <p></p> <p>Diagram Annotation:</p> <ul> <li>Initialization: The emulation task is initiated and multiple background tasks are spawned.</li> <li>Data Generation: Specialized factories operate concurrently to generate different types of synthetic data.</li> <li>Status Management: An in-memory repository tracks each thread\u2019s progress.</li> <li>Triggering Downstream Processes: Once all tasks are complete, further processing (e.g., Spark jobs, analytics) is initiated.</li> </ul>"},{"location":"papers/03-article/#data-flow-overview","title":"Data Flow Overview","text":"<p>The following enhanced diagram illustrates the comprehensive data flow within the Data Emulator Service as it integrates into Lakehouse Lab\u2019s broader pipeline. The diagram details every stage\u2014from client request to configuration, task scheduling, dynamic producer selection, and final data ingestion.</p> <p></p> <p>Diagram Annotation:</p> <ul> <li>Client: Initiates the emulation process.</li> <li>Data Emulator Service: Handles configuration, logging, signal management, and schedules background tasks.</li> <li>Producers: Use a selector to dynamically choose either Kafka (for streaming) or Minio (for batch processing) based on the synchronization method.</li> <li>Data Ingestion Layer: Receives the synthetic data for further downstream processing.</li> </ul>"},{"location":"papers/03-article/#in-depth-look-at-data-factories","title":"In-Depth Look at Data Factories","text":"<p>Our Data Emulator Service utilizes specialized factories to generate realistic synthetic data, each adhering to specific business rules:</p>"},{"location":"papers/03-article/#devicelogfactory","title":"DeviceLogFactory","text":"<p>Purpose: Generates synthetic device logs capturing details such as user ID, device type, operating system, network information, and session duration.</p> <p>Key Fields:</p> <ul> <li><code>log_id</code>, <code>user_id</code>, <code>device_id</code></li> <li><code>device_type</code> (e.g., mobile, desktop, tablet)</li> <li><code>os</code> (e.g., Windows, macOS, Linux)</li> <li><code>ip_address</code>, <code>location</code></li> <li><code>user_agent</code></li> <li><code>login_timestamp</code>, with an optional <code>logout_timestamp</code></li> </ul> <p>Operation: Simulates user sessions by generating a login timestamp within the last hour and conditionally creating a logout timestamp (70% chance) to mimic variable session durations.</p>"},{"location":"papers/03-article/#transactionfakefactory","title":"TransactionFakeFactory","text":"<p>Purpose: Produces synthetic financial transactions, incorporating advanced fraud-detection logic to simulate anomalies.</p> <p>Key Fields:</p> <ul> <li><code>transaction_id</code>, <code>user_id</code>, <code>amount</code>, <code>currency</code></li> <li><code>merchant</code>, <code>timestamp</code>, <code>location</code></li> <li><code>is_fraud</code> flag</li> </ul> <p>Business Rules: Evaluates transactions under multiple conditions:</p> <ul> <li>Account Takeover: Flags high-amount transactions with a 30% chance for compromised users.</li> <li>Card Testing: Detects patterns in low-amount transactions.</li> <li>Merchant Collusion: Flags transactions from high-risk merchants when amounts are substantial.</li> <li>Geographical Anomalies: Modifies location values based on user patterns.</li> <li>Random Fraud: Applies a minimal baseline fraud probability.</li> </ul>"},{"location":"papers/03-article/#fraud-rule-decision-flowchart","title":"Fraud Rule Decision Flowchart","text":""},{"location":"papers/03-article/#userprofilefactory","title":"UserProfileFactory","text":"<p>Purpose: Generates synthetic user profiles enriched with demographic details and risk assessments.</p> <p>Key Fields:</p> <ul> <li><code>user_id</code>, <code>name</code>, <code>email</code>, <code>phone</code></li> <li><code>date_of_birth</code>, <code>address</code>, <code>country</code></li> <li><code>signup_date</code>, <code>credit_score</code></li> <li><code>risk_level</code> (derived from credit score, demographics, and sign-up recency)</li> </ul> <p>Operation: Ensures unique user IDs via a thread-safe mechanism and produces detailed profiles with calculated risk levels based on predefined criteria.</p>"},{"location":"papers/03-article/#end-to-end-data-emulation-workflow","title":"End-to-End Data Emulation Workflow","text":"<p>The sequence diagram below outlines the complete operation of the Data Emulator Service\u2014from the client triggering an emulation to data generation and subsequent ingestion.</p> <p></p> <p>Annotation:</p> <ul> <li>Task Initialization: The emulation task is triggered and initialized.</li> <li>Data Generation: Specialized factories, such as TransactionFakeFactory, apply business rules to generate synthetic data.</li> <li>Data Publishing: The producer wrapper dispatches data to the appropriate ingestion layer.</li> <li>Status Tracking: Clients can query the status of emulation tasks via the API.</li> </ul>"},{"location":"papers/03-article/#api-workflow-example-request-processing","title":"API Workflow: Example Request Processing","text":"<p>The Data Emulator Service exposes a RESTful API for managing emulation tasks. Below is an example using <code>curl</code> to trigger an emulation for user profiles:</p> <pre><code>curl -X 'POST' 'http://localhost:8000/emulator/' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"emulator_sync\": \"minio\", \"emulation_domain\": \"user-profile\", \"timeout\": 1, \"format_type\": \"json\", \"sync_type\": \"grouped\", \"max_chunk_size\": 1024}'\n</code></pre>"},{"location":"papers/03-article/#flowchart-api-request-processing","title":"Flowchart: API Request Processing","text":"<p>Annotation:</p> <ul> <li> <p>Payload Reception and Validation:   The process begins when the client sends a POST request to the <code>/emulator</code> endpoint. The API parses the request payload (StartEmulatorDTO) and performs validation, ensuring all required fields and parameters are present and correct. This step is crucial to avoid processing invalid or incomplete requests.</p> </li> <li> <p>Dynamic Producer Selection:   Upon successful payload validation, the system determines the appropriate synchronization method. Based on this decision\u2014whether to use real-time streaming via Kafka or batch processing via Minio\u2014the service dynamically selects the corresponding producer wrapper. For example, if the sync method is Kafka, the <code>KafkaFactorySyncProducerWrapper</code> is employed; if it is Minio, then the <code>MinioFactorySyncProducerWrapper</code> is chosen.</p> </li> <li> <p>Task Scheduling:   Once the synchronization method is set and the appropriate data factory is chosen (e.g., UserProfileFactory for user-profile data), the service schedules the data generation task. This is managed asynchronously using FastAPI's BackgroundTasks, allowing the system to spawn multiple threads that generate synthetic data concurrently, simulating a high-throughput environment.</p> </li> <li> <p>Status Tracking and Response:   As tasks run in the background, the service maintains an in-memory status repository to track the progress of each task at a granular level. This ensures that, once data generation completes, downstream processes\u2014such as Apache Spark jobs, analytics, or ML pipelines\u2014are triggered promptly. Finally, the API returns a unique EmulationScheduledDTO with an ID that the client can use to query the task status later.</p> </li> </ul>"},{"location":"papers/03-article/#additional-resources-swaggeropenapi-documentation","title":"Additional Resources: Swagger/OpenAPI Documentation","text":"<p>For a comprehensive look at our Data Emulator Service API, explore our interactive Swagger documentation. This detailed OpenAPI resource covers every endpoint, parameter, and response model, making it an invaluable tool for testing and integration.</p> <p>Access the Swagger Documentation: Data Emulator Service OpenAPI Documentation</p>"},{"location":"papers/03-article/#conclusion","title":"Conclusion","text":"<p>Our Data Emulator Service is a cornerstone of the Lakehouse Lab architecture\u2014providing a secure, realistic environment for rigorous testing and innovation across the data pipeline. By generating synthetic data that mirrors complex real-world scenarios and applying sophisticated business logic, the service bridges the gap between simulation and production. This ensures that each phase\u2014from ingestion and processing to storage and analytics\u2014is robustly validated.</p> <p>In this architectural deep dive, we have covered:</p> <ul> <li>The Need for a Data Emulator: How dynamic, production-like data ensures system robustness.</li> <li>Core Architectural Components: An overview of our REST API, producer abstractions, and parallel data generation.</li> <li>In-Depth Data Factory Logic: Detailed explanations and flowcharts for generating synthetic device logs, transactions, and user profiles.</li> <li>Comprehensive Visualizations: Sequence diagrams, flowcharts, and PlantUML illustrations that capture the data flow and process orchestration.</li> <li>API Documentation: Direct access to our Swagger/OpenAPI documentation for further exploration.</li> </ul> <p>As our series continues, future articles will explore how this synthetic data is processed by Apache Spark, visualized in real-time dashboards, and leveraged to power advanced ML/AI pipelines. We welcome your feedback and look forward to sharing more insights and best practices as we push the boundaries of modern data architecture.</p>"},{"location":"papers/04-article/","title":"Orchestrating Data Emulation with Airflow in Lakehouse Lab","text":"<p>In modern data engineering, the ability to automate and orchestrate complex workflows is essential to maintaining system robustness and agility. At Lakehouse Lab, we have harnessed the power of Apache Airflow to streamline our data emulation processes\u2014enabling us to simulate realistic data scenarios across multiple domains. This orchestration not only validates our end-to-end pipelines but also lays a solid foundation for downstream processing with Apache Spark, real\u2011time analytics, and advanced ML/AI frameworks.</p>"},{"location":"papers/04-article/#the-strategic-role-of-airflow-in-our-ecosystem","title":"The Strategic Role of Airflow in Our Ecosystem","text":"<p>Airflow is at the heart of our data orchestration strategy. With its scalable, flexible, and modular design, it allows us to manage complex pipelines with precision. In our environment, Airflow serves three critical functions:</p> <ul> <li> <p>Triggering Data Emulation:   Airflow initiates synthetic data generation by invoking our Data Emulator Service via custom REST API calls. This automated triggering ensures that our system is continuously stress-tested under conditions that mimic production.</p> </li> <li> <p>Monitoring Emulation Status:   Once a task is triggered, Airflow diligently polls the Data Emulator Service to monitor progress. By ensuring that emulation tasks are completed successfully before subsequent steps are executed, we maintain the integrity of our data pipeline.</p> </li> <li> <p>Managing Dependencies and Error Handling:   Custom operators enforce strict execution order and effectively manage retries and errors, thereby guaranteeing that interdependent tasks are coordinated seamlessly. This tight orchestration minimizes the risk of cascading failures.</p> </li> </ul>"},{"location":"papers/04-article/#custom-operators-the-backbone-of-our-workflow","title":"Custom Operators: The Backbone of Our Workflow","text":"<p>To integrate the Data Emulator Service within our Airflow ecosystem, we developed two purpose-built custom operators. These operators encapsulate our core orchestration logic, ensuring robust and predictable workflow execution.</p>"},{"location":"papers/04-article/#1-startemulatoroperator","title":"1. StartEmulatorOperator","text":"<p>Purpose: This operator initiates the data emulation process by sending a POST request to our Data Emulator Service\u2019s REST API.</p> <p>Key Responsibilities:</p> <ul> <li> <p>Parameter Handling:   Accepts critical parameters such as synchronization method (e.g., Kafka for streaming, Minio for batch), emulation domain (e.g., user-profile, device-log, transaction), and additional formatting options (format type, sync type, maximum chunk size, timeout).</p> </li> <li> <p>Emulation Trigger:   Sends a well-defined payload to the emulator and retrieves a unique emulation ID, which is then pushed to Airflow\u2019s XCom for further processing.</p> </li> </ul> <p>High-Level Logic Sample:</p> <pre><code>payload = {emulator_sync, emulation_domain, format_type, sync_type, max_chunk_size, timeout}\nresponse = POST(emulator_endpoint, payload)\nif response is successful:\n    emulation_id = response[\"id\"]\n    return emulation_id\nelse:\n    raise error\n</code></pre>"},{"location":"papers/04-article/#2-statusemulationoperator","title":"2. StatusEmulationOperator","text":"<p>Purpose: This operator continuously monitors the status of an ongoing emulation task by polling the Data Emulator Service until it confirms completion.</p> <p>Key Responsibilities:</p> <ul> <li> <p>Polling Mechanism:   Retrieves the emulation ID from XCom and periodically issues GET requests to the status endpoint.</p> </li> <li> <p>Completion Verification:   Continues polling until the task status returns \u201ccompleted\u201d or a timeout is reached, then returns the final status to the workflow.</p> </li> </ul> <p>High-Level Logic Sample:</p> <pre><code>emulation_id = retrieve from XCom\nwhile retry_count &lt; max_retries:\n    status = GET(status_endpoint/emulation_id)\n    if status == \"completed\":\n        return emulation_id\n    else:\n        wait for poll_interval\nraise timeout error if not completed\n</code></pre>"},{"location":"papers/04-article/#constructing-a-multi-domain-emulation-dag","title":"Constructing a Multi-Domain Emulation DAG","text":"<p>Our DAG is architected to orchestrate multiple emulation tasks concurrently\u2014each representing a distinct data domain. This multi-domain approach not only accelerates our testing cycle but also verifies that our system can handle diverse, simultaneous workloads.</p>"},{"location":"papers/04-article/#dag-workflow-overview","title":"DAG Workflow Overview","text":"<ul> <li> <p>Start Task:   A simple PythonOperator logs the beginning of the workflow.</p> </li> <li> <p>Domain-Specific Emulation Tasks:   Separate StartEmulatorOperators trigger emulation for user profiles, device logs, and transactions. Each task is paired with its corresponding StatusEmulationOperator, ensuring that downstream processing is only initiated upon successful completion.</p> </li> <li> <p>End Task:   A concluding PythonOperator logs the completion of all tasks, signaling a successful run.</p> </li> </ul>"},{"location":"papers/04-article/#example-dag-code","title":"Example DAG Code","text":"<pre><code>import airflow\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom start_emulator_operator import StartEmulatorOperator\nfrom status_emulation_operator import StatusEmulationOperator\n\ndag = DAG(\n    dag_id=\"all_emulations_dag\",\n    default_args={\n        \"owner\": \"Fabio Caffarello\",\n        \"start_date\": airflow.utils.dates.days_ago(1),\n    },\n    schedule_interval=\"@daily\",\n)\n\nstart = PythonOperator(\n    task_id=\"start\",\n    python_callable=lambda: print(\"Jobs started\"),\n    dag=dag,\n)\n\nstart_emulator_user_profile = StartEmulatorOperator(\n    task_id=\"start_emulator_user_profile_task\",\n    endpoint=\"http://data-emulator:8000/emulator/\",\n    emulator_sync=\"minio\",\n    emulation_domain=\"user-profile\",\n    format_type=\"json\",\n    sync_type=\"grouped\",\n    max_chunk_size=1024,\n    timeout=30,\n    dag=dag,\n)\n\nstatus_emulation_user_profile = StatusEmulationOperator(\n    task_id=\"status_emulator_user_profile_task\",\n    endpoint=\"http://data-emulator:8000/emulator/{}/status\",\n    prev_task_id=\"start_emulator_user_profile_task\",\n    dag=dag,\n)\n\nstart_emulator_device = StartEmulatorOperator(\n    task_id=\"start_emulator_device_task\",\n    endpoint=\"http://data-emulator:8000/emulator/\",\n    emulator_sync=\"kafka\",\n    emulation_domain=\"device-log\",\n    format_type=\"json\",\n    sync_type=\"grouped\",\n    max_chunk_size=1024,\n    timeout=60,\n    dag=dag,\n)\n\nstatus_emulation_device = StatusEmulationOperator(\n    task_id=\"status_emulator_device_task\",\n    endpoint=\"http://data-emulator:8000/emulator/{}/status\",\n    prev_task_id=\"start_emulator_device_task\",\n    dag=dag,\n)\n\nstart_emulator_transaction = StartEmulatorOperator(\n    task_id=\"start_emulator_transaction_task\",\n    endpoint=\"http://data-emulator:8000/emulator/\",\n    emulator_sync=\"kafka\",\n    emulation_domain=\"transaction\",\n    format_type=\"json\",\n    sync_type=\"grouped\",\n    max_chunk_size=1024,\n    timeout=60,\n    dag=dag,\n)\n\nstatus_emulation_transaction = StatusEmulationOperator(\n    task_id=\"status_emulator_transaction_task\",\n    endpoint=\"http://data-emulator:8000/emulator/{}/status\",\n    prev_task_id=\"start_emulator_transaction_task\",\n    dag=dag,\n)\n\nend = PythonOperator(\n    task_id=\"end\",\n    python_callable=lambda: print(\"Jobs completed successfully\"),\n    dag=dag,\n)\n\nstart &gt;&gt; start_emulator_user_profile &gt;&gt; status_emulation_user_profile\nstart &gt;&gt; start_emulator_device &gt;&gt; status_emulation_device\nstart &gt;&gt; start_emulator_transaction &gt;&gt; status_emulation_transaction\nstatus_emulation_user_profile &gt;&gt; end\nstatus_emulation_device &gt;&gt; end\nstatus_emulation_transaction &gt;&gt; end\n</code></pre>"},{"location":"papers/04-article/#visual-representation","title":"Visual Representation","text":"<p>Below are several screenshots captured from our production environment:</p> <ul> <li> <p>Figure 1: Airflow DAG Graph </p> </li> <li> <p>Figure 2: Minio Interface </p> </li> <li> <p>Figure 3: Redpanda (Kafka) Interface </p> </li> </ul> <p>Diagram Annotation:</p> <ul> <li>The DAG graph shows multiple domain-specific emulation tasks executing concurrently.</li> <li>Each task triggers a corresponding status check, ensuring robust validation before completion.</li> <li>The screenshots from Minio and Redpanda illustrate real-time data flow into our ingestion systems.</li> </ul>"},{"location":"papers/04-article/#conclusion","title":"Conclusion","text":"<p>Apache Airflow is a cornerstone in our Lakehouse Lab, orchestrating data emulation with unmatched precision. Through custom operators and a meticulously constructed multi-domain DAG, we seamlessly trigger, monitor, and manage the generation of synthetic data. This orchestration validates our data pipelines end-to-end\u2014from ingestion and processing to storage and advanced analytics\u2014ensuring that every component is robust, scalable, and production-ready.</p>"},{"location":"reference/libs/ddd/adapters/api/","title":"API","text":"<p>The API provides an HTTP interface for interacting with the Emulator Service. Built on FastAPI, it leverages modular controllers to coordinate use cases, dependency injection to streamline configuration and resource instantiation, and background tasks to offload long-running emulation processes.</p>"},{"location":"reference/libs/ddd/adapters/api/#features","title":"Features","text":"<ul> <li> <p>RESTful Interface:   Exposes endpoints to trigger and manage emulation processes. A dedicated <code>/emulator</code> endpoint (provided by the controllers library) allows external clients to initiate emulator workflows.</p> </li> <li> <p>Modular Architecture:   Organized into controllers and use cases, the API layer wires these components together within a FastAPI application while maintaining a clear separation of concerns.</p> </li> <li> <p>Dependency Injection:   Utilizes FastAPI\u2019s dependency injection to automatically supply configuration settings, storage clients (Minio), producer implementations (Kafka), and other dependencies.</p> </li> <li> <p>Background Processing:   Emulation tasks run as background jobs via FastAPI\u2019s BackgroundTasks, ensuring that the API remains responsive while handling long-running processes asynchronously.</p> </li> <li> <p>Graceful Shutdown:   A shutdown event hook is available for cleanup tasks, ensuring that resources are properly released when the service stops.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/api/#installation","title":"Installation","text":"<p>Install the API library along with its dependencies using your package manager (e.g., Poetry). Make sure you have configured environment variables or settings files for your external resources (e.g., Kafka, Minio).</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-adapters-api --local\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/adapters/api/#endpoints","title":"Endpoints","text":""},{"location":"reference/libs/ddd/adapters/api/#get","title":"GET <code>/</code>","text":"<p>Returns a welcome message to confirm that the API is running.</p> <p>Example Response:</p> <pre><code>{\n  \"message\": \"Welcome to the Emulator Service REST API!\"\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#post-emulator","title":"POST <code>/emulator</code>","text":"<p>Triggers the emulator process using the configuration provided in the request payload (conforming to the <code>StartEmulatorDTO</code> schema). This endpoint invokes the Start Emulator Use Case to set up producers, generate fake data, and schedule a background task.</p> <p>Request Payload Example:</p> <pre><code>{\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre> <p>Example Response:</p> <pre><code>{\n  \"id\": \"generated-uuid-string\",\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#configuration","title":"Configuration","text":"<p>The API relies on a configuration object (using a custom <code>Settings</code> class) stored on the FastAPI app state. This configuration provides connection details for Kafka, Minio, and other necessary resources.</p> <p>Example Configuration on Startup:</p> <pre><code>from emulator_settings.settings import Settings\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Emulator Service REST API\",\n    description=\"API for the Emulator Service.\",\n    version=\"1.0.0\"\n)\n\napp.state.config = Settings(\n    kafka_bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",\n    kafka_password=\"your_password\",\n    minio_endpoint=\"minio.example.com\",\n    minio_access_key=\"your_access_key\",\n    minio_secret_key=\"your_secret_key\",\n    minio_secure=False\n)\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/#background-tasks-and-graceful-shutdown","title":"Background Tasks and Graceful Shutdown","text":"<ul> <li> <p>Background Tasks:   Emulation processing is executed in the background via FastAPI\u2019s <code>BackgroundTasks</code>, ensuring that requests return promptly while the heavy processing continues asynchronously.</p> </li> <li> <p>Shutdown Hooks:   The <code>@app.on_event(\"shutdown\")</code> decorator registers cleanup functions to handle any necessary resource release or finalization when the service shuts down.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/api/#running-the-api","title":"Running the API","text":"<p>To run the API locally, you can use Uvicorn:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>Ensure that your application (e.g., the <code>main.py</code> module) includes the API router and configuration, as shown in the usage examples above.</p>"},{"location":"reference/libs/ddd/adapters/api/#testing","title":"Testing","text":"<p>Unit tests for the API controllers and use cases are provided. To run the API tests, execute:</p> <pre><code>npx nx test ddd-adapters-api\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/","title":"Emulator rest api","text":"<p>Emulator Service REST API This module sets up a FastAPI application for the Emulator Service. It includes the main application instance, routes, and event handlers.</p>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/#libs.ddd.adapters.api.api.emulator_rest_api.root","title":"<code>root()</code>","text":"<p>Root endpoint for the Emulator Service REST API. Returns a welcome message.</p> Source code in <code>libs/ddd/adapters/api/api/emulator_rest_api.py</code> <pre><code>@app.get(\"/\", tags=[\"Root\"])\ndef root():\n    \"\"\"\n    Root endpoint for the Emulator Service REST API.\n    Returns a welcome message.\n    \"\"\"\n    return {\"message\": \"Welcome to the Emulator Service REST API!\"}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/api/code_reference/api/emulator_rest_api/#libs.ddd.adapters.api.api.emulator_rest_api.shutdown_event","title":"<code>shutdown_event()</code>","text":"<p>Event handler for application shutdown. Can be used to perform cleanup tasks.</p> Source code in <code>libs/ddd/adapters/api/api/emulator_rest_api.py</code> <pre><code>@app.on_event(\"shutdown\")\ndef shutdown_event():\n    \"\"\"\n    Event handler for application shutdown.\n    Can be used to perform cleanup tasks.\n    \"\"\"\n    # Perform any necessary cleanup tasks here\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/","title":"Controllers","text":"<p>The Controllers Library encapsulates your HTTP endpoints, providing a RESTful interface for triggering and managing emulator processes. Built on top of FastAPI, the library leverages dependency injection to seamlessly integrate configuration settings, storage clients, and producer strategies into your application. This ensures that your emulator use case is easily accessible via HTTP while maintaining a clean separation of concerns.</p>"},{"location":"reference/libs/ddd/adapters/controllers/#features","title":"Features","text":"<ul> <li> <p>RESTful Endpoints:   Exposes endpoints (e.g., POST <code>/emulator</code>) that accept input via DTOs and return structured responses, enabling external clients to trigger emulator processes.</p> </li> <li> <p>Dependency Injection:   Uses FastAPI\u2019s dependency injection system to automatically obtain configuration parameters and instantiate required dependencies such as Kafka producers, Minio storage clients, and use cases.</p> </li> <li> <p>Centralized Error Handling:   Employs HTTP exceptions with appropriate status codes to manage errors and communicate issues clearly to API consumers.</p> </li> <li> <p>Integration with Background Tasks:   Schedules emulation tasks in the background using FastAPI's BackgroundTasks, ensuring non-blocking request handling during long-running emulation processes.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#installation","title":"Installation","text":"<p>Add the Controllers library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-controllers --local\n</code></pre> <p>Ensure that all required dependencies (e.g., FastAPI, confluent-kafka, Faker, and your logger library) are installed via your dependency manager (such as Poetry).</p>"},{"location":"reference/libs/ddd/adapters/controllers/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/adapters/controllers/#configuration-and-dependency-injection","title":"Configuration and Dependency Injection","text":"<p>The controllers rely on FastAPI dependency injection to supply configuration settings (via a custom <code>Settings</code> model) and to instantiate needed clients:</p> <ul> <li>get_config: Retrieves the application\u2019s configuration from the FastAPI app state.</li> <li>get_minio_client: Instantiates a <code>MinioStorageClient</code> using configuration values.</li> <li>get_kafka_producer: Instantiates a <code>KafkaProducerStrategy</code> using configuration values.</li> <li>get_start_emulator_usecase: Combines the above dependencies to construct the <code>StartEmulatorUseCase</code>.</li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#endpoints","title":"Endpoints","text":"<p>The library defines an APIRouter under the <code>/emulator</code> prefix with one key endpoint:</p>"},{"location":"reference/libs/ddd/adapters/controllers/#post-emulator","title":"POST <code>/emulator</code>","text":"<ul> <li> <p>Description:   Triggers the emulator process by accepting a <code>StartEmulatorDTO</code> payload and scheduling a background task that produces data using the appropriate producer (Kafka or Minio).</p> </li> <li> <p>Request Body:   Expects a DTO matching <code>StartEmulatorDTO</code>, which includes parameters such as emulator synchronization type, emulation domain, and timeout.</p> </li> <li> <p>Response:   Returns an <code>EmulationScheduledDTO</code> response containing the scheduled emulation details (including a unique emulation ID).</p> </li> <li> <p>Example Request:</p> </li> </ul> <pre><code>POST /emulator HTTP/1.1\nContent-Type: application/json\n\n{\n    \"emulator_sync\": \"kafka\",\n    \"emulation_domain\": \"transaction\",\n    \"timeout\": 60\n}\n</code></pre> <ul> <li>Example Response:</li> </ul> <pre><code>{\n  \"id\": \"generated-uuid-string\",\n  \"emulator_sync\": \"kafka\",\n  \"emulation_domain\": \"transaction\",\n  \"timeout\": 60\n}\n</code></pre> <pre><code>GET /emulator/&lt;emulation_id&gt;/status HTTP/1.1\n</code></pre> <ul> <li>Example Response:</li> </ul> <pre><code>{\n  \"id\": \"694a9cee-c9e9-4999-9a72-77e5817ca0a3\",\n  \"status\": {\n    \"global_status\": \"completed\",\n    \"threads\": {\n      \"0\": \"finished\",\n      \"1\": \"finished\",\n      \"2\": \"finished\",\n      \"3\": \"finished\",\n      \"4\": \"finished\"\n    }\n  }\n}\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/#integration-example","title":"Integration Example","text":"<p>Here\u2019s a sample integration that uses the Controllers library within a FastAPI application:</p> <pre><code>from fastapi import FastAPI\nfrom controllers.emulator_controller import router as emulator_router\n\napp = FastAPI()\napp.state.config = ...  # Initialize your Settings instance here.\n\n# Include the emulator endpoints\napp.include_router(emulator_router)\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Settings:   The controllers depend on a <code>Settings</code> class that holds configuration values such as Kafka bootstrap servers, Minio endpoints, and authentication credentials.</p> </li> <li> <p>Dependency Mapping:   The endpoint uses dependency providers to instantiate:</p> </li> <li> <p>A Minio storage client (<code>MinioStorageClient</code>),</p> </li> <li>A Kafka producer strategy (<code>KafkaProducerStrategy</code>),</li> <li> <p>A unified use case (<code>StartEmulatorUseCase</code>) that orchestrates background data emulation tasks.</p> </li> <li> <p>Background Task Scheduling:   Emulation tasks are scheduled using FastAPI\u2019s <code>BackgroundTasks</code>, enabling asynchronous processing of data production without blocking API responses.</p> </li> </ul>"},{"location":"reference/libs/ddd/adapters/controllers/#testing","title":"Testing","text":"<p>Unit tests for the Controllers library are provided and can be executed using your CI command:</p> <pre><code>npx nx test ddd-application-controllers\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/","title":"Emulator controller","text":""},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_config","title":"<code>get_config(request)</code>","text":"<p>Dependency to get the application configuration. This function retrieves the configuration from the request's state. It is used as a dependency in FastAPI routes to access the configuration settings. Args:     request (Request): The FastAPI request object. Returns:     Settings: The application configuration settings.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_config(request: Request) -&gt; Settings:\n    \"\"\"\n    Dependency to get the application configuration.\n    This function retrieves the configuration from the request's state.\n    It is used as a dependency in FastAPI routes to access the configuration\n    settings.\n    Args:\n        request (Request): The FastAPI request object.\n    Returns:\n        Settings: The application configuration settings.\n    \"\"\"\n    return request.app.state.config\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_repository","title":"<code>get_repository(request)</code>","text":"<p>Dependency to get the in-memory repository. This function retrieves the repository from the request's state. It is used as a dependency in FastAPI routes to access the repository instance. Args:     request (Request): The FastAPI request object. Returns:     InMemoryRepository: The in-memory repository instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_repository(request: Request) -&gt; InMemoryRepository:\n    \"\"\"\n    Dependency to get the in-memory repository.\n    This function retrieves the repository from the request's state.\n    It is used as a dependency in FastAPI routes to access the repository\n    instance.\n    Args:\n        request (Request): The FastAPI request object.\n    Returns:\n        InMemoryRepository: The in-memory repository instance.\n    \"\"\"\n    return request.app.state.repository\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_minio_client","title":"<code>get_minio_client(config=Depends(get_config))</code>","text":"<p>Dependency to get the MinIO storage client. This function creates and returns a MinIO storage client instance using the configuration settings provided. Args:     config (Settings): The application configuration settings. Returns:     MinioStorageClient: The MinIO storage client instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_minio_client(\n    config: Settings = Depends(get_config),\n) -&gt; MinioStorageClient:  # noqa: B008\n    \"\"\"\n    Dependency to get the MinIO storage client.\n    This function creates and returns a MinIO storage client instance\n    using the configuration settings provided.\n    Args:\n        config (Settings): The application configuration settings.\n    Returns:\n        MinioStorageClient: The MinIO storage client instance.\n    \"\"\"\n    return MinioStorageClient(\n        endpoint=config.minio_endpoint,\n        access_key=config.minio_access_key,\n        secret_key=config.minio_secret_key,\n        secure=config.minio_secure,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_kafka_producer","title":"<code>get_kafka_producer(config=Depends(get_config))</code>","text":"<p>Dependency to get the Kafka producer. This function creates and returns a Kafka producer instance using the configuration settings provided. Args:     config (Settings): The application configuration settings. Returns:     KafkaProducerStrategy: The Kafka producer instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_kafka_producer(\n    config: Settings = Depends(get_config),\n) -&gt; KafkaProducerStrategy:  # noqa: B008\n    \"\"\"\n    Dependency to get the Kafka producer.\n    This function creates and returns a Kafka producer instance\n    using the configuration settings provided.\n    Args:\n        config (Settings): The application configuration settings.\n    Returns:\n        KafkaProducerStrategy: The Kafka producer instance.\n    \"\"\"\n    return KafkaProducerStrategy(\n        bootstrap_servers=config.kafka_bootstrap_servers,\n        kafka_username=config.kafka_username,\n        kafka_password=config.kafka_password,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_start_emulator_usecase","title":"<code>get_start_emulator_usecase(config=Depends(get_config), kafka_producer=Depends(get_kafka_producer), minio_client=Depends(get_minio_client), repository=Depends(get_repository))</code>","text":"<p>Dependency to get the StartEmulatorUseCase instance. This function creates and returns an instance of the StartEmulatorUseCase using the configuration settings, Kafka producer, and MinIO client provided. Args:     config (Settings): The application configuration settings.     kafka_producer (KafkaProducerStrategy): The Kafka producer instance.     minio_client (MinioStorageClient): The MinIO storage client instance.     repository (InMemoryRepository): The in-memory repository instance. Returns:     StartEmulatorUseCase: The StartEmulatorUseCase instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_start_emulator_usecase(\n    config: Settings = Depends(get_config),  # noqa: B008\n    kafka_producer: KafkaProducerStrategy = Depends(get_kafka_producer),  # noqa: B008\n    minio_client: MinioStorageClient = Depends(get_minio_client),  # noqa: B008\n    repository: InMemoryRepository = Depends(get_repository),  # noqa: B008\n) -&gt; StartEmulatorUseCase:\n    \"\"\"\n    Dependency to get the StartEmulatorUseCase instance.\n    This function creates and returns an instance of the StartEmulatorUseCase\n    using the configuration settings, Kafka producer, and MinIO client provided.\n    Args:\n        config (Settings): The application configuration settings.\n        kafka_producer (KafkaProducerStrategy): The Kafka producer instance.\n        minio_client (MinioStorageClient): The MinIO storage client instance.\n        repository (InMemoryRepository): The in-memory repository instance.\n    Returns:\n        StartEmulatorUseCase: The StartEmulatorUseCase instance.\n    \"\"\"\n    return StartEmulatorUseCase(\n        kafka_producer=kafka_producer,\n        kafka_brokers=config.kafka_bootstrap_servers,\n        minio_client=minio_client,\n        repository=repository,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.get_status_emulator_usecase","title":"<code>get_status_emulator_usecase(repository=Depends(get_repository))</code>","text":"<p>Dependency to get the StatusEmulatorUseCase instance. This function creates and returns an instance of the StatusEmulatorUseCase using the configuration settings, Kafka producer, and MinIO client provided. Args:     repository (InMemoryRepository): The in-memory repository instance. Returns:     StatusEmulatorUseCase: The StatusEmulatorUseCase instance.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>def get_status_emulator_usecase(\n    repository: InMemoryRepository = Depends(get_repository),  # noqa: B008\n) -&gt; StatusEmulatorUseCase:\n    \"\"\"\n    Dependency to get the StatusEmulatorUseCase instance.\n    This function creates and returns an instance of the StatusEmulatorUseCase\n    using the configuration settings, Kafka producer, and MinIO client provided.\n    Args:\n        repository (InMemoryRepository): The in-memory repository instance.\n    Returns:\n        StatusEmulatorUseCase: The StatusEmulatorUseCase instance.\n    \"\"\"\n    return StatusEmulatorUseCase(repository=repository)\n</code></pre>"},{"location":"reference/libs/ddd/adapters/controllers/code_reference/controllers/emulator_controller/#libs.ddd.adapters.controllers.controllers.emulator_controller.generate_emulation","title":"<code>generate_emulation(dto, background_tasks, usecase=Depends(get_start_emulator_usecase))</code>","text":"<p>Endpoint to start the emulator. This endpoint receives a StartEmulatorDTO object, processes it using the StartEmulatorUseCase, and returns an EmulationScheduledDTO object. Args:     dto (StartEmulatorDTO): The data transfer object containing the         emulation parameters.     background_tasks (BackgroundTasks): FastAPI background tasks         instance for handling background tasks.     usecase (StartEmulatorUseCase): The use case instance for starting         the emulator. Returns:     EmulationScheduledDTO: The data transfer object containing the         emulation scheduling result. Raises:     HTTPException: If there is an error during the emulation process.</p> Source code in <code>libs/ddd/adapters/controllers/controllers/emulator_controller.py</code> <pre><code>@router.post(\"/\", response_model=EmulationScheduledDTO, status_code=201)\ndef generate_emulation(\n    dto: StartEmulatorDTO,\n    background_tasks: BackgroundTasks,\n    usecase: StartEmulatorUseCase = Depends(get_start_emulator_usecase),  # noqa: B008\n):\n    \"\"\"\n    Endpoint to start the emulator.\n    This endpoint receives a StartEmulatorDTO object, processes it using the\n    StartEmulatorUseCase, and returns an EmulationScheduledDTO object.\n    Args:\n        dto (StartEmulatorDTO): The data transfer object containing the\n            emulation parameters.\n        background_tasks (BackgroundTasks): FastAPI background tasks\n            instance for handling background tasks.\n        usecase (StartEmulatorUseCase): The use case instance for starting\n            the emulator.\n    Returns:\n        EmulationScheduledDTO: The data transfer object containing the\n            emulation scheduling result.\n    Raises:\n        HTTPException: If there is an error during the emulation process.\n    \"\"\"\n    try:\n        return usecase.execute(dto, background_tasks, num_threads=5)\n    except Exception as e:\n        raise HTTPException(status_code=HTTPStatus.BAD_REQUEST, detail=str(e)) from e\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/","title":"DTOs","text":"<p>The DTOs Library provides structured representations of emulation data, streamlining communication between different layers of your application. It ensures data consistency and integrity through the use of immutable data classes and robust data validation.</p>"},{"location":"reference/libs/ddd/application/dtos/#overview","title":"Overview","text":"<p>This library includes several DTOs:</p> <ul> <li>EmulationScheduledDTO: An immutable data class representing the scheduled details of an emulation.</li> <li>StartEmulatorDTO: A Pydantic model that validates and parses the input data for initiating an emulation.</li> <li>StatusDTO: An immutable data class representing the overall status of an emulation.</li> <li>EmulationStatusDTO: An immutable data class encapsulating an emulation identifier along with its status details.</li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#features","title":"Features","text":"<ul> <li> <p>Immutable Data Structures:   The <code>EmulationScheduledDTO</code>, <code>StatusDTO</code>, and <code>EmulationStatusDTO</code> are implemented as frozen dataclasses. Once instantiated, their values cannot be modified, ensuring data integrity across the system.</p> </li> <li> <p>Robust Input Validation:   The <code>StartEmulatorDTO</code> leverages Pydantic\u2019s <code>BaseModel</code> to enforce type checking and validate input data when starting an emulation.</p> </li> <li> <p>Consistent Data Transfer:   Each DTO encapsulates essential details such as synchronization settings, domain information, file format type, timeout settings, and unique identifiers (where applicable).</p> </li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#installation","title":"Installation","text":"<p>To add the DTOs library to your monorepo, run the following command:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-dtos --local\n</code></pre> <p>Make sure that the required dependencies (e.g., <code>pydantic</code>) are included in your environment.</p>"},{"location":"reference/libs/ddd/application/dtos/#dto-details","title":"DTO Details","text":""},{"location":"reference/libs/ddd/application/dtos/#emulationscheduleddto","title":"EmulationScheduledDTO","text":"<p>An immutable dataclass representing the scheduled details of an emulation. This DTO carries comprehensive information about an emulation and is defined as follows:</p> <ul> <li>id: An <code>EmulationID</code> value object representing the unique identifier of the emulation.</li> <li>emulator_sync: A string that indicates synchronization details.</li> <li>format_type: A string that specifies the file format.</li> <li>sync_type: A string that indicates the type of synchronization.</li> <li>emulation_domain: A string representing the emulation domain.</li> <li>max_chunk_size: An integer representing the maximum size of data chunks.</li> <li>timeout: An integer that defines the timeout duration in seconds.</li> </ul> <p>Example:</p> <pre><code>from dtos.emulation_dto import EmulationScheduledDTO\nfrom value_objects.emulator_id import EmulationID\n\n# Create an instance of EmulationScheduledDTO\nemulation_dto = EmulationScheduledDTO(\n    id=EmulationID.generate(),\n    emulator_sync=\"sync_value\",\n    format_type=\"json\",\n    sync_type=\"grouped\",\n    emulation_domain=\"domain_value\",\n    max_chunk_size=1024,\n    timeout=30\n)\n\nprint(emulation_dto)\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/#startemulatordto","title":"StartEmulatorDTO","text":"<p>A Pydantic model used for validating and parsing input data when starting an emulation. The model ensures that only correctly formatted data is used to initiate the process.</p> <ul> <li>emulator_sync: A string indicating synchronization details.</li> <li>format_type: A string representing the file format.</li> <li>sync_type: A string indicating the synchronization strategy.</li> <li>emulation_domain: A string specifying the emulation domain.</li> <li>max_chunk_size: An integer that denotes the maximum size of data chunks.</li> <li>timeout: An integer specifying the timeout duration.</li> </ul> <p>Example:</p> <pre><code>from dtos.emulation_dto import StartEmulatorDTO\n\n# Data dictionary for starting an emulation\ndata = {\n    \"emulator_sync\": \"sync_value\",\n    \"format_type\": \"json\",\n    \"sync_type\": \"grouped\",\n    \"emulation_domain\": \"domain_value\",\n    \"max_chunk_size\": 1024,\n    \"timeout\": 30\n}\n\n# Validate and parse the input data\nstart_dto = StartEmulatorDTO(**data)\nprint(start_dto)\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/#statusdto","title":"StatusDTO","text":"<p>An immutable dataclass representing the status of an emulation. It contains two important pieces of information:</p> <ul> <li>global_status: A string summarizing the overall status.</li> <li>threads: A dictionary where each key is a thread identifier and the value is a string representing its status.</li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#emulationstatusdto","title":"EmulationStatusDTO","text":"<p>This immutable dataclass encapsulates an emulation's unique identifier along with its corresponding status, which is represented using the <code>StatusDTO</code>:</p> <ul> <li>id: An <code>EmulationID</code> value object that uniquely identifies the emulation.</li> <li>status: An instance of <code>StatusDTO</code> that describes the overall status and thread-specific statuses.</li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#configuration-and-best-practices","title":"Configuration and Best Practices","text":"<ul> <li> <p>Immutability:   Using <code>@dataclass(frozen=True)</code> for several DTOs ensures that their instances remain immutable, thereby promoting data integrity throughout the application.</p> </li> <li> <p>Validation:   Employing Pydantic\u2019s <code>BaseModel</code> for the <code>StartEmulatorDTO</code> ensures robust input validation. This guards against erroneous data from external sources and helps maintain system reliability.</p> </li> <li> <p>Consistent Data Transfer:   By using these DTOs, the library standardizes data representation across different layers of the application, simplifying both integration and maintenance.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/dtos/#testing","title":"Testing","text":"<p>Unit tests are not included in the library by default, but you can create them as needed. To run tests for the library, navigate to the project directory and execute:</p> <pre><code>npx nx test ddd-application-dtos\n</code></pre> <p>Remember to adjust your testing configurations (for example, in <code>project.json</code>) so that test failures do not halt your build process if needed.</p>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/","title":"Emulation dto","text":""},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.EmulationScheduledDTO","title":"<code>EmulationScheduledDTO</code>  <code>dataclass</code>","text":"<p>Data Transfer Object representing an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>@dataclass(frozen=True)\nclass EmulationScheduledDTO:\n    \"\"\"Data Transfer Object representing an emulation.\"\"\"\n\n    id: EmulationID\n    emulator_sync: str\n    format_type: str\n    sync_type: str\n    emulation_domain: str\n    max_chunk_size: int\n    timeout: int\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.StartEmulatorDTO","title":"<code>StartEmulatorDTO</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>DTO for starting an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>class StartEmulatorDTO(BaseModel):\n    \"\"\"DTO for starting an emulation.\"\"\"\n\n    emulator_sync: str\n    format_type: str\n    sync_type: str\n    max_chunk_size: int\n    emulation_domain: str\n    max_chunk_size: int\n    timeout: int\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.StatusDTO","title":"<code>StatusDTO</code>  <code>dataclass</code>","text":"<p>Data Transfer Object representing the status of an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>@dataclass(frozen=True)\nclass StatusDTO:\n    \"\"\"Data Transfer Object representing the status of an emulation.\"\"\"\n\n    global_status: str\n    threads: dict[str, str]\n</code></pre>"},{"location":"reference/libs/ddd/application/dtos/code_reference/dtos/emulation_dto/#libs.ddd.application.dtos.dtos.emulation_dto.EmulationStatusDTO","title":"<code>EmulationStatusDTO</code>  <code>dataclass</code>","text":"<p>Data Transfer Object representing the status of an emulation.</p> Source code in <code>libs/ddd/application/dtos/dtos/emulation_dto.py</code> <pre><code>@dataclass(frozen=True)\nclass EmulationStatusDTO:\n    \"\"\"Data Transfer Object representing the status of an emulation.\"\"\"\n\n    id: EmulationID\n    status: StatusDTO\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/","title":"Use Cases","text":"<p>The Use Cases Library encapsulates the core business logic of your application. It orchestrates multiple components\u2014such as messaging producers, fake data generators, and storage clients\u2014to implement real-world scenarios. A prime example is the Start Emulator Use Case, which initiates an emulation process by coordinating between Kafka or Minio producers, data factories, and background tasks provided by FastAPI.</p>"},{"location":"reference/libs/ddd/application/usecases/#features","title":"Features","text":"<ul> <li> <p>Abstract Sync Producer Interface:   The library defines a consistent interface, <code>SyncProducer</code>, which standardizes methods like <code>produce()</code>, <code>flush()</code>, and <code>setup_resource()</code> for synchronous message production. This abstraction allows seamless integration with different backend systems (e.g., Kafka or Minio).</p> </li> <li> <p>Producer Wrapper Implementations:   The library includes concrete producer wrappers:</p> </li> <li> <p>KafkaFactorySyncProducerWrapper:     Uses Confluent Kafka\u2019s producer to send JSON messages. It automatically creates topics if they do not exist.</p> </li> <li> <p>MinioFactorySyncProducerWrapper:     Wraps a Minio client to upload JSON or CSV messages as objects into a bucket. The implementation supports two modes\u2014\u201cgrouped\u201d and \u201cchunked\u201d\u2014with proper buffering and flushing mechanisms.</p> </li> <li> <p>Flexible Data Emulation:   By leveraging various fake data factories, the use cases generate realistic data for domains such as transactions, device logs, or user profiles. This feature is useful for testing, simulating operational environments, or batch processing.</p> </li> <li> <p>Background Task Scheduling:   The use case integrates with FastAPI\u2019s <code>BackgroundTasks</code> to schedule long-running, non-blocking emulation tasks. This allows continuous data production without impacting the responsiveness of your main API.</p> </li> <li> <p>Resource Setup and Parallel Production:   The use case:</p> </li> <li>Automatically sets up required resources (e.g., creates Kafka topics or Minio buckets) if they are not already provisioned.</li> <li>Supports parallel data production by spawning multiple threads. A global stop event and timer ensure a graceful shutdown once a configured timeout is reached.</li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#installation","title":"Installation","text":"<p>To include the Use Cases Library in your monorepo, run the following command:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-application-usecases --local\n</code></pre> <p>Ensure that all necessary dependencies (e.g., <code>confluent-kafka</code>, <code>fastapi</code>, <code>Faker</code>, your logger library, etc.) are installed via your dependency manager (such as Poetry).</p>"},{"location":"reference/libs/ddd/application/usecases/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/application/usecases/#instantiating-producer-wrappers","title":"Instantiating Producer Wrappers","text":"<p>Choose the appropriate producer wrapper based on your infrastructure:</p>"},{"location":"reference/libs/ddd/application/usecases/#kafka-producer-wrapper","title":"Kafka Producer Wrapper","text":"<pre><code>from producers.kafka.producer import KafkaProducerStrategy\nfrom ddd.application.usecases.start_emulator import KafkaFactorySyncProducerWrapper\n\n# Configure your Kafka producer strategy (set bootstrap servers, credentials, etc.)\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",   # Optional, for SASL_SSL configuration\n    kafka_password=\"your_password\"    # Optional, for SASL_SSL configuration\n)\n\n# Wrap the Kafka producer\nkafka_wrapper = KafkaFactorySyncProducerWrapper(\n    kafka_producer=kafka_producer,\n    kafka_brokers=\"localhost:9092\"\n)\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#minio-producer-wrapper","title":"Minio Producer Wrapper","text":"<pre><code>from storage.minio.storage import MinioStorageClient\nfrom ddd.application.usecases.start_emulator import MinioFactorySyncProducerWrapper\n\n# Initialize the Minio client using your endpoint and credentials.\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False  # Set True if using HTTPS.\n)\n\n# Wrap the Minio client.\nminio_wrapper = MinioFactorySyncProducerWrapper(minio_client)\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#starting-the-emulator","title":"Starting the Emulator","text":"<p>The Start Emulator Use Case coordinates the emulation process by:</p> <ul> <li>Determining the target resource (Kafka topic or Minio bucket) based on the specified domain.</li> <li>Selecting the correct fake data factory for generating realistic records.</li> <li>Scheduling a background task that spawns multiple threads to continuously produce data until the emulation timeout is reached.</li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#example-with-fastapi","title":"Example (with FastAPI)","text":"<pre><code>from fastapi import FastAPI, BackgroundTasks\nfrom ddd.application.usecases.start_emulator import StartEmulatorUseCase\nfrom producers.kafka.producer import KafkaProducerStrategy\nfrom storage.minio.storage import MinioStorageClient\nfrom dtos.emulation_dto import StartEmulatorDTO\n\n# Create your FastAPI application.\napp = FastAPI()\n\n# Instantiate Kafka and Minio components.\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",\n    kafka_password=\"your_password\"\n)\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False\n)\n\n# Create the Start Emulator Use Case.\nstart_emulator_usecase = StartEmulatorUseCase(\n    kafka_producer=kafka_producer,\n    kafka_brokers=\"localhost:9092\",\n    minio_client=minio_client\n)\n\n@app.post(\"/start-emulator\")\ndef start_emulator(dto: StartEmulatorDTO, background_tasks: BackgroundTasks):\n    # Specify the number of parallel threads (e.g., 5).\n    emulation_scheduled = start_emulator_usecase.execute(dto, background_tasks, num_threads=5)\n    return emulation_scheduled\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/#key-components","title":"Key Components","text":"<ul> <li> <p>SyncProducer (Abstract Base Class):   Establishes a common contract for synchronous message producers via methods such as <code>produce()</code>, <code>flush()</code>, and <code>setup_resource()</code>. This abstraction provides consistency across different messaging systems.</p> </li> <li> <p>Producer Wrappers:</p> </li> <li> <p>KafkaFactorySyncProducerWrapper:     Leverages Confluent Kafka\u2019s producer to send JSON payloads and automatically creates topics if missing.</p> </li> <li> <p>MinioFactorySyncProducerWrapper:     Uses a Minio client to upload messages (as JSON or CSV) to a bucket. It supports grouped or chunked buffering with automatic flushing.</p> </li> <li> <p>Background Emulation Task:   The use case schedules a background task that spawns multiple threads. Each thread repeatedly produces messages using a fake data factory until a specified timeout triggers a graceful stop.</p> </li> <li> <p>Resource and Factory Mapping:   The use case includes internal mappings that associate emulation domains (e.g., \u201ctransaction\u201d, \u201cuser-profile\u201d, \u201cdevice-log\u201d) with target topics/buckets and their respective fake data factories.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Resource Mapping:   The emulator maps different emulation domains to corresponding target resources (Kafka topics or Minio buckets). A default resource is used for domains that are not explicitly supported.</p> </li> <li> <p>Fake Data Factories:   Each domain (e.g., \"transaction\", \"device-log\", etc.) is associated with a fake data factory that generates realistic records, ensuring the emulation closely resembles actual operational data flows.</p> </li> <li> <p>Parallel Processing:   The use case supports high-throughput production through multi-threading. A stop event and timer guarantee that production terminates gracefully after the configured timeout.</p> </li> </ul>"},{"location":"reference/libs/ddd/application/usecases/#testing","title":"Testing","text":"<p>The repository includes a comprehensive test suite covering:</p> <ul> <li>Producer wrapper selection and resource setup.</li> <li>Correct execution of background tasks and parallel data production.</li> <li>Validation of the emulation logic and fake data generation.</li> </ul> <p>To run the tests:</p> <pre><code>npx nx test ddd-application-usecases\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/","title":"Start emulator","text":""},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer","title":"<code>SyncProducer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for synchronous producers.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class SyncProducer(ABC):\n    \"\"\"Abstract base class for synchronous producers.\"\"\"\n\n    @abstractmethod\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"Produces a message to the given topic.\"\"\"\n        pass\n\n    @abstractmethod\n    def flush(self) -&gt; None:\n        \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n        pass\n\n    @abstractmethod\n    def setup_resource(self, topic: str) -&gt; None:\n        \"\"\"Sets up the producer resource (e.g., create a topic or bucket).\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.produce","title":"<code>produce(topic, key, value)</code>  <code>abstractmethod</code>","text":"<p>Produces a message to the given topic.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"Produces a message to the given topic.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.flush","title":"<code>flush()</code>  <code>abstractmethod</code>","text":"<p>Flushes the producer, ensuring all messages are sent.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef flush(self) -&gt; None:\n    \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.SyncProducer.setup_resource","title":"<code>setup_resource(topic)</code>  <code>abstractmethod</code>","text":"<p>Sets up the producer resource (e.g., create a topic or bucket).</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>@abstractmethod\ndef setup_resource(self, topic: str) -&gt; None:\n    \"\"\"Sets up the producer resource (e.g., create a topic or bucket).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper","title":"<code>KafkaFactorySyncProducerWrapper</code>","text":"<p>               Bases: <code>SyncProducer</code></p> <p>Synchronous Kafka producer wrapper implementation.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class KafkaFactorySyncProducerWrapper(SyncProducer):\n    \"\"\"Synchronous Kafka producer wrapper implementation.\"\"\"\n\n    def __init__(\n        self,\n        kafka_producer: KafkaProducerStrategy,\n        kafka_brokers: str,\n        num_partitions: int = 5,\n        replication_factor: int = 2,\n    ):\n        self.kafka_producer = kafka_producer\n        self.kafka_brokers = kafka_brokers\n        self.num_partitions = num_partitions\n        self.replication_factor = replication_factor\n\n    def setup_resource(self, topic: str) -&gt; None:\n        \"\"\"Creates the Kafka topic if it does not already exist.\"\"\"\n        admin_client = AdminClient({\"bootstrap.servers\": self.kafka_brokers})\n        metadata = admin_client.list_topics(timeout=10)\n        if topic not in metadata.topics:\n            new_topic = NewTopic(\n                topic=topic,\n                num_partitions=self.num_partitions,\n                replication_factor=self.replication_factor,\n            )\n            fs = admin_client.create_topics([new_topic])\n            for t, future in fs.items():\n                try:\n                    future.result()\n                    logger.info(f\"Topic {t} created successfully\")\n                except Exception as e:\n                    logger.error(f\"Failed to create topic {t}: {e}\")\n        else:\n            logger.info(f\"Topic {topic} already exists\")\n\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Produces a message to a Kafka topic.\n\n        Args:\n            topic (str): The Kafka topic to send the message to.\n            key (str): The key of the message.\n            value (dict[str, Any]): The message payload.\n        \"\"\"\n        try:\n            self.kafka_producer.produce(topic=topic, key=key, value=json.dumps(value))\n        except Exception as e:\n            logger.error(f\"Kafka production error: {e}\")\n\n    def flush(self) -&gt; None:\n        \"\"\"Flushes the Kafka producer.\"\"\"\n        self.kafka_producer.flush()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.setup_resource","title":"<code>setup_resource(topic)</code>","text":"<p>Creates the Kafka topic if it does not already exist.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def setup_resource(self, topic: str) -&gt; None:\n    \"\"\"Creates the Kafka topic if it does not already exist.\"\"\"\n    admin_client = AdminClient({\"bootstrap.servers\": self.kafka_brokers})\n    metadata = admin_client.list_topics(timeout=10)\n    if topic not in metadata.topics:\n        new_topic = NewTopic(\n            topic=topic,\n            num_partitions=self.num_partitions,\n            replication_factor=self.replication_factor,\n        )\n        fs = admin_client.create_topics([new_topic])\n        for t, future in fs.items():\n            try:\n                future.result()\n                logger.info(f\"Topic {t} created successfully\")\n            except Exception as e:\n                logger.error(f\"Failed to create topic {t}: {e}\")\n    else:\n        logger.info(f\"Topic {topic} already exists\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.produce","title":"<code>produce(topic, key, value)</code>","text":"<p>Produces a message to a Kafka topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>str</code> <p>The Kafka topic to send the message to.</p> required <code>key</code> <code>str</code> <p>The key of the message.</p> required <code>value</code> <code>dict[str, Any]</code> <p>The message payload.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Produces a message to a Kafka topic.\n\n    Args:\n        topic (str): The Kafka topic to send the message to.\n        key (str): The key of the message.\n        value (dict[str, Any]): The message payload.\n    \"\"\"\n    try:\n        self.kafka_producer.produce(topic=topic, key=key, value=json.dumps(value))\n    except Exception as e:\n        logger.error(f\"Kafka production error: {e}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.KafkaFactorySyncProducerWrapper.flush","title":"<code>flush()</code>","text":"<p>Flushes the Kafka producer.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flushes the Kafka producer.\"\"\"\n    self.kafka_producer.flush()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper","title":"<code>MinioFactorySyncProducerWrapper</code>","text":"<p>               Bases: <code>SyncProducer</code></p> <p>Synchronous Minio producer wrapper implementation.</p> <p>This implementation uses a MinioClient to upload messages as individual objects.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class MinioFactorySyncProducerWrapper(SyncProducer):\n    \"\"\"Synchronous Minio producer wrapper implementation.\n\n    This implementation uses a MinioClient to upload messages as individual objects.\n    \"\"\"\n\n    def __init__(\n        self,\n        minio_client: MinioStorageClient,\n        sync_type: str,\n        format_type: str,\n        max_chunk_size: int = None,\n    ):\n        self.minio_client = minio_client\n        self.bucket = None\n        self.sync_type = sync_type.lower()\n        self.format_type = format_type.lower()\n        self.max_chunk_size = max_chunk_size\n        self.lock = threading.Lock()\n        if self.sync_type in (\"grouped\", \"chunked\"):\n            self.buffer = []\n            self.current_chunk_size = 0\n\n    def setup_resource(self, bucket_name: str) -&gt; None:\n        \"\"\"\n        Creates a bucket on Minio if it does not already exist.\n\n        Args:\n            bucket_name (str): The name of the bucket (used as the \"topic\").\n        \"\"\"\n        self.bucket = bucket_name\n        buckets = self.minio_client.list_buckets()\n        if bucket_name not in buckets:\n            self.minio_client.create_bucket(bucket_name)\n            logger.info(f\"Bucket {bucket_name} created successfully\")\n        else:\n            logger.info(f\"Bucket {bucket_name} already exists\")\n\n    def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Produces a message to the Minio bucket.\n        Args:\n            topic (str): The Minio bucket name (used as the \"topic\").\n            key (str): The key of the message.\n            value (dict[str, Any]): The message payload.\n        \"\"\"\n        if self.sync_type in (\"grouped\", \"chunked\"):\n            serialized_value = json.dumps(value)\n            message_bytes = serialized_value.encode(\"utf-8\")\n            message_size = len(message_bytes)\n\n            with self.lock:\n                self.buffer.append(value)\n                if self.sync_type == \"chunked\":\n                    self.current_chunk_size += message_size\n                    if (\n                        self.max_chunk_size is not None\n                        and self.current_chunk_size &gt;= self.max_chunk_size\n                    ):\n                        self._flush_current_chunk()\n        else:\n            message_bytes = json.dumps(value).encode(\"utf-8\")\n            object_name = f\"{key}_{int(time.time() * 1000)}.{self.format_type}\"\n            with self.lock:\n                self.minio_client.upload_bytes(self.bucket, object_name, message_bytes)\n            logger.info(f\"Uploaded object {object_name} to bucket {self.bucket}\")\n\n    def _flush_current_chunk(self) -&gt; None:\n        \"\"\"Flushes the current chunk of messages to Minio.\"\"\"\n        if not self.buffer:\n            return\n\n        if self.format_type == \"json\":\n            aggregate_data = json.dumps(self.buffer)\n        elif self.format_type == \"csv\":\n            aggregate_data = convert_to_csv(self.buffer)\n        else:\n            aggregate_data = json.dumps(self.buffer)\n\n        object_name = f\"aggregated_{int(time.time() * 1000)}.{self.format_type}\"\n        self.minio_client.upload_bytes(\n            self.bucket, object_name, aggregate_data.encode(\"utf-8\")\n        )\n        logger.info(\n            f\"Chunk flushed: {object_name} with size {self.current_chunk_size} bytes\"\n        )\n        self.buffer.clear()\n        self.current_chunk_size = 0\n\n    def flush(self) -&gt; None:\n        \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n        if self.sync_type in (\"grouped\", \"chunked\"):\n            with self.lock:\n                self._flush_current_chunk()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.setup_resource","title":"<code>setup_resource(bucket_name)</code>","text":"<p>Creates a bucket on Minio if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the bucket (used as the \"topic\").</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def setup_resource(self, bucket_name: str) -&gt; None:\n    \"\"\"\n    Creates a bucket on Minio if it does not already exist.\n\n    Args:\n        bucket_name (str): The name of the bucket (used as the \"topic\").\n    \"\"\"\n    self.bucket = bucket_name\n    buckets = self.minio_client.list_buckets()\n    if bucket_name not in buckets:\n        self.minio_client.create_bucket(bucket_name)\n        logger.info(f\"Bucket {bucket_name} created successfully\")\n    else:\n        logger.info(f\"Bucket {bucket_name} already exists\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.produce","title":"<code>produce(topic, key, value)</code>","text":"<p>Produces a message to the Minio bucket. Args:     topic (str): The Minio bucket name (used as the \"topic\").     key (str): The key of the message.     value (dict[str, Any]): The message payload.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce(self, topic: str, key: str, value: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Produces a message to the Minio bucket.\n    Args:\n        topic (str): The Minio bucket name (used as the \"topic\").\n        key (str): The key of the message.\n        value (dict[str, Any]): The message payload.\n    \"\"\"\n    if self.sync_type in (\"grouped\", \"chunked\"):\n        serialized_value = json.dumps(value)\n        message_bytes = serialized_value.encode(\"utf-8\")\n        message_size = len(message_bytes)\n\n        with self.lock:\n            self.buffer.append(value)\n            if self.sync_type == \"chunked\":\n                self.current_chunk_size += message_size\n                if (\n                    self.max_chunk_size is not None\n                    and self.current_chunk_size &gt;= self.max_chunk_size\n                ):\n                    self._flush_current_chunk()\n    else:\n        message_bytes = json.dumps(value).encode(\"utf-8\")\n        object_name = f\"{key}_{int(time.time() * 1000)}.{self.format_type}\"\n        with self.lock:\n            self.minio_client.upload_bytes(self.bucket, object_name, message_bytes)\n        logger.info(f\"Uploaded object {object_name} to bucket {self.bucket}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.MinioFactorySyncProducerWrapper.flush","title":"<code>flush()</code>","text":"<p>Flushes the producer, ensuring all messages are sent.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def flush(self) -&gt; None:\n    \"\"\"Flushes the producer, ensuring all messages are sent.\"\"\"\n    if self.sync_type in (\"grouped\", \"chunked\"):\n        with self.lock:\n            self._flush_current_chunk()\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.ProducerWrapperFactory","title":"<code>ProducerWrapperFactory</code>","text":"<p>Factory class for creating producer wrappers based on the sync type.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class ProducerWrapperFactory:\n    \"\"\"Factory class for creating producer wrappers based on the sync type.\"\"\"\n\n    def __init__(\n        self,\n        kafka_producer: KafkaProducerStrategy,\n        kafka_brokers: str,\n        minio_client: MinioStorageClient,\n    ):\n        self.kafka_producer = kafka_producer\n        self.kafka_brokers = kafka_brokers\n        self.minio_client = minio_client\n\n    def create_producer_wrapper(self, dto: StartEmulatorDTO) -&gt; SyncProducer:\n        \"\"\"\n        Creates a producer wrapper based on the sync type specified in the DTO.\n        Args:\n            dto (StartEmulatorDTO): The DTO containing emulator parameters.\n        Returns:\n            SyncProducer: An instance of the appropriate producer wrapper.\n        Raises:\n            ValueError: If the sync type is not supported.\n        \"\"\"\n        if dto.emulator_sync == \"minio\":\n            return MinioFactorySyncProducerWrapper(\n                minio_client=self.minio_client,\n                sync_type=dto.sync_type,\n                format_type=dto.format_type,\n                max_chunk_size=dto.max_chunk_size,\n            )\n        elif dto.emulator_sync == \"kafka\":\n            return KafkaFactorySyncProducerWrapper(\n                self.kafka_producer,\n                self.kafka_brokers,\n            )\n        else:\n            raise ValueError(f\"Unsupported emulator sync type: {dto.emulator_sync}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.ProducerWrapperFactory.create_producer_wrapper","title":"<code>create_producer_wrapper(dto)</code>","text":"<p>Creates a producer wrapper based on the sync type specified in the DTO. Args:     dto (StartEmulatorDTO): The DTO containing emulator parameters. Returns:     SyncProducer: An instance of the appropriate producer wrapper. Raises:     ValueError: If the sync type is not supported.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def create_producer_wrapper(self, dto: StartEmulatorDTO) -&gt; SyncProducer:\n    \"\"\"\n    Creates a producer wrapper based on the sync type specified in the DTO.\n    Args:\n        dto (StartEmulatorDTO): The DTO containing emulator parameters.\n    Returns:\n        SyncProducer: An instance of the appropriate producer wrapper.\n    Raises:\n        ValueError: If the sync type is not supported.\n    \"\"\"\n    if dto.emulator_sync == \"minio\":\n        return MinioFactorySyncProducerWrapper(\n            minio_client=self.minio_client,\n            sync_type=dto.sync_type,\n            format_type=dto.format_type,\n            max_chunk_size=dto.max_chunk_size,\n        )\n    elif dto.emulator_sync == \"kafka\":\n        return KafkaFactorySyncProducerWrapper(\n            self.kafka_producer,\n            self.kafka_brokers,\n        )\n    else:\n        raise ValueError(f\"Unsupported emulator sync type: {dto.emulator_sync}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase","title":"<code>StartEmulatorUseCase</code>","text":"<p>Use case for starting the data emulator with flexible sync strategies.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>class StartEmulatorUseCase:\n    \"\"\"Use case for starting the data emulator with flexible sync strategies.\"\"\"\n\n    def __init__(\n        self,\n        kafka_producer: KafkaProducerStrategy,\n        kafka_brokers: str,\n        minio_client: MinioStorageClient,\n        repository: InMemoryRepository,\n    ):\n        self.repository = repository\n        self.topics_mapping = {\n            \"transaction\": \"transactions\",\n            \"user-profile\": \"user-profiles\",\n            \"device-log\": \"device-logs\",\n            \"default\": \"default_topic\",\n        }\n        self.fake_factories: dict[str, Any] = {\n            \"transaction\": TransactionFakeFactory,\n            \"user-profile\": UserProfileFactory,\n            \"device-log\": DeviceLogFactory,\n        }\n        self.producer_wrapper_factory = ProducerWrapperFactory(\n            kafka_producer=kafka_producer,\n            kafka_brokers=kafka_brokers,\n            minio_client=minio_client,\n        )\n\n    def execute(\n        self, dto: StartEmulatorDTO, background_tasks: BackgroundTasks, num_threads: int\n    ) -&gt; EmulationScheduledDTO:\n        \"\"\"\n        Executes the emulator and schedules the background emulation task.\n\n        Args:\n            dto (StartEmulatorDTO): The DTO containing emulator parameters.\n            background_tasks (BackgroundTasks): FastAPI background tasks manager.\n            num_threads (int): Number of parallel threads to run.\n\n        Returns:\n            EmulationScheduledDTO: DTO with details about the scheduled emulation.\n\n        Raises:\n            ValueError: If the sync type or domain is not supported.\n        \"\"\"\n        emulation_id = EmulationID.generate()\n        self.repository.create_status(emulation_id.value, \"processing\")\n\n        # Determine the target topic (or bucket name) based on the emulation domain.\n        domain = dto.emulation_domain.lower()\n        topic = self.topics_mapping.get(domain, self.topics_mapping[\"default\"])\n        producer_wrapper = self.producer_wrapper_factory.create_producer_wrapper(dto)\n        producer_wrapper.setup_resource(topic)\n\n        # Retrieve the appropriate fake factory for the specified domain.\n        fake_factory_class = self.fake_factories.get(domain)\n        if fake_factory_class is None:\n            raise ValueError(f\"Domain not supported: {dto.emulation_domain}\")\n        fake_factory = fake_factory_class()\n\n        # Schedule the background emulation task.\n        background_tasks.add_task(\n            self._run_emulation_task,\n            emulation_id,\n            producer_wrapper,\n            topic,\n            fake_factory,\n            dto.timeout,\n            num_threads,\n        )\n\n        return EmulationScheduledDTO(\n            id=emulation_id,\n            emulator_sync=dto.emulator_sync,\n            emulation_domain=dto.emulation_domain,\n            format_type=dto.format_type,\n            sync_type=dto.sync_type,\n            max_chunk_size=dto.max_chunk_size,\n            timeout=dto.timeout,\n        )\n\n    def produce_data(\n        self,\n        emulation_id: Any,\n        thread_id: int,\n        producer: SyncProducer,\n        topic: str,\n        stop_event: threading.Event,\n        factory: Any,\n    ) -&gt; None:\n        \"\"\"\n        Produces data continuously until the stop event is triggered.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            thread_id (int): Identifier for the thread.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            stop_event (threading.Event): Event to signal when to stop production.\n            factory(Any): The fake data factory.\n        \"\"\"\n        self.repository.update_thread_status(emulation_id.value, thread_id, \"started\")\n        while not stop_event.is_set():\n            fake_data = factory.generate()\n            if fake_data is None:\n                logger.info(f\"Thread {thread_id} - No more data to process\")\n                break\n            message_payload = {\n                \"emulation_id\": str(emulation_id.value),\n                \"timestamp\": time.time(),\n                \"data\": fake_data,\n            }\n            try:\n                key = fake_data.get(\"transaction_id\", str(time.time()))\n                producer.produce(topic=topic, key=key, value=message_payload)\n                logger.info(f\"Thread {thread_id} - Produced message: {message_payload}\")\n            except Exception as e:\n                logger.error(f\"Failed to produce message: {e}\")\n                self.repository.update_thread_status(\n                    emulation_id.value, thread_id, \"error\"\n                )\n        self.repository.update_thread_status(emulation_id.value, thread_id, \"finished\")\n\n    def produce_data_in_parallel(\n        self,\n        emulation_id: Any,\n        producer: SyncProducer,\n        topic: str,\n        factory: Any,\n        stop_event: threading.Event,\n        num_threads: int,\n    ) -&gt; None:\n        \"\"\"\n        Starts multiple threads to produce data in parallel.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            factory (Any): The fake data factory.\n            stop_event (threading.Event): Event to signal when to stop production.\n            num_threads (int): Number of parallel threads to run.\n        \"\"\"\n        threads = []\n        try:\n            for i in range(num_threads):\n                thread = threading.Thread(\n                    target=self.produce_data,\n                    args=(emulation_id, i, producer, topic, stop_event, factory),\n                )\n                thread.daemon = True\n                thread.start()\n                threads.append(thread)\n            for thread in threads:\n                thread.join()\n        except Exception as e:\n            logger.error(f\"Failed to start threads: {e}\")\n\n    def _run_emulation_task(\n        self,\n        emulation_id: Any,\n        producer: SyncProducer,\n        topic: str,\n        factory: Any,\n        timeout: float,\n        num_threads: int,\n    ) -&gt; None:\n        \"\"\"\n        Runs the emulation task until the specified timeout elapses.\n\n        Args:\n            emulation_id (Any): Unique emulation identifier.\n            producer (SyncProducer): The producer instance.\n            topic (str): Target topic or output resource.\n            factory (Any): The fake data factory.\n            timeout (float): Emulation duration in seconds.\n            num_threads (int): Number of parallel threads to run.\n        \"\"\"\n        stop_event = threading.Event()\n        timer = threading.Timer(timeout, stop_event.set)\n        timer.start()\n\n        self.produce_data_in_parallel(\n            emulation_id, producer, topic, factory, stop_event, num_threads\n        )\n        timer.cancel()\n        producer.flush()\n        logger.info(\"Emulation finished\")\n        self.repository.update_status(emulation_id.value, \"completed\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.execute","title":"<code>execute(dto, background_tasks, num_threads)</code>","text":"<p>Executes the emulator and schedules the background emulation task.</p> <p>Parameters:</p> Name Type Description Default <code>dto</code> <code>StartEmulatorDTO</code> <p>The DTO containing emulator parameters.</p> required <code>background_tasks</code> <code>BackgroundTasks</code> <p>FastAPI background tasks manager.</p> required <code>num_threads</code> <code>int</code> <p>Number of parallel threads to run.</p> required <p>Returns:</p> Name Type Description <code>EmulationScheduledDTO</code> <code>EmulationScheduledDTO</code> <p>DTO with details about the scheduled emulation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sync type or domain is not supported.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def execute(\n    self, dto: StartEmulatorDTO, background_tasks: BackgroundTasks, num_threads: int\n) -&gt; EmulationScheduledDTO:\n    \"\"\"\n    Executes the emulator and schedules the background emulation task.\n\n    Args:\n        dto (StartEmulatorDTO): The DTO containing emulator parameters.\n        background_tasks (BackgroundTasks): FastAPI background tasks manager.\n        num_threads (int): Number of parallel threads to run.\n\n    Returns:\n        EmulationScheduledDTO: DTO with details about the scheduled emulation.\n\n    Raises:\n        ValueError: If the sync type or domain is not supported.\n    \"\"\"\n    emulation_id = EmulationID.generate()\n    self.repository.create_status(emulation_id.value, \"processing\")\n\n    # Determine the target topic (or bucket name) based on the emulation domain.\n    domain = dto.emulation_domain.lower()\n    topic = self.topics_mapping.get(domain, self.topics_mapping[\"default\"])\n    producer_wrapper = self.producer_wrapper_factory.create_producer_wrapper(dto)\n    producer_wrapper.setup_resource(topic)\n\n    # Retrieve the appropriate fake factory for the specified domain.\n    fake_factory_class = self.fake_factories.get(domain)\n    if fake_factory_class is None:\n        raise ValueError(f\"Domain not supported: {dto.emulation_domain}\")\n    fake_factory = fake_factory_class()\n\n    # Schedule the background emulation task.\n    background_tasks.add_task(\n        self._run_emulation_task,\n        emulation_id,\n        producer_wrapper,\n        topic,\n        fake_factory,\n        dto.timeout,\n        num_threads,\n    )\n\n    return EmulationScheduledDTO(\n        id=emulation_id,\n        emulator_sync=dto.emulator_sync,\n        emulation_domain=dto.emulation_domain,\n        format_type=dto.format_type,\n        sync_type=dto.sync_type,\n        max_chunk_size=dto.max_chunk_size,\n        timeout=dto.timeout,\n    )\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.produce_data","title":"<code>produce_data(emulation_id, thread_id, producer, topic, stop_event, factory)</code>","text":"<p>Produces data continuously until the stop event is triggered.</p> <p>Parameters:</p> Name Type Description Default <code>emulation_id</code> <code>Any</code> <p>Unique emulation identifier.</p> required <code>thread_id</code> <code>int</code> <p>Identifier for the thread.</p> required <code>producer</code> <code>SyncProducer</code> <p>The producer instance.</p> required <code>topic</code> <code>str</code> <p>Target topic or output resource.</p> required <code>stop_event</code> <code>Event</code> <p>Event to signal when to stop production.</p> required <code>factory(Any)</code> <p>The fake data factory.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce_data(\n    self,\n    emulation_id: Any,\n    thread_id: int,\n    producer: SyncProducer,\n    topic: str,\n    stop_event: threading.Event,\n    factory: Any,\n) -&gt; None:\n    \"\"\"\n    Produces data continuously until the stop event is triggered.\n\n    Args:\n        emulation_id (Any): Unique emulation identifier.\n        thread_id (int): Identifier for the thread.\n        producer (SyncProducer): The producer instance.\n        topic (str): Target topic or output resource.\n        stop_event (threading.Event): Event to signal when to stop production.\n        factory(Any): The fake data factory.\n    \"\"\"\n    self.repository.update_thread_status(emulation_id.value, thread_id, \"started\")\n    while not stop_event.is_set():\n        fake_data = factory.generate()\n        if fake_data is None:\n            logger.info(f\"Thread {thread_id} - No more data to process\")\n            break\n        message_payload = {\n            \"emulation_id\": str(emulation_id.value),\n            \"timestamp\": time.time(),\n            \"data\": fake_data,\n        }\n        try:\n            key = fake_data.get(\"transaction_id\", str(time.time()))\n            producer.produce(topic=topic, key=key, value=message_payload)\n            logger.info(f\"Thread {thread_id} - Produced message: {message_payload}\")\n        except Exception as e:\n            logger.error(f\"Failed to produce message: {e}\")\n            self.repository.update_thread_status(\n                emulation_id.value, thread_id, \"error\"\n            )\n    self.repository.update_thread_status(emulation_id.value, thread_id, \"finished\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.StartEmulatorUseCase.produce_data_in_parallel","title":"<code>produce_data_in_parallel(emulation_id, producer, topic, factory, stop_event, num_threads)</code>","text":"<p>Starts multiple threads to produce data in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>emulation_id</code> <code>Any</code> <p>Unique emulation identifier.</p> required <code>producer</code> <code>SyncProducer</code> <p>The producer instance.</p> required <code>topic</code> <code>str</code> <p>Target topic or output resource.</p> required <code>factory</code> <code>Any</code> <p>The fake data factory.</p> required <code>stop_event</code> <code>Event</code> <p>Event to signal when to stop production.</p> required <code>num_threads</code> <code>int</code> <p>Number of parallel threads to run.</p> required Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def produce_data_in_parallel(\n    self,\n    emulation_id: Any,\n    producer: SyncProducer,\n    topic: str,\n    factory: Any,\n    stop_event: threading.Event,\n    num_threads: int,\n) -&gt; None:\n    \"\"\"\n    Starts multiple threads to produce data in parallel.\n\n    Args:\n        emulation_id (Any): Unique emulation identifier.\n        producer (SyncProducer): The producer instance.\n        topic (str): Target topic or output resource.\n        factory (Any): The fake data factory.\n        stop_event (threading.Event): Event to signal when to stop production.\n        num_threads (int): Number of parallel threads to run.\n    \"\"\"\n    threads = []\n    try:\n        for i in range(num_threads):\n            thread = threading.Thread(\n                target=self.produce_data,\n                args=(emulation_id, i, producer, topic, stop_event, factory),\n            )\n            thread.daemon = True\n            thread.start()\n            threads.append(thread)\n        for thread in threads:\n            thread.join()\n    except Exception as e:\n        logger.error(f\"Failed to start threads: {e}\")\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/start_emulator/#libs.ddd.application.usecases.usecases.start_emulator.convert_to_csv","title":"<code>convert_to_csv(dict_list)</code>","text":"<p>Converts a list of dictionaries to a CSV string. Each dictionary represents a row, and the keys are the column headers. Args:     dict_list (list): List of dictionaries to convert. Returns:     str: CSV formatted string.</p> Source code in <code>libs/ddd/application/usecases/usecases/start_emulator.py</code> <pre><code>def convert_to_csv(dict_list) -&gt; str:\n    \"\"\"\n    Converts a list of dictionaries to a CSV string.\n    Each dictionary represents a row, and the keys are the column headers.\n    Args:\n        dict_list (list): List of dictionaries to convert.\n    Returns:\n        str: CSV formatted string.\n    \"\"\"\n    if not dict_list:\n        return \"\"\n    headers = list(dict_list[0].keys())\n    lines = [\",\".join(headers)]\n    for data in dict_list:\n        row = \",\".join(str(data.get(key, \"\")) for key in headers)\n        lines.append(row)\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/status_emulation/","title":"Status emulation","text":""},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/status_emulation/#libs.ddd.application.usecases.usecases.status_emulation.StatusEmulatorUseCase","title":"<code>StatusEmulatorUseCase</code>","text":"<p>Use case for checking the status of the emulator. This use case checks if the emulator is running and returns its status.</p> Source code in <code>libs/ddd/application/usecases/usecases/status_emulation.py</code> <pre><code>class StatusEmulatorUseCase:\n    \"\"\"\n    Use case for checking the status of the emulator.\n    This use case checks if the emulator is running and returns its status.\n    \"\"\"\n\n    def __init__(self, repository: InMemoryRepository):\n        self.repository = repository\n\n    def execute(self, emulation_id: str) -&gt; EmulationStatusDTO:\n        \"\"\"\n        Execute the use case to check the status of the emulator.\n        Returns a dictionary containing the status of the emulator.\n        \"\"\"\n        logger.info(\"Checking emulator status...\")\n        status = self.repository.get_status(emulation_id)\n        if status is None:\n            logger.error(f\"Emulation ID {emulation_id} not found.\")\n            raise ValueError(f\"Emulation ID {emulation_id} not found.\")\n        return EmulationStatusDTO(id=emulation_id, status=StatusDTO(**status))\n</code></pre>"},{"location":"reference/libs/ddd/application/usecases/code_reference/usecases/status_emulation/#libs.ddd.application.usecases.usecases.status_emulation.StatusEmulatorUseCase.execute","title":"<code>execute(emulation_id)</code>","text":"<p>Execute the use case to check the status of the emulator. Returns a dictionary containing the status of the emulator.</p> Source code in <code>libs/ddd/application/usecases/usecases/status_emulation.py</code> <pre><code>def execute(self, emulation_id: str) -&gt; EmulationStatusDTO:\n    \"\"\"\n    Execute the use case to check the status of the emulator.\n    Returns a dictionary containing the status of the emulator.\n    \"\"\"\n    logger.info(\"Checking emulator status...\")\n    status = self.repository.get_status(emulation_id)\n    if status is None:\n        logger.error(f\"Emulation ID {emulation_id} not found.\")\n        raise ValueError(f\"Emulation ID {emulation_id} not found.\")\n    return EmulationStatusDTO(id=emulation_id, status=StatusDTO(**status))\n</code></pre>"},{"location":"reference/libs/ddd/entities/","title":"Entities","text":"<p>The Entities Library provides domain entities that encapsulate business logic and data for your application. It currently includes the <code>Emulation</code> entity, which represents an emulation instance with a unique identifier, a timeout value, an emulator type, and a timestamp indicating when it was created.</p>"},{"location":"reference/libs/ddd/entities/#features","title":"Features","text":"<ul> <li> <p>Unique Identification:   Each <code>Emulation</code> entity is assigned a unique identifier using the <code>EmulationID</code> value object. This ensures that every emulation instance can be uniquely referenced.</p> </li> <li> <p>Automatic Timestamping:   The entity records its creation time (<code>created_at</code>) automatically using the current UTC datetime.</p> </li> <li> <p>Flexible Data Structure:   The <code>Emulation</code> entity includes core attributes such as <code>timeout</code> and <code>emulator_type</code>, and it can be easily extended to accommodate additional properties.</p> </li> <li> <p>Easy Serialization:   The <code>to_dict</code> method converts the entity to a dictionary, making it simple to serialize data for APIs, logging, or other integrations.</p> </li> </ul>"},{"location":"reference/libs/ddd/entities/#installation","title":"Installation","text":"<p>Add the Entities library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-entities --local\n</code></pre>"},{"location":"reference/libs/ddd/entities/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/entities/#creating-an-emulation-entity","title":"Creating an Emulation Entity","text":"<p>To create an instance of an <code>Emulation</code> entity, simply import the class and instantiate it with the required parameters:</p> <pre><code>from entities.emulation import Emulation\n\n# Create an Emulation entity with a timeout of 30 and a specified emulator type.\nemulation = Emulation(timeout=30, emulator_type=\"test_emulator\")\nprint(emulation.to_dict())\n</code></pre>"},{"location":"reference/libs/ddd/entities/#understanding-the-attributes","title":"Understanding the Attributes","text":"<ul> <li> <p>id:   A unique identifier automatically generated using the <code>EmulationID</code> value object.</p> </li> <li> <p>timeout:   An integer representing the timeout period for the emulation.</p> </li> <li> <p>emulator_type:   A string specifying the type of emulator.</p> </li> <li> <p>created_at:   A UTC timestamp indicating when the emulation was created.</p> </li> </ul>"},{"location":"reference/libs/ddd/entities/#testing","title":"Testing","text":"<p>Unit tests are included to ensure that the <code>Emulation</code> entity behaves as expected. To run the tests, navigate to the library directory and execute:</p> <pre><code>npx nx test ddd-entities\n</code></pre>"},{"location":"reference/libs/ddd/entities/code_reference/entities/emulation/","title":"Emulation","text":""},{"location":"reference/libs/ddd/infra/producers/","title":"Producers","text":"<p>The Producers Library provides a unified interface for producing messages to external systems, with a focus on Kafka messaging. This library includes a base producer abstraction and a concrete Kafka producer implementation. It is designed to be extensible, allowing future support for additional messaging systems while maintaining a consistent API.</p>"},{"location":"reference/libs/ddd/infra/producers/#features","title":"Features","text":"<ul> <li>Unified Producer Interface:   Defines a standard interface (<code>BaseProducer</code>) for message production across different platforms.</li> <li> <p>Kafka Producer Implementation:   Provides a robust implementation (<code>KafkaProducerStrategy</code>) using Confluent Kafka for producing messages to Kafka topics.</p> </li> <li> <p>Configurable and Secure:   Supports both plaintext and SASL_SSL configurations for secure message production.</p> </li> <li> <p>Callback and Delivery Reporting:   Includes a delivery report mechanism that logs successful and failed message deliveries.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/producers/#installation","title":"Installation","text":"<p>To add the Producers library to your monorepo, run:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-infra-producers --local\n</code></pre> <p>Ensure your environment includes all required dependencies, such as <code>confluent_kafka</code> and your logger library.</p>"},{"location":"reference/libs/ddd/infra/producers/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/infra/producers/#instantiating-the-kafka-producer","title":"Instantiating the Kafka Producer","text":"<p>Import and create an instance of the <code>KafkaProducerStrategy</code> with the necessary configuration:</p> <pre><code>from producers.infra.producers.kafka.producer import KafkaProducerStrategy\n\n# Initialize the Kafka producer with bootstrap servers and, optionally, SASL credentials.\nkafka_producer = KafkaProducerStrategy(\n    bootstrap_servers=\"localhost:9092\",\n    kafka_username=\"your_username\",  # Optional\n    kafka_password=\"your_password\"   # Optional\n)\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/#producing-messages","title":"Producing Messages","text":"<p>To produce a message to a Kafka topic, use the <code>produce</code> method. The message value should be a dictionary that will be JSON-encoded before sending:</p> <pre><code>topic = \"my_topic\"\nkey = \"unique_key\"\nvalue = {\"event\": \"user_signup\", \"user_id\": 12345}\n\n# Produce a message to the specified topic.\nkafka_producer.produce(topic, key, value)\n</code></pre> <p>The producer immediately polls for delivery events and logs the outcome using the configured callback.</p>"},{"location":"reference/libs/ddd/infra/producers/#flushing-the-producer","title":"Flushing the Producer","text":"<p>When you need to ensure that all queued messages have been sent, call the <code>flush</code> method:</p> <pre><code># Flush any remaining messages with a timeout (in seconds).\nkafka_producer.flush(timeout=30)\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/#delivery-reporting","title":"Delivery Reporting","text":"<p>The producer's delivery report callback logs whether messages are successfully delivered or if they encountered errors. You can customize this behavior by extending or overriding the default callback.</p>"},{"location":"reference/libs/ddd/infra/producers/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Producer Configuration:   The Kafka producer is configured with options for batching, compression, and buffering. By default, it uses gzip compression and a client ID of \"emulator-producer\".</p> </li> <li> <p>If SASL credentials are provided, the producer is configured to use SASL_SSL.</p> </li> <li> <p>Otherwise, the producer uses PLAINTEXT.</p> </li> <li> <p>Callback Function:   The <code>delivery_report</code> method is used as a callback for reporting message delivery status. Successful deliveries and errors are both logged for troubleshooting and auditing.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/producers/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that all functionality works as expected. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test ddd-infra-producers\n</code></pre>"},{"location":"reference/libs/ddd/infra/producers/code_reference/producers/base_producer/","title":"Base producer","text":""},{"location":"reference/libs/ddd/infra/repository/in-memory/","title":"In-Memory Repository","text":"<p>The In-Memory Repository Library provides a lightweight, transient storage mechanism for status data during runtime. It is designed to facilitate quick state management for components such as background tasks or microservices, keeping track of global and thread-specific statuses in memory.</p>"},{"location":"reference/libs/ddd/infra/repository/in-memory/#features","title":"Features","text":"<ul> <li>In-Memory Data Storage:   Uses a Python dictionary to store status entries for quick retrieval and updates.</li> <li>Flexible Status Management:   Supports creating new status entries and updating global or thread-specific statuses.</li> <li>Thread-Safe Operations:   Employs a locking mechanism to ensure that user IDs are managed safely across threads.</li> </ul>"},{"location":"reference/libs/ddd/infra/repository/in-memory/#installation","title":"Installation","text":"<p>Add the In-Memory Repository library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-inmemory --local\n</code></pre> <p>Ensure that your environment is configured for Python development and that any necessary dependencies are installed.</p>"},{"location":"reference/libs/ddd/infra/repository/in-memory/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/infra/repository/in-memory/#creating-an-in-memory-repository-instance","title":"Creating an In-Memory Repository Instance","text":"<p>First, import and instantiate the repository:</p> <pre><code>from mem_reposiriry.in_memory_repository import InMemoryRepository\n\n# Create a repository instance\nrepo = InMemoryRepository()\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/#creating-and-updating-status","title":"Creating and Updating Status","text":"<p>You can create a new status entry, update the global status, and update the status of individual threads as follows:</p> <pre><code># Create a new status entry\nrepo.create_status(\"emulation1\", \"pending\")\n\n# Update the global status\nrepo.update_status(\"emulation1\", \"running\")\n\n# Update the status of a specific thread (e.g., thread 1)\nrepo.update_thread_status(\"emulation1\", thread_id=1, status=\"processing\")\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/#retrieving-the-status-entry","title":"Retrieving the Status Entry","text":"<p>To retrieve the status for a given key:</p> <pre><code>status = repo.get_status(\"emulation1\")\nprint(status)\n# Example output: {\"global_status\": \"running\", \"threads\": {1: \"processing\"}}\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/","title":"In memory repository","text":""},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/#libs.ddd.infra.repository.in-memory.mem_repository.in_memory_repository.InMemoryRepository","title":"<code>InMemoryRepository</code>","text":"Source code in <code>libs/ddd/infra/repository/in-memory/mem_repository/in_memory_repository.py</code> <pre><code>class InMemoryRepository:\n    def __init__(self) -&gt; None:\n        self._store: dict[str, dict[str, any]] = {}\n\n    def create_status(self, key: str, status: str) -&gt; None:\n        \"\"\"\n        Create a new status entry in the repository.\n        Args:\n            key (str): The key for the status entry.\n            status (str): The initial status.\n        \"\"\"\n        self._store[key] = {\"global_status\": status, \"threads\": {}}\n\n    def update_status(self, key: str, status: str) -&gt; None:\n        \"\"\"\n        Update the global status of an existing entry.\n        Args:\n            key (str): The key for the status entry.\n            status (str): The new global status.\n        \"\"\"\n        if key in self._store:\n            self._store[key][\"global_status\"] = status\n\n    def update_thread_status(self, key: str, thread_id: int, status: str) -&gt; None:\n        \"\"\"\n        Update the status of a specific thread in an existing entry.\n        Args:\n            key (str): The key for the status entry.\n            thread_id (int): The ID of the thread to update.\n            status (str): The new status for the thread.\n        \"\"\"\n        if key in self._store:\n            self._store[key][\"threads\"][thread_id] = status\n\n    def get_status(self, key: str) -&gt; dict[str, any] | None:\n        \"\"\"\n        Retrieve the status entry for a given key.\n        Args:\n            key (str): The key for the status entry.\n        Returns:\n            dict[str, any] | None: The status entry if found, otherwise None.\n        \"\"\"\n        return self._store.get(key)\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/#libs.ddd.infra.repository.in-memory.mem_repository.in_memory_repository.InMemoryRepository.create_status","title":"<code>create_status(key, status)</code>","text":"<p>Create a new status entry in the repository. Args:     key (str): The key for the status entry.     status (str): The initial status.</p> Source code in <code>libs/ddd/infra/repository/in-memory/mem_repository/in_memory_repository.py</code> <pre><code>def create_status(self, key: str, status: str) -&gt; None:\n    \"\"\"\n    Create a new status entry in the repository.\n    Args:\n        key (str): The key for the status entry.\n        status (str): The initial status.\n    \"\"\"\n    self._store[key] = {\"global_status\": status, \"threads\": {}}\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/#libs.ddd.infra.repository.in-memory.mem_repository.in_memory_repository.InMemoryRepository.update_status","title":"<code>update_status(key, status)</code>","text":"<p>Update the global status of an existing entry. Args:     key (str): The key for the status entry.     status (str): The new global status.</p> Source code in <code>libs/ddd/infra/repository/in-memory/mem_repository/in_memory_repository.py</code> <pre><code>def update_status(self, key: str, status: str) -&gt; None:\n    \"\"\"\n    Update the global status of an existing entry.\n    Args:\n        key (str): The key for the status entry.\n        status (str): The new global status.\n    \"\"\"\n    if key in self._store:\n        self._store[key][\"global_status\"] = status\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/#libs.ddd.infra.repository.in-memory.mem_repository.in_memory_repository.InMemoryRepository.update_thread_status","title":"<code>update_thread_status(key, thread_id, status)</code>","text":"<p>Update the status of a specific thread in an existing entry. Args:     key (str): The key for the status entry.     thread_id (int): The ID of the thread to update.     status (str): The new status for the thread.</p> Source code in <code>libs/ddd/infra/repository/in-memory/mem_repository/in_memory_repository.py</code> <pre><code>def update_thread_status(self, key: str, thread_id: int, status: str) -&gt; None:\n    \"\"\"\n    Update the status of a specific thread in an existing entry.\n    Args:\n        key (str): The key for the status entry.\n        thread_id (int): The ID of the thread to update.\n        status (str): The new status for the thread.\n    \"\"\"\n    if key in self._store:\n        self._store[key][\"threads\"][thread_id] = status\n</code></pre>"},{"location":"reference/libs/ddd/infra/repository/in-memory/code_reference/mem_repository/in_memory_repository/#libs.ddd.infra.repository.in-memory.mem_repository.in_memory_repository.InMemoryRepository.get_status","title":"<code>get_status(key)</code>","text":"<p>Retrieve the status entry for a given key. Args:     key (str): The key for the status entry. Returns:     dict[str, any] | None: The status entry if found, otherwise None.</p> Source code in <code>libs/ddd/infra/repository/in-memory/mem_repository/in_memory_repository.py</code> <pre><code>def get_status(self, key: str) -&gt; dict[str, any] | None:\n    \"\"\"\n    Retrieve the status entry for a given key.\n    Args:\n        key (str): The key for the status entry.\n    Returns:\n        dict[str, any] | None: The status entry if found, otherwise None.\n    \"\"\"\n    return self._store.get(key)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/","title":"Storage","text":"<p>The Storage Library provides a unified interface for interacting with bucket-based storage systems. Currently, it includes an implementation for Minio via the <code>MinioStorageClient</code>. The design is extensible so that future implementations (e.g., AWS S3, Google Cloud Storage) can be added seamlessly by conforming to the <code>StorageClient</code> interface.</p>"},{"location":"reference/libs/ddd/infra/storage/#features","title":"Features","text":"<ul> <li>Unified Interface:   Implements a common interface (<code>StorageClient</code>) for storage operations, ensuring consistent usage across different providers.</li> <li> <p>Bucket Operations:   Supports creating buckets, listing buckets, and listing objects within buckets.</p> </li> <li> <p>Object Operations:   Provides methods to upload files or raw bytes (or BytesIO data) and to download files either as local files or as bytes.</p> </li> <li> <p>URI Generation:   Generates URIs for accessing stored objects, making integration with external systems easier.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/storage/#installation","title":"Installation","text":"<p>Add the Storage library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name ddd-infra-storage --local\n</code></pre> <p>Ensure that your environment has all required dependencies, including <code>minio</code> and your logger library.</p>"},{"location":"reference/libs/ddd/infra/storage/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/infra/storage/#instantiating-the-minio-storage-client","title":"Instantiating the Minio Storage Client","text":"<p>To begin using the Minio storage client, import and instantiate <code>MinioStorageClient</code> with the appropriate connection parameters:</p> <pre><code>from storage.minio.storage import MinioStorageClient\n\n# Initialize the client with your Minio endpoint and credentials.\nminio_client = MinioStorageClient(\n    endpoint=\"minio.example.com\",\n    access_key=\"your_access_key\",\n    secret_key=\"your_secret_key\",\n    secure=False\n)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#creating-a-bucket","title":"Creating a Bucket","text":"<p>Create a new bucket on the Minio server:</p> <pre><code>bucket_name = \"my_bucket\"\nminio_client.create_bucket(bucket_name)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#listing-buckets","title":"Listing Buckets","text":"<p>Retrieve a list of all buckets available on the Minio server:</p> <pre><code>buckets = minio_client.list_buckets()\nprint(\"Buckets:\", buckets)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#uploading-files-and-bytes","title":"Uploading Files and Bytes","text":""},{"location":"reference/libs/ddd/infra/storage/#upload-a-file","title":"Upload a File","text":"<p>Upload a local file to a specified bucket and obtain its URI:</p> <pre><code>uri = minio_client.upload_file(\"my_bucket\", \"example.txt\", \"path/to/local/file.txt\")\nprint(\"File uploaded to:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#upload-raw-bytes","title":"Upload Raw Bytes","text":"<p>Upload data as bytes (or using a BytesIO instance) and get its URI:</p> <pre><code># Upload using raw bytes\nuri = minio_client.upload_bytes(\"my_bucket\", \"data.txt\", b\"Sample data\")\nprint(\"Data uploaded to:\", uri)\n\n# Upload using a BytesIO instance\nfrom io import BytesIO\nbytes_io = BytesIO(b\"Sample data from BytesIO\")\nuri = minio_client.upload_bytes(\"my_bucket\", \"data_bytesio.txt\", bytes_io)\nprint(\"Data uploaded to:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#downloading-files-and-bytes","title":"Downloading Files and Bytes","text":""},{"location":"reference/libs/ddd/infra/storage/#download-a-file-locally","title":"Download a File Locally","text":"<p>Download an object from a bucket and save it to a local file:</p> <pre><code>minio_client.download_file(\"my_bucket\", \"example.txt\", \"path/to/save/example.txt\")\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#download-as-bytes","title":"Download as Bytes","text":"<p>Download an object from a bucket and obtain its data as bytes:</p> <pre><code>data = minio_client.download_file_as_bytes(\"my_bucket\", \"example.txt\")\nprint(\"Downloaded data:\", data)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#listing-objects-in-a-bucket","title":"Listing Objects in a Bucket","text":"<p>Retrieve a list of object names stored in a specific bucket:</p> <pre><code>objects = minio_client.list_objects(\"my_bucket\")\nprint(\"Objects in bucket:\", objects)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#generating-object-uris","title":"Generating Object URIs","text":"<p>Generate a URI to access an object stored in a bucket:</p> <pre><code>uri = minio_client.get_uri(\"my_bucket\", \"example.txt\")\nprint(\"Access URI:\", uri)\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/#configuration-details","title":"Configuration Details","text":"<ul> <li> <p>Minio Connection:   The client is configured with the Minio server\u2019s endpoint and credentials. Use the <code>secure</code> flag to toggle between HTTP and HTTPS.</p> </li> <li> <p>Logging:   All operations log detailed messages using the integrated logger. Adjust or replace the logger if needed.</p> </li> <li> <p>Extensibility:   The library is designed around the <code>StorageClient</code> interface. Future implementations for other storage providers can be added with minimal changes to the application code.</p> </li> </ul>"},{"location":"reference/libs/ddd/infra/storage/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that all functionality works as expected. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test ddd-infra-storage\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/","title":"Bucket storage","text":""},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient","title":"<code>StorageClient</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>class StorageClient(ABC):\n    @abstractmethod\n    def create_bucket(self, bucket_name: str) -&gt; None:\n        \"\"\"Create a new bucket.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_buckets(self) -&gt; List[str]:\n        \"\"\"List all buckets available.\"\"\"\n        pass\n\n    @abstractmethod\n    def upload_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; str:\n        \"\"\"Upload a file to a bucket and return its URI.\"\"\"\n        pass\n\n    @abstractmethod\n    def upload_bytes(\n        self, bucket_name: str, object_name: str, bytes_data: bytes\n    ) -&gt; str:\n        \"\"\"Upload bytes data to a bucket and return its URI.\"\"\"\n        pass\n\n    @abstractmethod\n    def download_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; None:\n        \"\"\"Download an object from a bucket and save it locally.\"\"\"\n        pass\n\n    @abstractmethod\n    def download_file_as_bytes(self, bucket_name: str, object_name: str) -&gt; bytes:\n        \"\"\"Download an object from a bucket and return its data as bytes.\"\"\"\n        pass\n\n    @abstractmethod\n    def list_objects(self, bucket_name: str) -&gt; List[str]:\n        \"\"\"List object names in the specified bucket.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_uri(self, bucket_name: str, object_name: str) -&gt; str:\n        \"\"\"Generate a URI to access an object.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.create_bucket","title":"<code>create_bucket(bucket_name)</code>  <code>abstractmethod</code>","text":"<p>Create a new bucket.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef create_bucket(self, bucket_name: str) -&gt; None:\n    \"\"\"Create a new bucket.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.list_buckets","title":"<code>list_buckets()</code>  <code>abstractmethod</code>","text":"<p>List all buckets available.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef list_buckets(self) -&gt; List[str]:\n    \"\"\"List all buckets available.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.upload_file","title":"<code>upload_file(bucket_name, object_name, file_path)</code>  <code>abstractmethod</code>","text":"<p>Upload a file to a bucket and return its URI.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef upload_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; str:\n    \"\"\"Upload a file to a bucket and return its URI.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.upload_bytes","title":"<code>upload_bytes(bucket_name, object_name, bytes_data)</code>  <code>abstractmethod</code>","text":"<p>Upload bytes data to a bucket and return its URI.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef upload_bytes(\n    self, bucket_name: str, object_name: str, bytes_data: bytes\n) -&gt; str:\n    \"\"\"Upload bytes data to a bucket and return its URI.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.download_file","title":"<code>download_file(bucket_name, object_name, file_path)</code>  <code>abstractmethod</code>","text":"<p>Download an object from a bucket and save it locally.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef download_file(self, bucket_name: str, object_name: str, file_path: str) -&gt; None:\n    \"\"\"Download an object from a bucket and save it locally.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.download_file_as_bytes","title":"<code>download_file_as_bytes(bucket_name, object_name)</code>  <code>abstractmethod</code>","text":"<p>Download an object from a bucket and return its data as bytes.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef download_file_as_bytes(self, bucket_name: str, object_name: str) -&gt; bytes:\n    \"\"\"Download an object from a bucket and return its data as bytes.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.list_objects","title":"<code>list_objects(bucket_name)</code>  <code>abstractmethod</code>","text":"<p>List object names in the specified bucket.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef list_objects(self, bucket_name: str) -&gt; List[str]:\n    \"\"\"List object names in the specified bucket.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/infra/storage/code_reference/storage/bucket_storage/#libs.ddd.infra.storage.storage.bucket_storage.StorageClient.get_uri","title":"<code>get_uri(bucket_name, object_name)</code>  <code>abstractmethod</code>","text":"<p>Generate a URI to access an object.</p> Source code in <code>libs/ddd/infra/storage/storage/bucket_storage.py</code> <pre><code>@abstractmethod\ndef get_uri(self, bucket_name: str, object_name: str) -&gt; str:\n    \"\"\"Generate a URI to access an object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/","title":"Value Objects","text":"<p>The Value Objects Library provides a set of immutable objects to represent domain values consistently and reliably. It includes the <code>EmulationID</code> value object, which encapsulates a unique identifier for emulation instances using a UUID format.</p>"},{"location":"reference/libs/ddd/value-objects/#features","title":"Features","text":"<ul> <li>Immutable Data Structures: Uses Python\u2019s <code>@dataclass(frozen=True)</code> to ensure immutability.</li> <li>UUID Validation: Automatically validates that any provided identifier conforms to the UUID format.</li> <li>Convenience Method: Includes a class method to generate a new, valid <code>EmulationID</code>.</li> </ul>"},{"location":"reference/libs/ddd/value-objects/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name ddd-value-objects --local\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#usage","title":"Usage","text":""},{"location":"reference/libs/ddd/value-objects/#creating-an-emulationid","title":"Creating an EmulationID","text":"<p>Instantiate an <code>EmulationID</code> with a valid UUID string:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\n# Create an EmulationID using a valid UUID string\nemulation_id = EmulationID(\"123e4567-e89b-12d3-a456-426614174000\")\nprint(emulation_id.value)\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#generating-a-new-emulationid","title":"Generating a New EmulationID","text":"<p>Generate a new unique <code>EmulationID</code> using the provided class method:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\n# Generate a new EmulationID\nnew_id = EmulationID.generate()\nprint(new_id.value)\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#handling-invalid-ids","title":"Handling Invalid IDs","text":"<p>If an invalid UUID is provided, the class raises a <code>ValueError</code> during initialization:</p> <pre><code>from value_objects.emulation_id import EmulationID\n\ntry:\n    invalid_id = EmulationID(\"invalid-uuid\")\nexcept ValueError as e:\n    print(e)  # Output: \"Invalid EmulationID: invalid-uuid\"\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that the value objects behave as expected. To run the tests, navigate to the <code>libs/shared/value-objects</code> directory and execute:</p> <pre><code>npx nx test ddd-value-objects\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/","title":"Emulator id","text":""},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID","title":"<code>EmulationID</code>  <code>dataclass</code>","text":"<p>Value Object representing an emulation's unique identifier.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>@dataclass(frozen=True)\nclass EmulationID:\n    \"\"\"Value Object representing an emulation's unique identifier.\"\"\"\n\n    value: str\n\n    def __post_init__(self):\n        \"\"\"Ensure the ID is a valid UUID.\"\"\"\n        try:\n            uuid.UUID(self.value)  # Validate UUID format\n        except ValueError:\n            raise ValueError(f\"Invalid EmulationID: {self.value}\") from None\n\n    @classmethod\n    def generate(cls) -&gt; \"EmulationID\":\n        \"\"\"Generate a new AgentID.\"\"\"\n        return cls(value=str(uuid.uuid4()))\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Ensure the ID is a valid UUID.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Ensure the ID is a valid UUID.\"\"\"\n    try:\n        uuid.UUID(self.value)  # Validate UUID format\n    except ValueError:\n        raise ValueError(f\"Invalid EmulationID: {self.value}\") from None\n</code></pre>"},{"location":"reference/libs/ddd/value-objects/code_reference/value_objects/emulator_id/#libs.ddd.value-objects.value_objects.emulator_id.EmulationID.generate","title":"<code>generate()</code>  <code>classmethod</code>","text":"<p>Generate a new AgentID.</p> Source code in <code>libs/ddd/value-objects/value_objects/emulator_id.py</code> <pre><code>@classmethod\ndef generate(cls) -&gt; \"EmulationID\":\n    \"\"\"Generate a new AgentID.\"\"\"\n    return cls(value=str(uuid.uuid4()))\n</code></pre>"},{"location":"reference/libs/fake-factory/","title":"Fake Factory","text":"<p>The Fake Factory Library provides a collection of data generation factories designed to simulate realistic, synthetic data for testing, development, and feature store ingestion. Built on top of the Faker library, it offers a flexible base factory and specialized implementations that generate fraud-related records, such as device logs, transactions, and user profiles.</p>"},{"location":"reference/libs/fake-factory/#features","title":"Features","text":"<ul> <li> <p>Extensible Base Factory:   The library defines a <code>BaseFakeFactory</code> interface that can be extended to create new factories with custom logic.</p> </li> <li> <p>Realistic Data Generation:   Leveraging the Faker library, each factory produces synthetic yet realistic data, including names, emails, timestamps, and more.</p> </li> <li> <p>Fraud Simulation:   Specialized factories (e.g., for device logs, transactions, and user profiles) implement fraud rules to simulate various risk scenarios, enabling testing of fraud detection systems and data pipelines.</p> </li> <li> <p>Configurability:   Factories include configurable parameters (e.g., user ID ranges) and randomness to mimic real-world variability and edge cases.</p> </li> </ul>"},{"location":"reference/libs/fake-factory/#installation","title":"Installation","text":"<p>Add the Fake-Factory library to your monorepo by running:</p> <pre><code>npx nx run &lt;project&gt;:add --name fake-factory --local\n</code></pre> <p>Ensure that your environment includes the required dependencies (e.g., Faker) as defined in the <code>pyproject.toml</code> and <code>poetry.toml</code> files.</p>"},{"location":"reference/libs/fake-factory/#usage","title":"Usage","text":""},{"location":"reference/libs/fake-factory/#base-factory","title":"Base Factory","text":"<p>The <code>BaseFakeFactory</code> is the abstract base class that all specific factories extend. It defines the contract for generating fake records.</p> <pre><code>from fake_factory.base_factory import BaseFakeFactory\n\nclass MyFactory(BaseFakeFactory):\n    def generate(self) -&gt; dict[str, Any]:\n        # Implement record generation logic here.\n        return {\"dummy\": \"data\"}\n</code></pre>"},{"location":"reference/libs/fake-factory/#device-log-factory","title":"Device Log Factory","text":"<p>The <code>DeviceLogFactory</code> generates fake device log records, including unique log IDs, device details, and timestamps. For example:</p> <pre><code>from fake_factory.fraud.device_factory import DeviceLogFactory\n\nfactory = DeviceLogFactory()\ndevice_log = factory.generate()\nprint(device_log)\n</code></pre>"},{"location":"reference/libs/fake-factory/#transaction-factory","title":"Transaction Factory","text":"<p>The <code>TransactionFakeFactory</code> simulates transaction data with optional fraud labeling. It applies conditional fraud rules to raw transaction data for realistic risk simulation:</p> <pre><code>from fake_factory.fraud.transaction_factory import TransactionFakeFactory\n\ntransaction_factory = TransactionFakeFactory()\ntransaction = transaction_factory.generate(with_label=True)\nprint(transaction)\n</code></pre>"},{"location":"reference/libs/fake-factory/#user-profile-factory","title":"User Profile Factory","text":"<p>The <code>UserProfileFactory</code> generates user profiles with unique IDs and calculates a risk level based on factors such as credit score, signup date, and country:</p> <pre><code>from fake_factory.fraud.user_profile_factory import UserProfileFactory\n\nprofile_factory = UserProfileFactory()\nuser_profile = profile_factory.generate()\nprint(user_profile)\n</code></pre>"},{"location":"reference/libs/fake-factory/#configuration-details","title":"Configuration Details","text":"<ul> <li>Data Realism:   Each factory uses the Faker library to produce data that closely resembles real-world records.</li> <li>Fraud Rules:   The specialized fraud factories include multiple conditional branches to simulate different risk scenarios. For example, the TransactionFakeFactory can label transactions as fraudulent based on user compromise, card testing, or geographical anomalies.</li> <li>User ID Management:   The UserProfileFactory maintains a thread-safe queue of user IDs to ensure unique identifiers across generated profiles.</li> </ul>"},{"location":"reference/libs/fake-factory/#testing","title":"Testing","text":"<p>Unit tests are provided to validate the functionality and consistency of the data generators. To run the tests, navigate to the library\u2019s directory and execute:</p> <pre><code>npx nx test fake-factory\n</code></pre>"},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/","title":"Base factory","text":""},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/#libs.fake-factory.fake_factory.base_factory.BaseFakeFactory","title":"<code>BaseFakeFactory</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>libs/fake-factory/fake_factory/base_factory.py</code> <pre><code>class BaseFakeFactory(ABC):\n    @abstractmethod\n    def generate(self) -&gt; dict[str, Any]:\n        \"\"\"Generates a fake data record.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/libs/fake-factory/code_reference/fake_factory/base_factory/#libs.fake-factory.fake_factory.base_factory.BaseFakeFactory.generate","title":"<code>generate()</code>  <code>abstractmethod</code>","text":"<p>Generates a fake data record.</p> Source code in <code>libs/fake-factory/fake_factory/base_factory.py</code> <pre><code>@abstractmethod\ndef generate(self) -&gt; dict[str, Any]:\n    \"\"\"Generates a fake data record.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/libs/settings/emulator-settings/","title":"settings-emulator-settings","text":"<p>Project description here.</p>"},{"location":"reference/libs/settings/emulator-settings/code_reference/emulator_settings/settings/","title":"Settings","text":""},{"location":"reference/libs/shared/cliargs/","title":"CLI Arguments Parser","text":"<p>The CLI Arguments Parser Library provides a convenience function to generate a standard <code>argparse.ArgumentParser</code> with common command-line options such as verbose output, debug mode, logging level, and version information. It is designed to simplify the creation of consistent CLI interfaces across multiple applications.</p>"},{"location":"reference/libs/shared/cliargs/#features","title":"Features","text":"<ul> <li>Standard CLI Options: Pre-configured with commonly used command-line options.</li> <li>Consistent Interface: Ensures a uniform command-line interface for your applications.</li> <li>Ease of Integration: Quickly integrate the parser into any project with minimal configuration.</li> </ul>"},{"location":"reference/libs/shared/cliargs/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name shared-cliargs --local\n</code></pre>"},{"location":"reference/libs/shared/cliargs/#usage","title":"Usage","text":""},{"location":"reference/libs/shared/cliargs/#basic-setup","title":"Basic Setup","text":"<p>To create a standard argument parser for your application, import the <code>new_args_parser</code> function:</p> <pre><code>from cliargs.cliargs import new_args_parser\n\n# Create the argument parser with a brief description of your application\nparser = new_args_parser(\"Description of your application\")\nargs = parser.parse_args()\n\nif args.verbose:\n    print(\"Verbose mode enabled\")\n</code></pre>"},{"location":"reference/libs/shared/cliargs/#standard-cli-options","title":"Standard CLI Options","text":"<p>The parser includes the following command-line options:</p> <ul> <li><code>--verbose</code>: Enable verbose output.</li> <li><code>--debug</code>: Activate debug mode with detailed logging.</li> <li><code>--log-level</code>: Set the logging level. Acceptable values are \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".</li> <li><code>--version</code>: Display the application's version and exit.</li> </ul>"},{"location":"reference/libs/shared/cliargs/#configuration-details","title":"Configuration Details","text":"<p>The <code>new_args_parser</code> function initializes an <code>argparse.ArgumentParser</code> with the provided description and adds the standard CLI options listed above. You can easily extend the parser with additional arguments as required by your application.</p>"},{"location":"reference/libs/shared/cliargs/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure the argument parser functions correctly. To run the tests, navigate to the <code>libs/shared/cliargs</code> directory and execute:</p> <pre><code>npx nx test shared-cliargs\n</code></pre>"},{"location":"reference/libs/shared/cliargs/code_reference/cliargs/cli/","title":"Cli","text":"<p>Module for Creating a Standard Argument Parser</p> <p>This module provides a convenience function to generate an argparse.ArgumentParser instance with common command-line options such as verbose output, debug mode, logging level, and version information. It is designed to simplify the creation of consistent CLI interfaces across multiple applications.</p>"},{"location":"reference/libs/shared/cliargs/code_reference/cliargs/cli/#libs.shared.cliargs.cliargs.cli.new_args_parser","title":"<code>new_args_parser(description)</code>","text":"<p>Create and return an ArgumentParser with standard CLI options.</p> <p>This function initializes an argparse.ArgumentParser with the provided description and adds the following command-line arguments:</p> <pre><code>--verbose:\n    Enable verbose output.\n\n--debug:\n    Activate debug mode with detailed logging.\n\n--log-level:\n    Set the logging level. Acceptable values are \"DEBUG\", \"INFO\",\n    \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".\n\n--version:\n    Display the application's version and exit. The version is\n    hardcoded as \"0.1.0\".\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A brief description of the application, which is displayed in the help message.</p> required <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>argparse.ArgumentParser: An ArgumentParser object configured with the</p> <code>ArgumentParser</code> <p>standard command-line arguments.</p> Source code in <code>libs/shared/cliargs/cliargs/cli.py</code> <pre><code>def new_args_parser(description: str) -&gt; argparse.ArgumentParser:\n    \"\"\"Create and return an ArgumentParser with standard CLI options.\n\n    This function initializes an argparse.ArgumentParser with the provided\n    description and adds the following command-line arguments:\n\n        --verbose:\n            Enable verbose output.\n\n        --debug:\n            Activate debug mode with detailed logging.\n\n        --log-level:\n            Set the logging level. Acceptable values are \"DEBUG\", \"INFO\",\n            \"WARNING\", \"ERROR\", and \"CRITICAL\". Defaults to \"INFO\".\n\n        --version:\n            Display the application's version and exit. The version is\n            hardcoded as \"0.1.0\".\n\n    Args:\n        description (str): A brief description of the application, which is\n            displayed in the help message.\n\n    Returns:\n        argparse.ArgumentParser: An ArgumentParser object configured with the\n        standard command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output.\")\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Activate debug mode with detailed logging.\",\n    )\n    parser.add_argument(\n        \"--log-level\",\n        type=str,\n        default=\"INFO\",\n        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n        help=\"Set the logging level. Defaults to INFO.\",\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s 0.1.0\",\n        help=\"Show the application's version and exit.\",\n    )\n    return parser\n</code></pre>"},{"location":"reference/libs/shared/logger/","title":"Logger","text":"<p>The Logger Library provides a simple and effective way to configure logging for your Python applications. It uses JSON formatting for log messages, making it easier to integrate with log management systems and enabling structured logging.</p>"},{"location":"reference/libs/shared/logger/#features","title":"Features","text":"<ul> <li>JSON Formatted Logging: Logs are output in a structured JSON format.</li> <li>Configurable Log Levels: Easily set the desired log level via function parameters or environment variables.</li> <li>Modular Integration: Quickly integrate the logger into any module by providing the module name.</li> <li>Propagation Control: Option to propagate log messages to parent loggers.</li> </ul>"},{"location":"reference/libs/shared/logger/#installation","title":"Installation","text":"<pre><code>npx nx run &lt;project&gt;:add --name shared-logger --local\n</code></pre>"},{"location":"reference/libs/shared/logger/#usage","title":"Usage","text":""},{"location":"reference/libs/shared/logger/#basic-setup","title":"Basic Setup","text":"<p>To set up logging with a custom module name, import the <code>setup_logging</code> function:</p> <pre><code>from logger.log import setup_logging\n\n# Create a logger for your module\nlogger = setup_logging(__name__)\n\n# Log an informational message\nlogger.info(\"This is an info message.\")\n</code></pre>"},{"location":"reference/libs/shared/logger/#environment-based-setup","title":"Environment-Based Setup","text":"<p>You can also create a logger that automatically reads the log level from the <code>LOG_LEVEL</code> environment variable. Use the <code>get_logger_from_env</code> function as follows:</p> <pre><code>from logger.log import get_logger_from_env\n\n# Create a logger that uses the LOG_LEVEL environment variable\nlogger = get_logger_from_env(__name__)\n\n# Log a debug message (if LOG_LEVEL is set to DEBUG)\nlogger.debug(\"This is a debug message.\")\n</code></pre> <p>Before running your application, set the <code>LOG_LEVEL</code> environment variable if needed:</p> <pre><code>export LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"reference/libs/shared/logger/#configuration-details","title":"Configuration Details","text":"<ul> <li>JSON Formatter:   The logger uses the following format for JSON logs:</li> </ul> <pre><code>%(levelname)s %(filename)s %(message)s\n</code></pre> <p>You can modify this format directly in the code by adjusting the formatter setup if a different structure is required.</p> <ul> <li>Propagation:   The <code>setup_logging</code> function accepts a <code>propagate</code> parameter. Set it to <code>True</code> if you want log messages to propagate to the parent logger.</li> </ul>"},{"location":"reference/libs/shared/logger/#testing","title":"Testing","text":"<p>Unit tests are provided to ensure that the logger functions correctly. To run the tests, navigate to the <code>libs/shared/logger</code> directory and execute:</p> <pre><code>npx nx test shared-logger\n</code></pre>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/","title":"Log","text":"<p>Logging module.</p>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/#libs.shared.logger.logger.log.setup_logging","title":"<code>setup_logging(module_name, propagate=False, log_level=os.getenv('LOG_LEVEL', 'INFO').upper())</code>","text":"<p>Set up logging using JSON format.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The module name.</p> required <code>propagate</code> <code>bool</code> <p>Whether to propagate the logging to the parent logger.</p> <code>False</code> <code>log_level</code> <code>str</code> <p>The log level.</p> <code>upper()</code> <p>Returns:</p> Type Description <code>Logger</code> <p>The logger.</p> Source code in <code>libs/shared/logger/logger/log.py</code> <pre><code>def setup_logging(\n    module_name: str,\n    propagate: bool = False,\n    log_level: str = os.getenv(\"LOG_LEVEL\", \"INFO\").upper(),\n) -&gt; logging.Logger:\n    \"\"\"\n    Set up logging using JSON format.\n\n    Args:\n        module_name (str): The module name.\n        propagate (bool): Whether to propagate the logging to the parent logger.\n        log_level (str): The log level.\n\n    Returns:\n        The logger.\n    \"\"\"\n    log_handler = logging.StreamHandler()\n    formatter = json.JsonFormatter(\"%(levelname)s %(filename)s %(message)s\")\n    log_handler.setFormatter(formatter)\n\n    logger = logging.getLogger(module_name)\n    logger.addHandler(log_handler)\n    logger.propagate = propagate\n    logger.setLevel(logging.getLevelName(log_level))\n    return logger\n</code></pre>"},{"location":"reference/libs/shared/logger/code_reference/logger/log/#libs.shared.logger.logger.log.get_logger_from_env","title":"<code>get_logger_from_env(module_name)</code>","text":"<p>Get a logger using the <code>LOG_LEVEL</code> environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The module name.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>The logger.</p> Source code in <code>libs/shared/logger/logger/log.py</code> <pre><code>def get_logger_from_env(module_name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger using the `LOG_LEVEL` environment variable.\n\n    Args:\n        module_name (str): The module name.\n\n    Returns:\n        The logger.\n    \"\"\"\n    log_level = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n    return setup_logging(module_name, log_level=log_level)\n</code></pre>"},{"location":"reference/services/airflow-app/","title":"Airflow Application","text":"<p>The Airflow App is a core component of the Lakehouse Lab ecosystem, responsible for orchestrating data workflows and managing data processing jobs. This project leverages Apache Airflow to automate and schedule tasks such as running data pipelines, monitoring process statuses, and integrating with other Lakehouse Lab services.</p>"},{"location":"reference/services/airflow-app/#overview","title":"Overview","text":"<p>This Airflow application includes:</p> <ul> <li>DAGs:</li> <li><code>fraud_pipeline_dag.py</code>: Orchestrates the fraud detection pipeline, managing job dependencies and scheduling.</li> <li>Jobs:   Python scripts for processing data at different stages:</li> <li><code>bronze_user_profile.py</code>: Processes raw user profile data (Bronze layer).</li> <li><code>silver_user_profile.py</code>: Handles transformation and enrichment of user profile data (Silver layer).</li> <li>Plugins:   Custom Airflow operators:</li> <li><code>start_emulator_operator.py</code>: Initiates data emulator processes.</li> <li><code>status_emulation_operator.py</code>: Monitors the status of data emulation tasks.</li> </ul> <p>The project is managed using Poetry for dependency management. A Dockerfile is provided for containerized deployments.</p>"},{"location":"reference/services/airflow-app/#custom-plugins","title":"Custom Plugins","text":""},{"location":"reference/services/airflow-app/#start-emulator-operator","title":"Start Emulator Operator","text":"<p>The <code>start_emulator_operator.py</code> plugin defines a custom operator to trigger the Data Emulator Service via its API. This operator encapsulates the logic needed to initialize an emulator task and pass the necessary configuration.</p>"},{"location":"reference/services/airflow-app/#status-emulation-operator","title":"Status Emulation Operator","text":"<p>The <code>status_emulation_operator.py</code> plugin allows the Airflow DAG to query the status of an ongoing emulation process, ensuring that downstream tasks only run when data generation is complete.</p>"},{"location":"reference/services/airflow-app/#further-information","title":"Further Information","text":"<p>For more detailed technical documentation on the Data Emulator Service, including usage examples, API endpoints, and extended architecture details, please visit: Lakehouse Lab Data Emulator Documentation</p>"},{"location":"reference/services/data-emulator/","title":"Data Emulator Service","text":"<p>The Data Emulator Service orchestrates the emulation of data ingestion workflows. It provides a REST API\u2014built on FastAPI\u2014that enables external clients to trigger and manage emulator processes. The service integrates configuration management, logging, and resource setup (using Kafka and Minio) to simulate realistic data production. It leverages asynchronous background tasks and graceful signal handling for robust operation.</p>"},{"location":"reference/services/data-emulator/#overview","title":"Overview","text":"<p>The Data Emulator Service is designed to generate synthetic data for testing, development, and integration purposes. It consists of a primary process that:</p> <ul> <li>Parses command-line arguments using a custom CLI argument parser.</li> <li>Loads configuration settings via a custom <code>Settings</code> class.</li> <li>Sets up centralized logging.</li> <li>Spawns a child process to run the REST API server (using Uvicorn).</li> </ul> <p>The service uses an asynchronous signal handler to listen for termination signals (such as SIGINT and SIGTERM), ensuring that both the REST API server and any running tasks are shutdown gracefully.</p>"},{"location":"reference/services/data-emulator/#features","title":"Features","text":"<ul> <li> <p>RESTful API Interface:   Provides endpoints to interact with emulator use cases, allowing external triggering of long-running data emulation tasks.</p> </li> <li> <p>Modular Architecture:   Separates concerns by delegating configuration, logging, and resource initialization to dedicated modules. The REST API is run in an isolated child process.</p> </li> <li> <p>Configuration and Dependency Injection:   Uses a custom <code>Settings</code> class to configure external resources (Kafka, Minio) and logging. Command-line arguments control verbosity, debug mode, and log levels.</p> </li> <li> <p>Asynchronous Background Tasks:   Emulation tasks are scheduled asynchronously using FastAPI\u2019s BackgroundTasks, ensuring that API responses remain quick while the heavy lifting occurs in the background.</p> </li> <li> <p>Graceful Shutdown Handling:   A dedicated signal handler captures termination signals and coordinates the proper shutdown of the REST API process and any background tasks.</p> </li> </ul>"},{"location":"reference/services/data-emulator/#architecture","title":"Architecture","text":"<p>The service\u2019s main entry point performs the following steps:</p> <ol> <li> <p>Setup Service:</p> </li> <li> <p>Parses CLI arguments using <code>new_args_parser</code>.</p> </li> <li>Loads configuration settings from both CLI parameters and environment variables.</li> <li> <p>Sets up logging and propagates the log level to the application\u2019s environment.</p> </li> <li> <p>Start REST API Server:</p> </li> <li> <p>Spawns a child process to run the FastAPI REST server with Uvicorn.</p> </li> <li> <p>The REST API, which is built in a separate module (<code>api.emulator_rest_api</code>), receives the configuration via the app state.</p> </li> <li> <p>Signal Handling:</p> </li> <li> <p>Instantiates a <code>SignalHandler</code> that registers for termination signals.</p> </li> <li> <p>Asynchronously waits for a shutdown event; upon receiving a signal, it terminates the REST API child process and cleans up resources.</p> </li> <li> <p>Shutdown:</p> </li> <li>The main process joins the REST API process and exits gracefully.</li> </ol>"},{"location":"reference/services/data-emulator/#usage","title":"Usage","text":"<p>To build the current image for the Data Emulator Service, use:</p> <pre><code>npx nx image service-data-emulator\n</code></pre>"},{"location":"reference/services/data-emulator/#command-line-arguments","title":"Command-Line Arguments","text":"<p>The service supports CLI arguments (via the custom <code>new_args_parser</code>) for configuring log levels, verbosity, and debug modes. For example:</p> <pre><code>python main.py --log-level DEBUG --verbose\n</code></pre>"},{"location":"reference/services/data-emulator/#environment-configuration","title":"Environment Configuration","text":"<p>The service reads external resource settings (e.g., Kafka, Minio) from environment variables as well as the provided CLI arguments. For instance:</p> <ul> <li>Kafka:</li> <li><code>KAFKA_BOOTSTRAP_SERVERS</code></li> <li><code>KAFKA_USERNAME</code></li> <li><code>KAFKA_PASSWORD</code></li> <li>Minio:</li> <li><code>MINIO_ENDPOINT</code></li> <li><code>MINIO_ACCESS_KEY</code></li> <li><code>MINIO_SECRET_KEY</code></li> <li><code>MINIO_SECURE</code></li> </ul> <p>These values are loaded into the custom <code>Settings</code> class and assigned to the FastAPI app state before the REST API process starts.</p>"},{"location":"reference/services/data-emulator/#running-the-rest-api","title":"Running the REST API","text":"<p>The REST API server is launched as a child process using Uvicorn:</p> <pre><code>rest_app.state.config = config\nuvicorn.run(rest_app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n</code></pre> <p>This isolates the API layer while enabling the main process to manage service lifecycle and signal handling.</p>"},{"location":"reference/services/data-emulator/#signal-handling-and-graceful-shutdown","title":"Signal Handling and Graceful Shutdown","text":"<p>The service registers signal handlers for SIGINT and SIGTERM using an asynchronous <code>SignalHandler</code>. When a termination signal is received:</p> <ul> <li>The shutdown event is triggered.</li> <li>The REST API child process is terminated and joined.</li> <li>Any background tasks are allowed to complete, and resources are released before the service exits.</li> </ul>"},{"location":"reference/services/data-emulator/#logging","title":"Logging","text":"<p>The service configures logging early in the startup sequence. It sets the <code>LOG_LEVEL</code> environment variable and uses a custom logging setup to propagate messages. Verbose and debug modes are supported via CLI arguments.</p>"},{"location":"reference/services/data-emulator/#testing","title":"Testing","text":"<p>Unit tests are provided for the various modules including CLI argument parsing, configuration loading, REST API endpoints, and signal handling. To run the tests, execute:</p> <pre><code>npx nx test services-data-emulator\n</code></pre>"},{"location":"reference/services/data-emulator/openapi/","title":"OpenAPI Specification","text":""},{"location":"reference/tools/nx-compose/","title":"nx-compose","text":"<p>This library was generated with Nx.</p>"},{"location":"reference/tools/nx-compose/#building","title":"Building","text":"<p>Run <code>nx build nx-compose</code> to build the library.</p>"},{"location":"reference/tools/nx-compose/#running-unit-tests","title":"Running unit tests","text":"<p>Run <code>nx test nx-compose</code> to execute the unit tests via Jest.</p>"}]}